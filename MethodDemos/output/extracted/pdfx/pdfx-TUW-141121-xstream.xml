<Publication>
  <id>TUW-141121</id>
  <title>Integration of Text and Audio Features for Genre Classification in Music Information Retrieval</title>
  <abstractText>Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song’s audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the ‘content semantics’ of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that ‘sound simi- lar’ while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users’ differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.</abstractText>
  <keywords/>
  <authors/>
  <affiliations/>
  <sections>
    <Section>
      <title>1 Introduction</title>
      <type>deo:Introduction</type>
      <typeEnum>INTRODUCTION</typeEnum>
      <referenceIds/>
      <referenceCitations/>
    </Section>
    <Section>
      <title>2 Related Work</title>
      <type>deo:RelatedWork</type>
      <typeEnum>RELATEDWORK</typeEnum>
      <referenceIds>
        <string>1</string>
        <string>6</string>
        <string>5</string>
        <string>2</string>
        <string>4</string>
        <string>3</string>
      </referenceIds>
      <referenceCitations/>
    </Section>
    <Section>
      <title>3 Experiments</title>
      <type>DoCO:Section</type>
      <typeEnum>METHOD</typeEnum>
      <referenceIds/>
      <referenceCitations/>
    </Section>
    <Section>
      <title>4 Conclusions and Future Work</title>
      <type>deo:Conclusion</type>
      <typeEnum>CONCLUSIONS</typeEnum>
      <referenceIds/>
      <referenceCitations/>
    </Section>
  </sections>
  <citationContexts/>
  <references>
    <Reference>
      <id>ref35</id>
      <referenceIdString>35</referenceIdString>
      <authors/>
      <referenceText>1. Jonathan Foote. An overview of audio information retrieval. Multimedia Systems , 7(1):2–10, 1999.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
    <Reference>
      <id>ref36</id>
      <referenceIdString>36</referenceIdString>
      <authors/>
      <referenceText>2. Thomas Lidy and Andreas Rauber. Evaluation of feature extractors and psycho- acoustic transformations for music genre classification. In Proceedings of the Sixth International Conference on Music Information Retrieval (ISMIR 2005) , pages 34– 41, London, UK, September 11-15 2005.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
    <Reference>
      <id>ref37</id>
      <referenceIdString>37</referenceIdString>
      <authors/>
      <referenceText>3. Beth Logan, Andrew Kositsky, and Pedro Moreno. Semantic analysis of song lyrics. In Proceedings of the 2004 IEEE International Conference on Multimedia and Expo, ICME 2004, 27-30 June 2004, Taipei, Taiwan . IEEE, 2004.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
    <Reference>
      <id>ref38</id>
      <referenceIdString>38</referenceIdString>
      <authors/>
      <referenceText>4. Jose P. G. Mahedero, Alvaro  ́ Mart ́ ınez, Pedro Cano, Markus Koppenberger, and Fabien Gouyon. Natural language processing of lyrics. In MULTIMEDIA ’05: Proceedings of the 13th annual ACM international conference on Multimedia , pages 475–478, New York, NY, USA, 2005. ACM Press.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
    <Reference>
      <id>ref39</id>
      <referenceIdString>39</referenceIdString>
      <authors/>
      <referenceText>5. Andreas Rauber, Elias Pampalk, and Dieter Merkl. Using psycho-acoustic models and self-organizing maps to create a hierarchical structuring of music by musical styles. In Proceedings of the 3rd International Symposium on Music Information Retrieval , pages 71–80, Paris, France, October 13-17 2002.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
    <Reference>
      <id>ref40</id>
      <referenceIdString>40</referenceIdString>
      <authors/>
      <referenceText>6. George Tzanetakis and Perry Cook. Marsyas: A framework for audio analysis. Organized Sound , 4(30), 2000.</referenceText>
      <publication reference="/Publication[1]"/>
    </Reference>
  </references>
</Publication>