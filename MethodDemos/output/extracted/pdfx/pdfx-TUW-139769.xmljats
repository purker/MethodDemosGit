<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  SYSTEM "http://dtd.nlm.nih.gov/archiving/3.0/archivearticle3.dtd">
<article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xlink="http://www.w3.org/1999/xlink">
   <front>
      <journal-meta>
         <journal-id/>
         <journal-title-group>
            <journal-title/>
         </journal-title-group>
         <issn/>
         <publisher>
            <publisher-name/>
         </publisher>
      </journal-meta>
      <article-meta>
         <title-group>
            <article-title>An Extended Local Branching Framework and its Application to the Multidimensional Knapsack Problem</article-title>
         </title-group>
         <supplement>
            <p>DIPLOMARBEIT</p>
            <p>ausgeführt am Institut für Computergrafik und Algorithmen der Technischen Universität Wien unter der Anleitung von a.o. Univ.-Prof. Dipl.-Ing. Dr. Günther Raidl Univ.-Ass. Dipl.-Ing. Jakob Puchinger durch Daniel Lichtenberger Matr. Nr. 9825754 Märzstrasse 80/12 1150 Wien Datum Unterschrift</p>
         </supplement>
      </article-meta>
   </front>
   <body>
      <sec>
         <title>Abstract</title>
         <p>This thesis deals with local branching, a local search algorithm applied on top of a Branch and Cut algorithm for mixed integer programming problems. Local branching defines custom sized neighborhoods around given feasible solutions and solves them partially or completely before exploring the rest of the search space. Its goal is to improve the heuristic behavior of a given exact integer programming solver, i.e. to focus on finding good solutions early in the computation. Local branching is implemented as an extension to the open source Branch and Cut solver COIN/BCP. The framework’s main goal is to provide a generic implementation of local branching for integer programming problems. IP problems are optimization problems where some or all variables are integer values and must satisfy one or more (linear) constraints. Several extensions to the standard local branching algorithm were added to the framework. Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the local branching parameters adaptively during the computation. A major design goal was to provide a clean encapsulation of the local branching algorithm to facilitate embedding of the framework in other, higher-level search algorithms, for example in evolutionary algorithms. As an example application, a solver for the multidimensional knapsack problem is implemented. A custom local branching metaheuristic imposes node limits on local subtrees and adaptively tightens the search space by fixing variables and reducing the size of the neighborhood. Test results show that local branching can offer significant advantages to standard Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for large, complex test instances exploring the local neighborhood of a good feasible solution often yields better short-term results than the unguided standard Branch and Cut algorithm. Improving the solutions found early in the computation also helps to remove additional parts of the search tree, potentially leading to better solutions in longer runs.</p>
      </sec>
      <sec>
         <title>Zusammenfassung</title>
         <p>Diese Diplomarbeit beschäftigt sich mit Local Branching, einem lokalen Suchalgorithmus, der auf einem Branch and Cut Algorithmus für ganzzahlige Optimierungsprobleme aufsetzt. Local Branching definiert beliebig große Nachbarschaften um gegebene gültige Lösungen und löst diese teilweise oder komplett, bevor der Rest des Lösungsraums durchsucht wird. Das Ziel ist eine Verbesserung des heuristischen Verhaltens des gegebenen Solvers für ganzzahlige Optimierungsprobleme, d.h. sich auf das möglichst frühe Finden guter Lösungen zu konzentrieren. Local Branching ist als Erweiterung des Open Source Branch and Cut Solvers COIN/BCP implementiert. Das Hauptziel des Frameworks ist eine generische Implementierung von Local Branching für ganzzahlige Optimierungsprobleme, also Probleme, bei denen alle oder einige Variablen ganzzahlig sein müssen, und zusätzlich eine oder mehrere (lineare) Bedingungen in Form von Ungleichungen erfüllen müssen. Es wurden mehrere Erweiterungen zum Framework hinzugefügt: die pseudo-parallele Abarbeitung mehrerer lokaler Suchbäume, das vorzeitige Terminieren lokaler Suchbäume sowie eine unabhängige Variablen-Fixing- Heuristik. Durch diese Erweiterungen können die Parameter für Local Branching im Laufe der Berechnung beliebig verändert werden. Ein wesentliches Ziel beim Entwurf des Frameworks war eine klare Kapselung des Local Branching Algorithmus, um die Einbettung in andere, höhere Suchalgorithmen zu ermöglichen, etwa in evolutionäre Algorithmen. Als Beispielapplikation wurde ein Solver für das mehrdimensionale Rucksackproblem implementiert. Eine eigene Local Branching Metaheuristik beschränkt die Größe lokaler Bäume durch Knotenlimits und kann den Suchraum durch Anwendung der Variablen- Fixing-Heuristik weiter einschränken. Die Testergebnisse zeigen signifikante Vorteile für Local Branching im Vergleich zum normalen Branch and Cut Algorithmus. Vor allem für große, komplexe Testinstanzen liefert die Suche in lokalen Bäumen oft bessere Resultate am Anfang der Berechnung. Dadurch wird auch die Zeit zum Finden (und Beweisen) der optimalen Lösung potentiell verringert, da dadurch früher zusätzliche Teile des Suchbaums weggeschnitten werden können.</p>
      </sec>
      <sec>
         <title>Contents</title>
      </sec>
      <sec>
         <title>1 Introduction</title>
      </sec>
      <sec>
         <title>4</title>
         <p>1.1 Thesis Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5</p>
      </sec>
      <sec>
         <title>2 Branch and Cut</title>
      </sec>
      <sec>
         <title>6</title>
         <p>2.1 Integer Programming Problems . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.1 Convex Hull of an Integer Program . . . . . . . . . . . . . . . . . . 6 2.1.2 Relaxations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Branch and Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Cutting Plane Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Branch and Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10</p>
      </sec>
      <sec>
         <title>3 Local Branching</title>
      </sec>
      <sec>
         <title>11</title>
         <p>3.1 Soft vs. Hard Variable Fixing . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 A Basic Local Branching Framework . . . . . . . . . . . . . . . . . . . . . 12 3.3 Local Branching Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . 14</p>
      </sec>
      <sec>
         <title>4 An Advanced Local Branching Framework</title>
      </sec>
      <sec>
         <title>16</title>
         <p>4.1 Basic Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.1.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.2 Extending the Basic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.2.1 Using Multiple Local Trees . . . . . . . . . . . . . . . . . . . . . . 17 4.2.2 Aborting Local Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.2.3 Tightening the Search Tree by Variable Fixing . . . . . . . . . . . . 19 4.2.4 Utilizing the Extensions . . . . . . . . . . . . . . . . . . . . . . . . 20</p>
      </sec>
      <sec>
         <title>5 COIN/BCP</title>
      </sec>
      <sec>
         <title>21</title>
         <p>5.1 COIN Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.1.1 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.1.2 Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.2 Design of COIN/BCP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.2.1 Variables and Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.3 COIN/BCP modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.3.1 The Tree Manager Module . . . . . . . . . . . . . . . . . . . . . . . 23 5.3.2 The Linear Programming Module . . . . . . . . . . . . . . . . . . . 24 5.3.3 The Cut Generator Module . . . . . . . . . . . . . . . . . . . . . . . 24 5.3.4 The Variable Generator Module . . . . . . . . . . . . . . . . . . . . 24 5.4 The Linear Programming Module . . . . . . . . . . . . . . . . . . . . . . . 24</p>
         <p>5.4.1 The LP Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.4.2 Managing the LP Relaxation . . . . . . . . . . . . . . . . . . . . . . 24 5.4.3 Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5 Parallelizing COIN/BCP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5.1 Inter-Process Communication . . . . . . . . . . . . . . . . . . . . . 25 5.5.2 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.6 Developing Applications with COIN/BCP . . . . . . . . . . . . . . . . . . . 26 5.6.1 The BCP tm user Class . . . . . . . . . . . . . . . . . . . . . . . . 27 5.6.2 The BCP lp user Class . . . . . . . . . . . . . . . . . . . . . . . . 27</p>
      </sec>
      <sec>
         <title>6 Implementation of the Framework</title>
      </sec>
      <sec>
         <title>29</title>
         <p>6.1 Integrating Local Branching into COIN/BCP . . . . . . . . . . . . . . . . . 31 6.1.1 Identifying Local Tree Nodes . . . . . . . . . . . . . . . . . . . . . 31 6.1.2 The LB tm Module . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6.1.3 The LB lp Module . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.2 Managing Local Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2.1 The LocalTreeIndex . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.2.2 The LocalTreeManager . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.3 Controlling Local Branching . . . . . . . . . . . . . . . . . . . . . . . . . . 37 6.3.1 Implementing a Basic Local Branching Algorithm . . . . . . . . . . 38</p>
      </sec>
      <sec>
         <title>7 Multidimensional Knapsack Problems</title>
      </sec>
      <sec>
         <title>39</title>
         <p>7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7.1.1 Algorithms for Knapsack Problems . . . . . . . . . . . . . . . . . . 39 7.1.2 Multidimensional Knapsack Problems . . . . . . . . . . . . . . . . . 40 7.2 Heuristic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 7.2.1 Greedy Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 7.2.2 Relaxation-Based Heuristics . . . . . . . . . . . . . . . . . . . . . . 42 7.2.3 Hybrid Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 7.2.4 Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 42</p>
      </sec>
      <sec>
         <title>8 A Sample Application: MD-KP</title>
      </sec>
      <sec>
         <title>44</title>
         <p>8.1 KS tm implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 8.1.1 Test File Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 8.1.2 Setting up the Core Matrix . . . . . . . . . . . . . . . . . . . . . . . 45 8.1.3 Packing and Unpacking of Cuts . . . . . . . . . . . . . . . . . . . . 46 8.1.4 Sending the Problem Description to the LP Module . . . . . . . . . . 46 8.1.5 Creating a KS MetaHeuristic Object . . . . . . . . . . . . . . . . . . 47 8.2 KS lp Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 8.2.1 Generating Feasible Solutions . . . . . . . . . . . . . . . . . . . . . 48 8.2.2 Generating Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 8.3 KS init Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 8.4 KS MetaHeuristic Implementation . . . . . . . . . . . . . . . . . . . . . . . 50 8.4.1 Configuring Local Branching . . . . . . . . . . . . . . . . . . . . . 50 8.4.2 Setting up Local Branching . . . . . . . . . . . . . . . . . . . . . . 51 8.4.3 Creating the Initial Solution . . . . . . . . . . . . . . . . . . . . . . 52 8.4.4 Imposing Node Limits on Local Trees . . . . . . . . . . . . . . . . . 54</p>
         <p>8.4.5 Handling Terminated Local Trees . . . . . . . . . . . . . . . . . . . 55 8.5 Finishing Touches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55</p>
      </sec>
      <sec>
         <title>9 Test Results</title>
      </sec>
      <sec>
         <title>57</title>
         <p>9.1 Test Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 9.2 Test Results Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 9.2.1 Final Objective Comparison . . . . . . . . . . . . . . . . . . . . . . 58 9.2.2 Online Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 58 9.3 Local Branching Configurations . . . . . . . . . . . . . . . . . . . . . . . . 59 9.4 Short-Time Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 9.4.1 Local Branching and Node Limits . . . . . . . . . . . . . . . . . . . 60 9.4.2 Cut Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 9.4.3 Multiple Initial Solutions . . . . . . . . . . . . . . . . . . . . . . . . 67 9.5 Long Runs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68</p>
      </sec>
      <sec>
         <title>10 Summary and Outlook A COIN/BCP patches</title>
      </sec>
      <sec>
         <title>73 74</title>
         <p>A.1 Adding User-Defined Messages . . . . . . . . . . . . . . . . . . . . . . . . 74 A.2 Extending the Candidate List . . . . . . . . . . . . . . . . . . . . . . . . . . 75 A.2.1 include/BCP tm node.hpp . . . . . . . . . . . . . . . . . . . . . . . 75 A.2.2 include/BCP tm node.cpp . . . . . . . . . . . . . . . . . . . . . . . 76 A.2.3 TM/BCP tm functions.cpp . . . . . . . . . . . . . . . . . . . . . . . 77 A.3 Counting Pruned Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 A.3.1 TM/BCP tm msg node rec.cpp . . . . . . . . . . . . . . . . . . . . 78 A.3.2 TM/BCP tm msgproc.cpp . . . . . . . . . . . . . . . . . . . . . . . 78 A.4 Aborting Local Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 A.4.1 include/BCP tm node.hpp . . . . . . . . . . . . . . . . . . . . . . . 79 A.4.2 TM/BCP tm functions . . . . . . . . . . . . . . . . . . . . . . . . . 79</p>
      </sec>
      <sec>
         <title>B Test Scripts</title>
      </sec>
      <sec>
         <title>80</title>
         <p>B.1 Generating Log Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 B.2 Analyzing Log Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81</p>
      </sec>
      <sec>
         <title>Bibliography</title>
      </sec>
      <sec>
         <title>82</title>
         <p>Chapter 1</p>
      </sec>
      <sec>
         <title>Introduction</title>
         <p>Integer programming problems (IPs) are optimization problems that restrict some or all variables to integer values. In contrast to linear programming problems (LPs) without integrality constraints, IPs are NP-hard. Much research has gone into effective search algorithms for integer programs, leading to exact algorithms like Branch and Bound [ <xref id="XR51" ref-type="bibr" rid="R25">25</xref>], cutting plane algorithms [<xref id="XR52" ref-type="bibr" rid="R30">30</xref>], and a large variety of heuristical algorithms that trade optimality for quickly getting “good enough” solutions. This thesis considers the modification of standard Branch and Cut to follow ideas from local search based heuristics, the so-called local branching [<xref id="XR53" ref-type="bibr" rid="R13">13</xref>]. Branch and Bound is a generic algorithm for solving integer programming problems by partitioning the search space into smaller subproblems (branching), calculating bounds on the best solution that can be found in a subproblem (bounding), and removing those subproblems that are proven to contain only solutions inferior to the best known solution (pruning). The bounding operation is commonly executed by solving the LP relaxation (i.e. the IP problem without the integral constraints). Branch and Cut tries to delay the branching operation by adding constraints (cuts) that are violated by the current LP result, leading to a reduction of the search tree size. Local branching defines subproblems through additional local branching cuts that isolate a neighborhood of a certain size around a given feasible solution. By exploring this smaller subproblem before the rest of the search tree, the intention is to improve good feasible solutions before continuing Branch and Cut in a standard way. Several extensions have been added to local branching: pseudo-concurrent tree exploration, the possibility to abort local trees, and a variable fixing heuristic have been added. Due to its general design, local branching can be used with any IP solver. A large part of integer programming is concerned with combinatorial problems . These include for example the subset sum equality problem, various graph theory problems, and the well-known family of knapsack problems. In this thesis, the multidimensional knapsack problem is used to demonstrate the use and the benefits of local branching. Although all types of knapsack problems are NP-hard, some problems can be efficiently solved by enumerative techniques like dynamic programming. For others, like the multidimensional knapsack problem, no such methods are known. These problems supply well suited testcases for fully fledged Branch and Cut solvers, and are often too complex to be solved to optimality in reasonable time. In chapter 2, an overview of integer programming problems, cutting plane techniques and Branch and Bound algorithms is given to summarize the building blocks of Branch and Cut. Chapter 3 provides an introduction to local branching as proposed by Fischetti and Lodi [<xref id="XR57" ref-type="bibr" rid="R13">13</xref>]. Chapter 4 introduces the framework implemented for this thesis, including extensions to the local branching algorithm, and describes the overall design of the interface to the framework. In chapter 5, an overview of the open source COIN/BCP framework used for implementing local branching is given. Chapter 6 contains the implementation details of the local branching framework. An overview of knapsack problems in general and multidimensional knapsack problems in particular is given in chapter 7. The implementation of a sample local branching application for the multidimensional knapsack problem is described in chapter 8. Test results exploring the benefits and drawbacks of local branching based on the sample application are given in chapter 9. Chapter 10 summarizes the results and provides a brief outlook on possible future work. In appendix A, the patches necessary for the COIN/BCP source code are described. Appendix B provides a brief overview of the test scripts used for analyzing the local branching test runs.</p>
         <p>1.1 Thesis Overview</p>
         <p>Chapter 2</p>
      </sec>
      <sec>
         <title>Branch and Cut</title>
         <p>Branch and Cut is an exact algorithm for solving integer programming problems. It combines cutting plane methods with Branch and Bound. The following introduction is based on Lee and Mitchell’s Branch and Bound tutorial [ <xref id="XR62" ref-type="bibr" rid="R26">26</xref>], Mitchell’s introduction to Branch and Cut [<xref id="XR63" ref-type="bibr" rid="R29">29</xref>], the COIN/BCP User’s Manual by Ralphs and Ladanyi [<xref id="XR64" ref-type="bibr" rid="R36">36</xref>], and the book on integer programming by Laurence Wolsey [<xref id="XR65" ref-type="bibr" rid="R42">42</xref>]. An integer programming problem (IP) is an optimization problem in which some or all variables are restricted to integer values. A given objective function has to be maximized or mini- mized in a solution space constrained by inequalities. A mixed integer programming problem (MIP) contains both integer and continuous variables, a pure integer programming problem restricts all variables to be integer. Mixed or pure 0-1 integer programming problems restrict all integer variables to be 0 or 1, thus they are also called binary integer programming problems . In this thesis we will concentrate on linear 0-1 integer programming problems where all variables are binary and all terms of the objective function and constraints are linear. The objective function should be maximized. A linear 0-1 IP can then be stated as:</p>
         <p>2.1 Integer Programming Problems</p>
         <p>with A ∈ R m × n , b ∈ R m and c ∈ R n . We can define the solution space S of a problem as</p>
         <p>2.1.1 Convex Hull of an Integer Program In algebraic topology, Ax ≤ b defines a convex polyhedron which contains all feasible solutions of the integer program. H. Weyl proved in 1935 that a convex polyhedron can be defined as the intersection of a finite number of half-spaces or as the convex hull combined with the conical hull of a finite number of vectors or points. If the problem is formulated in rational  numbers, Weyl’s theorem implies the existence of a finite system of linear inequalities whose solution set coincides with the convex hull of our solution space S , also written as conv ( S ) . This directly leads to cutting plane algorithms for solving integer programming problems that will be described in section 2.3.</p>
         <p>2.1.2 Relaxations A key concept of integer programming is that of problem relaxation. A relaxation of an optimization problem as stated in equation (2.1) is an optimization problem</p>
         <p>where S ⊆ S R and c T x ≤ c T R x for all x ∈ S . The relaxed solution space is a superset of the problem solution space, and the relaxed objective function is equal to or greater than the original function for all feasible solutions of the given problem. A common relaxation for linear integer programming problems is the linear programming relaxation ( LP relaxation ). The integer constraints on all variables are removed and the problem can then be solved with linear programming methods. The most common algorithm for solving linear programs is the simplex method invented by George Bernard Dantzig in 1947. There are instances where the simplex method requires an exponential number of steps, but those problems seem to be highly unlikely in practical applications where the simplex method achieves very good performance. Khachian’s ellipsoid algorithm [ <xref id="XR82" ref-type="bibr" rid="R22">22</xref>] proved that linear programming was polynomial in 1979. Karmarkar’s interior-point method [<xref id="XR83" ref-type="bibr" rid="R20">20</xref>] was both a practical and theoretical improvement over the ellipsoid algorithm. Branch and Bound is a class of exact algorithms for various optimization problems, especially integer programming problems and combinatorial optimization problems (COP). It partitions the solution space into smaller subproblems that can be solved independently ( branching ). Bounding discards subproblems that cannot contain the optimal solution, thus decreasing the size of the solution space. Branch and Bound was first proposed by Land and Doig in 1960 [<xref id="XR86" ref-type="bibr" rid="R25">25</xref>] for solving integer programs. Given a maximization problem as described in equations (2.1) and (2.2), a Branch and Bound algorithm iteratively partitions the solution space S , for example by branching on binary variables - fixing one of them to 0 in one branch and to 1 in the other branch. For each subproblem an upper bound on the objective value is calculated. The upper bound is guaranteed to be equal to or greater than the optimal solution for this subproblem. When a feasible solution (i.e., no fractional variables remaining) is found, all subproblems whose upper bounds are lower than this solution’s objective value can be discarded. The best known feasible solution represents a lower bound for all subproblems, and only subproblems with an upper bound greater than the global lower bound have to be considered. Discarding a subproblem is called fathoming or pruning . Upper bounds for a subproblem can be obtained by relaxing the subproblem, thus they are often obtained by optimizing the subproblem’s LP relaxation. <xref id="XR87" ref-type="fig" rid="F2.1">Figure 2.1</xref> summarizes the above steps using a pseudo-code notation. The sequence of subproblems created by branching can be organized as a rooted directed graph. The original</p>
         <p>2.2 Branch and Bound</p>
         <p>1. Initialize list of all subproblems C = { S } 2. Generate a feasible solution and store it in s . It is not necessary to generate a feasible solution (e.g. by heuristics), but it can help to reduce the search tree size. When no initial solution is provided, the objective value for s is set to −∞ . 3. Repeat while C = ∅ : (a) Take a subproblem S from C (b) Relax S and solve the relaxed problem (c) Decide to branch or prune as explained in <xref id="XR90" ref-type="fig" rid="F2">figure 2.2</xref>. 4. Return s</p>
         <fig id="F2.1">
            <caption>
               <p>Figure 2.1: Branch and Bound pseudo-code</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>problem is the root node with edges going to each of its children. This graph is called the search tree and its nodes represent all generated subproblems .  As in section 2.1, we will consider a binary integer programming problem, its mathematical formulation is stated in equations (2.1) and (2.2). The fundamental concept used for cutting plane algorithms is that of a valid inequality . An inequality</p>
         <p>2.3 Cutting Plane Algorithms</p>
         <p>is valid if πx ≤ π 0 for all x ∈ S , where S contains all feasible solutions of the IP. The basic idea of cutting planes is to describe the convex hull conv ( S ) of the original problem by adding valid inequalities to the LP relaxation until the LP solution becomes feasible for the original problem. Mitchell [<xref id="XR100" ref-type="bibr" rid="R30">30</xref>] outlines the following structure of a cutting plane algorithm:</p>
         <p>1. Solve the LP relaxation using linear programming methods such as the simplex algorithm. 2. If the LP solution is feasible for the integral problem, return the optimal solution. 3. Otherwise add cutting planes to the relaxation that separate the LP solution from the convex hull of feasible integral points. 4. Go to first step.</p>
         <p>Cutting planes can be generated with or without problem specific knowledge. One method of obtaining cutting planes is by combining inequalities from the current LP relaxation. This is known as integer rounding , and the resulting cutting planes are called Chvátal-Gomory cutting planes [<xref id="XR103" ref-type="bibr" rid="R15">15</xref>, <xref id="XR104" ref-type="bibr" rid="R16">16</xref>, <xref id="XR105" ref-type="bibr" rid="R7">7</xref>]. The following example is taken from [<xref id="XR106" ref-type="bibr" rid="R30">30</xref>]. Consider the integer programming problem</p>
         <p>Depending on the solution of the relaxed problem, do one of the following: 1. No solution was found, the relaxed problem is infeasible. Then there is also no feasible solution in S , thus the subproblem is pruned. 2. The optimal solution is not better than s . The subproblem can be pruned because its upper bound is lower than the global lower bound. 3. The optimal solution is better than s and it is in S (the integer constraints are satisfied). Replace s with the new optimal solution. The subtree can be pruned because no better solution can be found. 4. The optimal solution is better than s but it is not in S (at least one integer constraint is violated). In this case S is partitioned into n smaller subproblems such that: n i =1 S i = S . Each of these children of S is added to C . This is the common case and is usually called branching .</p>
         <fig id="F2.2">
            <caption>
               <p>Figure 2.2: Deciding to branch or prune</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>A cutting plane is obtained by a weighted combination of inequalities, e.g.</p>
         <p>gives the valid inequality</p>
         <p>This inequality is valid for all LP relaxations, but in a feasible solution, the left hand side must be an integer value. This leads to the inequality</p>
         <p>Gomory’s cutting plane algorithm will find the optimal solution by iterating the steps as described above. However, the number of steps to describe the convex hull (called the Chvátal rank ) is typically very high, leading to very slow convergence [<xref id="XR126" ref-type="bibr" rid="R10">10</xref>, <xref id="XR127" ref-type="bibr" rid="R11">11</xref>]. It can be enhanced using techniques like adding many Chvátal-Gomory cuts at once, as shown in [<xref id="XR128" ref-type="bibr" rid="R1">1</xref>] and [<xref id="XR129" ref-type="bibr" rid="R5">5</xref>]. Another approach is to combine cutting plane methods with Branch and Bound, which leads to a method called Branch and Cut .</p>
         <p>1. Initialize candidate list C = { S } 2. Generate a feasible solution and store it in s 3. Repeat while C = ∅ : (a) Take a subproblem S from C (b) Relax S and solve the relaxed problem, store LP result in s (c) Repeat: (1) Try to add cuts to the relaxed problem that are violated by s (2) Exit loop when no new cuts were generated in step 1 (3) Solve relaxed problem again, store LP result in s . Note that the objective value of s is monotonically decreasing since the added cuts render infeasible the previous LP results. (d) Depending on s , decide to branch or prune the node as shown in <xref id="XR132" ref-type="fig" rid="F2">figure 2.2</xref>. 4. Return s</p>
         <fig id="F2.3">
            <caption>
               <p>Figure 2.3: Branch and Cut pseudo-code</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>2.4 Branch and Cut</p>
         <p>Branch and Cut methods use Branch and Bound to partition the solution space into smaller subproblems, but also utilize cutting plane methods to tighten the relaxation and thus to reduce the size of the search tree. Branch and Cut was first proposed by Padberg and Rinaldi [<xref id="XR136" ref-type="bibr" rid="R31">31</xref>] as a framework for solving traveling salesman problems. The purpose of cutting planes or cuts is to reduce the upper bound derived from the optimal solution of the LP relaxation. A smaller upper bound makes pruning the subproblem more likely, thus reducing the search tree size. When the algorithm failed to generate new cuts that are violated by the current LP solution, the subproblem is branched as in Branch and Bound. As cut generation can be very expensive, it is common to generate cuts only for some nodes in the search tree. For example, it might be reasonable to generate cuts for every eighth node or for all nodes at a depth of a multiple of eight. The cut-and-branch variant adds cutting planes only at the root node. A pseudo-code formulation of Branch and Cut is given in <xref id="XR137" ref-type="fig" rid="F2">figure 2.3</xref>.</p>
         <p>Chapter 3</p>
      </sec>
      <sec>
         <title>Local Branching</title>
         <p>While there exist sophisticated solvers for integer programming problems, for many hard problems the optimal solution is often hard to find within a reasonable time. Therefore, it becomes increasingly important to find reasonably good solutions early in the computation process. Local Branching is a local search meta-heuristic for integer programs proposed by Fischetti and Lodi in 2002 [ <xref id="XR142" ref-type="bibr" rid="R13">13</xref>] that is entirely embedded in a Branch and Cut framework. Its goal is to improve the heuristic behavior of a given MIP solver without losing optimality, that is, to find good feasible solutions as soon as possible while still being able to find the global optimum and prove its optimality. Local Branching works by partitioning the search tree through so-called local branching cuts . Since those local cuts are just specific constraints for integer programming problems, they can be expressed like normal IP constraints using any generic MIP solver. A common technique for IP heuristics is hard variable fixing. For example, a heuristic might use a LP solver to compute a continuous optimal solution, heuristically fix some variables to integer values (e.g. by rounding the variable with the least fractional value), and then repeating these steps for the resulting subproblem without the fixed variables. This way, relatively good (but probably not optimal) solutions may be found in reasonable time even for hard problems. The major downside of this approach is that it may be nearly impossible during the early stages to decide which variable should be fixed. This inevitably leads to bad fixings which may not be detected until much later, requiring some kind of backtracking to undo bad choices. To overcome this limitation of variable fixing, Fischetti and Lodi proposed soft variable fixing . It does not select a single variable for fixing, but only specifies that a certain percentage of all variables of a given feasible solution should be fixed. This approach is best illustrated using the binary integer programming problem described in section 2.1. Supposing there is a feasible solution and 90% of its nonzero variables should be fixed to 1, Fischetti and Lodi add a soft fixing constraint n n</p>
         <p>3.1 Soft vs. Hard Variable Fixing</p>
         <p>to the current formulation. x j represents the feasible solution around which a local neigh-</p>
         <p>borhood is isolated, i.e. in any feasible solution x j only 10% of those variables set to 1 in x j may be flipped to 0. The idea is that fixing 90% of the variables helps the solver to find good solutions as effectively as when fixing a large number of variables, but with a much larger degree of freedom.  Given a binary integer programming problem as stated in section 2.1 and a feasible solution x , the binary support S  ̄ is defined as S  ̄ := { j ∈ [1 , n ] : x j = 1 } , i.e. the indices of those variables that are set to 1. A soft fixing constraint in terms of the previous section can then be formulated as</p>
         <p>3.2 A Basic Local Branching Framework</p>
         <p>Fischetti and Lodi call this a local branching constraint that counts all binary variables that flipped their value from zero to one or from one to zero compared to x . ∆( x, x ) actually represents the Hamming distance between x and x , thus the constraint is also called Hamming distance constraint . When the cardinality of S  ̄ is fixed, this constraint is equivalent to</p>
         <p>because for every variable x j with j ∈ S  ̄ that flips from one to zero another variable must flip from zero to one. This definition is consistent with the classical k’-opt neighborhood for the Traveling Salesman Problem, where at most k edges may be replaced. A local branching constraint partitions the search tree in two disjunct branches</p>
         <p>The local branch is completely solved before continuing with the normal branch. When a new global optimum x 2 was found in the local branch, local branching can continue with the new solution by adding a new constraint to the remaining “normal” branch, again partitioning the search tree in two disjunct branches</p>
         <p>This scheme works as long as the local branching trees yield new global optima and is illustrated in <xref id="XR164" ref-type="fig" rid="F3">figure 3.2</xref>. The numbers indicate the sequence in which the subproblems are generated and processed. The actual optimization problems are solved by a generic MIP solver. The size of the local subtrees at positions 2, 4 and 6 depend on the choice of the k parameter. Small values of k define a relatively small neighborhood that is easier to solve, but may not contain solutions that are significantly better than the current one. Larger values of k offer higher degrees of freedom during the tree search, but drastically increase the size of the local branching trees.</p>
         <p>1 ∆( x, x 1 ) ≤ k   d ∆( x, x 1 ) &gt; k   d   d   d   d     d d 2 3 MIP ∆( x, x 2 ) ≤ k   d ∆( x, x 2 ) &gt; k   d solver   d improved solution x 2   d   d     d d 4 5 MIP ∆( x, x 3 ) ≤ k   d ∆( x, x 3 ) &gt; k   d solver   d improved solution x 3   d   d     d d 6 7 MIP MIP solver solver no improved solution</p>
         <fig id="F3.1">
            <caption>
               <p>Figure 3.1: Local Branching</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>3.3 Local Branching Extensions</p>
         <p>Fischetti and Lodi [<xref id="XR171" ref-type="bibr" rid="R13">13</xref>] proposed several extensions to the standard local branching algorithm described in the previous section.</p>
         <p>• Imposing a time limit on local branching trees allows to use large values of k without having to explore a local tree completely. When time runs out and a better solution was found in the local tree, the algorithm creates a new local tree at the original root node using the new solution. However, since the previous local tree was not explored completely, this may lead to a duplication of effort as the optimal solution might still be in the first local tree, and its search space can therefore not be excluded. If the time limit is reached without finding a new better solution, k is decreased to speed up the exploration of the local tree. • Diversification may be used when a local tree did not improve the best known solution. Fischetti and Lodi suggest to start with a soft diversification by enlarging the neighborhood, e.g. by k/ 2 . When no better solution is found in this larger local tree, they apply a strong diversification by taking another (worse) solution and restarting local branching with this solution. • Embedding local branching in heuristic frameworks like Tabu Search , Variable Neighborhood Search , Simulated Annealing or Evolutionary Algorithms can be easily done, since local branching naturally defines a custom sized neighborhood around a given solution. Additional constraints imposed by the heuristic framework can be described as linear cuts which makes them easy to join with local branching constraints. • Working with infeasible solutions is necessary for problems where finding an initial feasible solution is hard, e.g. for hard set partitioning models. In order to use an infeasible solution as initial solution for local branching, one may define additional slack variables for some of the constraints while penalizing them in the objective function. • General integer variables require a new definition of the local branching constraint. Some general integer problems still have a relevant subset of 0-1 variables that can be used for local branching. In case there are no relevant binary variables, introducing weights leads to a viable local branching constraint. In a MIP model that involves the bounds l j ≤ x j ≤ u j for j = 1 . . . n , a local branching constraint can be defined as ∆( x, x ) := μ j ( x j − l j ) + μ j ( u j − x j ) + μ j ( x + j + x j − ) ≤ k. j : x j = l j j : x j = u j j : l j &lt; x j &lt;u j (3.6) The weights are defined as μ j = 1 / ( u j − l j ) , while x j + and x − j define additional slack variables that satisfy the equation x j = x j + x j + − x − j , x + j ≥ 0 , x j − ≥ 0 .</p>
         <p>Of course, there are other possibilities to improve the standard local branching algorithm not proposed by Fischetti and Lodi. The following enhancements have been integrated in our local branching framework as described in chapter 4.</p>
         <p>• Fixing variables allows to tighten the neighborhood when the original local tree was too large to be explored completely. Variables that share the same value in the incumbent solution and in the solution of the LP relaxation are less likely to change in the global optimum. By fixing some of those variables in the local tree and adding a corresponding cut to the remaining tree local branching can avoid calculating parts of the tree that will probably yield no better results. This approach was proposed by Danna et al. [<xref id="XR176" ref-type="bibr" rid="R8">8</xref>] and is known as RINS (Relaxation Induced Neighborhood Search). • Concurrent exploration of different local trees provides diversification by creating several local trees from different feasible solutions and exploring them simultaneously.</p>
         <p>Chapter 4</p>
      </sec>
      <sec>
         <title>An Advanced Local Branching Framework</title>
         <p>In this chapter a generic framework for local branching is described. Standard local branching is implemented as described in chapter 3, and several extensions are introduced to improve its performance. The main goal of this framework is to provide a local search algorithm for higher-level metaheuristics, for example evolutionary algorithms. These metaheuristics can use local branching for exploring the neighborhood of certain solutions, and use the generated feasible solutions as input for their own improvement algorithms. The actual implementation of the framework will be described in chapter 6.  A sequential version of the standard local branching algorithm provides the basis for the framework. It is capable of using local branching to completely solve a problem without further user intervention. The main phases of the local branching algorithm are:</p>
         <p>4.1 Basic Functionality</p>
         <p>1. Generate an initial solution for the first tree . 2. Initialize the first local tree using the previously generated solution. 3. Repeat: (a) Completely solve the local tree . (b) When the local search terminates: • A better feasible solution was found in this local tree: create a new local tree using the improved solution from this local tree as initial solution. • No better feasible solution was found : abort local branching. 4. Solve the rest of the search tree . 5. Return optimal solution .</p>
         <p>The initial solution can be created by a custom heuristic, or it is derived from the optimum of the root node’s LP relaxation (e.g. by rounding or truncating the LP solution). The default implementation uses local branching constraints as described in section 3, i.e. Hamming  distance constraints defining a neighborhood around the solution according to the distance parameter k . Other constraints for branching might be implemented by the user as well. 4.1.1 Limitations The standard local branching algorithm works well for some instances, but has several limitations:</p>
         <p>1. Depending on the number of variables and the value of k , the Hamming distance constraint possibly defines a very large neighborhood. Given a binary IP with n variables, a feasible solution has n k = ( n − n k ! )! k ! neighbors with a Hamming distance of k (the local tree includes all neighbors with a Hamming distance not larger than k , so the actual search tree is even larger). 2. Depending on the specific problem, there might be more than one reasonable initial solution for local branching. When a given heuristic returns several promising solutions that would create (partially) disjunct local trees (i.e. their Hamming distance is greater than k ), only one neighborhood can be explored. 3. While a local branching constraint defines a neighborhood around a feasible solution, it provides no further guidance for exploring this neighborhood besides the standard branch and cut strategies (e.g. best bound first search). Other local search heuristics might help to tighten the search tree.</p>
         <p>Preliminary testing with multidimensional knapsack problems confirmed these shortcomings. The test instances contain integer programming problems with 100 to 500 variables and 5 to 30 constraints. Detailed results will be discussed in chapter 9. The larger test instances ( n ≥ 250 , d ≥ 10 ) contain too many variables for using k values larger than approximately 5 . This allows at most five variables to flip their values, and likely prohibits significant improvements to the initial solution of a local tree. For any value k &gt; 5 even the local tree defines a subproblem that is often too complex to be solved completely within the given time limit. Additionally, the first initial solution is either derived by a first fit heuristic or by rounding the first LP solution, and both are unlikely to be in a small neighborhood of the optimal solution.  In order to address the shortcomings described in the previous section, three extensions have been added to the standard local branching algorithm. The first one eliminates the restric- tion of sequential execution by allowing to create new local trees before the previous one(s) are finished. On a related issue, the second extension allows to abort local trees before they are completely solved. The last extension tries to reduce subproblem complexity by fixing variables that are less likely to change in the optimal result than others. 4.2.1 Using Multiple Local Trees The first major extension to standard local branching is the support for pseudo-concurrent exploration of several local trees. It removes the burden of relying on one local tree at a time,</p>
         <p>4.2 Extending the Basic Algorithm</p>
         <p>1 ∆( x, x 1 ) ≤ k   d   d   d   d   d     d d 2 2 ∆( x, x 2 ) ≤ k   d   d   d   d   d     d d c 2 3 MIP ' solver ...</p>
         <fig id="F4.1">
            <caption>
               <p>Figure 4.1: Pseudo-concurrent local tree exploration using a single MIP solver instance</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>probably leading to a more robust search tree exploration less likely to be caught in local optima of the objective function. The framework provides a method to spawn any number of local branching trees simply by providing an initial solution. However, these trees are not parallelized in the sense of a separate process dedicated to a single local tree. Instead, there is still a single pool for subproblems where all local tree nodes are stored. Depending on the chosen tree search strategy, some trees will probably get more time than others with less promising nodes. For example, when a best bound first tree search is performed, a local tree whose nodes have relatively poor upper bounds will get less time than a tree with more promising nodes. Unfortunately this kind of pseudo-concurrent exploration likely leads to a duplication of effort in some cases since it is not known a priori which trees will actually be completely solved, and thus, inverse local branching constraints cannot be considered. When a local tree is prematurely terminated, no information about this local tree (except feasible solutions found so far) can be further utilized: the neighborhood defined by this tree cannot be excluded from future local trees because it still may contain the optimal solution. The framework achieves parallel exploration by a simple modification to the standard local branching algorithm: before a local tree is not completely solved, the inverse local branching constraint for the rest of the search tree remains inactive. When a local tree is prematurely terminated, the inverse constraint is removed from all future local trees. <xref id="XR197" ref-type="fig" rid="F4.2.1">Figure 4.2.1</xref> shows the modified version of the original algorithm previously shown in <xref id="XR198" ref-type="fig" rid="F3">figure 3.2</xref>. All parts of the search tree labeled with the same number are parallelized, thus the inverse constraints on the right hand side are missing. The initial solutions x 1 and x 2 are not necessarily related; they are supplied by the higher-level metaheuristic. 4.2.2 Aborting Local Trees Aborting local trees greatly enhances control over local branching. It becomes possible to impose time or node limits on local trees, abort stagnating local trees in favor of more promising ones, or simply restart local branching with new solutions (possibly generated outside the framework.) When a local tree is aborted, the inverse local branching constraint (defining the search tree outside the local tree) must be removed. This also applies to the variable fixing constraint introduced in the next section. Besides that, the only issue is to find some criteria for premature local tree termination. Specifying a time limit is rather complex due to the distributed design of COIN/BCP. Different machines may have different performance ratings, and it would require non-trivial extensions to COIN to track the time spent for each subproblem, potentially across several LP processes, cut generators, and variable generators. The local branching framework offers extensive information about the number of nodes processed per local tree instead. Using these facilities, it is easy to impose a node limit on a local tree, or to specify a maximum number of nodes that can be processed in a single local tree before an improvement is found. The downside of this approach is that the number of nodes that can be processed in a given CPU timeslice depends on the complexity of the IP problem. Thus node limits may have to be set heuristically, for example by some constant value multiplied with a weighted sum of the number of variables and the number of constraints. 4.2.3 Tightening the Search Tree by Variable Fixing The variable fixing extension to local branching is based on Relaxation Induced Neighborhood Search (RINS) proposed by Danna et al. [<xref id="XR202" ref-type="bibr" rid="R8">8</xref>]. The underlying assumption is that variables having the same integer value in the incumbent solution and in the LP relaxation are likely to be set to their optimal value. By fixing some of those variables to their current value, the local search focuses attention on the fractional variables. Compared with reducing search tree size by reducing the value of k , variable fixing gives more freedom to the exploration of the more promising fractional variables while ignoring the allegedly less promising integral variables of the current LP optimum. Choosing the best variables to be fixed is a problem by itself. The framework picks a random selection from the set of all variables having the same integer values in the integral and the LP solution. The number of fixed variables is given relative to the total number of variables in the (sub-)problem. In the following, let F 1 denote the indices of variables fixed to one, and F 0 the indices of variables fixed to zero. While fixing variables in the local tree can be done directly in the MIP solver, the inverse constraint is a bit more complicated: a node becomes feasible when at least one of the fixed variables changed its value. Ignoring the local branching constraint, this can be achieved through a new row cut of the form</p>
         <p>When using Hamming distance cuts for local branching both constraints can be combined to a single cut. A solution is feasible outside the local tree when either the Hamming distance is greater than k , or at least one variable flipped. In other words, when a variable flips (and  the above inequality becomes valid), the Hamming distance constraint should be considered irrelevant. The following row cut achieves these goals:</p>
         <p>where ∆( x, x ) denotes the Hamming distance between the initial solution x and the current solution x as defined in equation (3.2). When one of those fixed variables flips, the right hand side will be less or equal to zero. Since the Hamming distance is always greater or equal to zero, the constraint is satisfied (even if the Hamming distance is smaller than k ).  4.2.4 Utilizing the Extensions When developing a local branching metaheuristic, most often a combination of the extensions described above works best. For example, it is apparent that when aborting local trees, one may also change the local branching parameters for the value of k and the number of variables to be fixed. When using multiple trees, it may be reasonable to start different trees with different local branching parameters. The combination of variable fixing and node limits on local trees is a straight-forward way of tightening the search tree as the global solution improves. For example, the search may be started with rather weak constraints, i.e. a relatively high value of k and no or little variable fixing. When the local tree fails to yield new solutions in a given node limit, it is aborted and the parameters are modified. For example, the value of k is decreased or the number of variables to be fixed is increased. Then a new local tree can be created, using the new parameters and the last improved solution from the previous tree (or even the initial solution of the previous tree, since the tightening might lead to a faster convergence towards an improved solution). See chapter 8 for a sample implementation of this tightening scheme.</p>
         <p>Chapter 5</p>
      </sec>
      <sec>
         <title>COIN/BCP</title>
         <p>The implementation of the local branching framework is based on the Branch and Cut and Price framework (BCP) that is part of the Computational Infrastructure for Operations Research (COIN) project [ <xref id="XR216" ref-type="bibr" rid="R28">28</xref>]. By augmenting an existing Branch and Cut framework reimplementation of a MIP solver is avoided. Furthermore, developers familiar with COIN/BCP can easily use the framework. 5.1.1 History As research in combinatorial optimization advanced tremendously over the past decades, developers faced increasing complexity when trying to implement efficient versions of their algorithms. While standard algorithms like Branch and Cut exist, problem-dependent algorithms often required custom implementations due to problem-dependent methods like variable or cut generation. In the early 1990s a research group was founded with the goal of providing developers a generic software framework which could be adapted to specific problems. This led to the release of COMPSys (Combinatorial Optimization Multi-processing System) [<xref id="XR219" ref-type="bibr" rid="R12">12</xref>]. After several revisions this project became SYMPHONY (Single- or Multi-Process Optimization over Networks). In 1998, a reimplementation in C++ was started at IBM research. As a result, the COIN project was publicly announced in the first half of 2000, including a generic Branch, Cut and Price framework codenamed COIN/BCP . IBM guaranteed to support the online infrastructure for the COIN project for three years, including the website at <ext-link ext-link-type="uri" href="http://www.coin-or.org">http://www.coin-or.org</ext-link> , several mailing lists and the source code repository. Much of the initial development was done by IBM researchers, but in the past years the spirit of open source has picked up and has led to various contributions by external researchers.</p>
         <p>5.1 COIN Overview</p>
         <p>5.1.2 Components Our framework uses the following components of the COIN project: • BCP : the Branch, Cut and Price framework used for solving MIPs. • OSI : the Open Solver Interface, a standardized API for calling math programming solvers. It is used by BCP to call a simplex solver for solving the LP relaxations. A</p>
         <p>wide variety of solvers is supported, most notably the free COIN/CLP and the commercial CPLEX MIP solvers. • CLP : COIN LP, the COIN project’s LP solver. This is a free implementation of a simplex solver. • CGL : A Cut Generator Library for generating standard cuts for IP problems like Gomory cuts or knapsack cover cuts. 5.2 Design of COIN/BCP</p>
         <p>The following introduction to the design of COIN/BCP is based on the user manual by Ralphs and Ladányi [ <xref id="XR225" ref-type="bibr" rid="R36">36</xref>]. The major design goals for COIN/BCP are portability, efficiency and ease of use. It provides a black-box design with a clean end-user interface that keeps most of the actual implementation hidden from the user. COIN/BCP was developed using an object-oriented approach. The central objects are cuts and variables that can be used as base classes for user-defined objects. Additionally, user objects provide methods that can be re-implemented to alter specific aspects of the algorithm, like tightening variables or adding cuts. While this approach enables a straight-forward implementation for many combinatorial optimization tasks, there is still enough flexibility left even for implementing complex meta-algorithms like local branching with little or no changes of the COIN/BCP code. 5.2.1 Variables and Cuts Since search trees can easily contain hundreds of thousands of nodes, a simple object-oriented approach storing the variables and cuts for each node in objects leads to excessive memory consumption. COIN/BCP tries to reduce memory usage by keeping the number of active variables and cuts (the active set ) as small as possible by using data structures that make it possible to move objects in and out of the active set efficiently. This is accomplished by maintaining an abstract representation of each global object that keeps information about adding or removing it from a particular problem instance (i.e. a particular LP relaxation). In other words, a variable does not represent a specific column of a LP relaxation, but is an abstract object that can be realized as a specific column of a LP relaxation. Similarly, a cut does not describe a specific row of a LP relaxation, but it contains an abstract cut that can be realized as a row in a LP relaxation. COIN/BCP distinguishes between two groups of cuts and variables: so-called core cuts and core variables that are active in all subproblems, and extra cuts and extra variables that can be added and removed dynamically. Extra cuts help to reduce the active set, but require additional bookkeeping when adding or removing them from the formulation. There are two different types of extra cuts:</p>
         <p>• Indexed cuts are represented by a unique index value. The user must be able to generate the corresponding row cut when given the index number by using some kind of a virtual global list known only to the user. This is the most efficient way of representing extra cuts in a formulation and is particularly useful when the number of cuts is very high and most likely only few are violated by any feasible solution. Using indexed cuts, only</p>
         <p>constraints that are violated by a given LP solution have to be realized as rows in the LP relaxation. The downside is the extra bookkeeping involved for adding and removing those cuts, and the user must find an enumeration scheme when using indexed cuts. • Algorithmic cuts give the user absolute freedom, especially in the case when the number of cuts is not known a priori and the cuts cannot be enumerated. The only requirement is that the user must be able to generate the corresponding row when given a set of active variables by some sort of algorithm. The downside, as with indexed cuts, is the fair amount of bookkeeping involved for creating and removing algorithmic cuts.</p>
         <p>Indexed and algorithmic variables work in a similar way. Indexed variables are generated by the user when given an index number. They are useful when given a big number of variables while most likely only few of them would be different from zero. Adding all variables to the core matrix would increase the problem complexity enormously. Similar to algorithmic cuts, algorithmic variables can describe any user-defined constraint that is violated by a given LP solution. Indexed and algorithmic variables are also essential for column generation algorithms that generate variables during the computation.  COIN/BCP is grouped into four independent modules. They communicate using a message- passing protocol which is defined in a separate API. Thus they are well equipped for parallel execution, even on separate machines connected by a network. 5.3.1 The Tree Manager Module The tree manager ( TM ) module represents the master process of the computation algorithm. It is responsible for problem initialization and controls the other modules. There is only one tree manager for any computation. Its main functions are:</p>
         <p>5.3 COIN/BCP modules</p>
         <p>• Reading parameters and the problem instance from the command line or from a file. • Constructing the root node of the search tree. • Beginning the computation by creating LP processes (see below) to solve individual nodes of the search tree. • Receiving new solutions from the child LP modules and storing the best one. • Receiving new subproblems and storing them for later processing. • Pruning subproblems based on the global upper bound. • Sending stored subproblems to idle LP processes. • Printing the final result when the computation has finished (i.e. all subprocesses are in an idle state and all subproblems have been solved.)</p>
         <p>5.3.2 The Linear Programming Module The linear programming ( LP ) module performs the actual computation, i.e. the bounding and branching operations. Its main functionality includes:</p>
         <p>• Requesting new subproblems from the tree manager. • Receiving and processing subproblems. • Choosing branching objects and sending the resulting subproblems back to the tree manager.</p>
         <p>5.3.3 The Cut Generator Module Since cut generation may be computationally expensive, it can be performed inside a separate cut generator module. It receives a LP solution by a LP process, tries to generate valid inequalities violated by this solution, and sends the cuts back to the LP solver. Then it remains in an idle state until a new solution is sent to the cut generator.  5.3.4 The Variable Generator Module Similar to the cut generator module, the variable generator module’s only responsibility is to generate variables for a given LP solution. If any variables are generated, they are sent back to the requesting LP process and the variable generator module keeps waiting for new solutions. For a better understanding of our local branching extensions, a deeper explanation of the linear programming module is required. The LP module uses a LP engine for finding upper bounds, generates cuts and variables when necessary and performs branching operations.</p>
         <p>5.4 The Linear Programming Module</p>
         <p>5.4.1 The LP Engine The LP module uses the Open Solver Interface (OSI) in order to communicate with third-party LP libraries or LP engines .</p>
         <p>5.4.2 Managing the LP Relaxation The LP module is responsible for managing extra variables and cuts. It does so by maintaining a local cut pool where any generated extra cuts are stored. In each iteration, up to a specified number of the strongest cuts are added to the problem. A cut’s strongness corresponds to the degree of violation in the current LP solution. Cuts that proved ineffective over a specified number of iterations are purged from the cut pool. Variables can be tightened by user-defined methods before solving the LP.  5.4.3 Branching Branching is performed when no new cuts were generated or the user forces branching. A branching object describing all of the new cuts and variables and their corresponding bounds is generated and sent back to the tree manager. The branching operation can be based on cuts or on variables. Optionally strong branching can be performed. In strong branching, several branching objects are created and then pre-solved, i.e. quickly optimized in a probably non- optimal way. The most promising candidate, based on some internal rule (e.g. best objective value) or on the user’s decision, is used for branching. By default, COIN/BCP uses branching on fractional variables. One (standard branching) or several (strong branching) fractional variables are selected and corresponding branching objects are generated. The selection of variables can be user-defined or based on some internal rule, COIN/BCP allows to specify the number of the most fractional variables (i.e. those nearest to 0.5 in a binary optimization problem) and the number of those variables close to one to be selected for (strong) branching. When the total number of variables is greater than one, strong branching has to be enabled. Since Branch and Cut methods can heavily benefit from parallelization one design goal of COIN/BCP was that of parallelization. There are two main sources of parallelism: Obviously, each subproblem of the candidate list can be solved independently from the others. This can be accomplished by spawning more than one instance of the LP module, either on one machine (reasonable for multi-processor systems) or on a cluster of computers connected by a network. The second source lies within the processing of a single subproblem: the individual tasks can be parallelized with the LP solver, which means a node can be completely processed in roughly the time it takes the LP engine to solve the relaxation. This is the reason that the potentially expensive cut and variable generators are placed in separate modules outside the LP module. In COIN/BCP, the architecture is based on a master/slave model. The tree manager assumes the role of the master process in control of slave processes that execute its orders. The tree manager is responsible for spawning at least one process of each type (LP, cut generation, variable generation) and keeping them busy until all subproblems are solved. 5.5.1 Inter-Process Communication COIN/BCP allows the user to choose between sequential and parallel execution. It is based on an abstract message passing protocol with parallel and sequential implementations. The former is an interface to the Parallel Virtual Machine (PVM) protocol, the latter emulates a parallel machine that effectively executes the algorithm sequentially inside a single process. Support for other communication frameworks can be added by implementing the abstract base class of COIN/BCP’s messaging framework. One instance of an object in memory can never be shared between different modules since these modules might be executed in different processes or on different machines. Instead, objects can be packed and unpacked into a COIN-specific buffer class (BCP buffer). The messaging framework uses these buffers for communication between modules, thus it is possible to transmit user-defined objects by implementing methods for packing and unpacking objects of these types. 5.5.2 Fault Tolerance Using distributed computation, fault tolerance becomes important because a single crashed machine should not cause the termination of the whole program. For this purpose, the tree manager tracks all processes and restarts them as necessary. When a process is lost, the subproblems assigned to this process are reassigned to other processes. Additionally, new machines can be added to the distributed network on the fly without having to restart the computation. This section gives a brief overview of the basic steps for developing an application with COIN/BCP. It focuses on the parts that will be modified in our local branching framework, a more complete description can be found in the COIN/BCP user manual [<xref id="XR254" ref-type="bibr" rid="R36">36</xref>]. Developing an application for a specific problem basically means subclassing some of COIN/BCP’s provided classes, implementing some abstract methods and overriding others to diverge from default behavior. The main classes designed for derivation by the user are:</p>
         <p>5.5 Parallelizing COIN/BCP</p>
         <p>5.6 Developing Applications with COIN/BCP</p>
         <p>• BCP lp user : The user-defined LP module extension. By subclassing it it is possible to modify the LP module’s decisions (e.g. whether to branch or generate cuts) and to store problem-specific data. One object of this type is generated for each LP process. • BCP tm user : The user-defined tree manager extension. It is embedded into the tree manager module and is responsible among other things for initializing the problem and deciding on the tree search strategy. • BCP vg user is used for implementing user-defined variable generators. • BCP cg user provides a possibility to generate cuts inside a separate process. • USER initialize : This class is used for instantiating objects of the derived BCP xx user classes. • BCP cut : This abstract base class is used for describing cuts, allowing the user to derive problem-specific cut classes. The subclasses for core, indexed and algorithmic cuts are derived from this class. • BCP var : This class can be subclassed for describing user-defined variable types. Sub- classes for core, indexed and algorithmic variables are already defined. • BCP solution : The abstract base class for describing feasible solutions. A generic implementation exists (BCP solution generic).</p>
         <p>In the BCP tm user and BCP lp user classes there are some key methods that are of great importance for the implementation of our local branching framework. For a complete description of these classes, refer to the autogenerated documentation and the user’s manual [<xref id="XR257" ref-type="bibr" rid="R36">36</xref>].</p>
         <p>5.6.1 The BCP tm user Class • pack module data() : This method is invoked to pack the data needed to start the computation in other modules. This can be used for sending problem-specific data (e.g. local branching parameters) to the LP module. • unpack feasible solution() : This method is called when the tree manager received a new feasible solution. By overriding this method the user gets every feasible solution found by a LP module, which can be used for further enhancements (e.g. crossover between two or more solutions when using a genetic algorithm). • initialize core() and create root() are used for initializing the problem (probably by reading it from a file) and setting up the root node of the search tree. • compare tree nodes() : This method is essential for the tree search strategy. It is invoked by the tree manager when a new tree node was received which will be inserted in the candidate queue. By overriding this method it is possible to use an arbitrary tree search strategy, we use this to calculate local tree nodes before any other tree nodes. The standard implementation can be configured by a parameter to perform either a depth first, breadth first, or best bound first tree search.</p>
      </sec>
      <sec>
         <title>The candidate queue</title>
         <p>The tree manager keeps a list of all unprocessed subproblems in a single candidate queue . This is also known as a single-pool BCP algorithm . The candidate queue is implemented in the BCP node queue class as a heap-based priority queue. When the tree manager receives new subproblems from one of the LP processes, the priority of the item is determined indirectly by repeatedly calling the binary compare tree nodes() function until the final position of the subproblem has been found. The subproblems remain in the candidate queue until taken out by the tree manager for an idle LP process. Since the lower bound may have increased since the subproblem was inserted into the queue, the subproblem may be pruned before it is actually sent to an LP process. In this case, the next subproblem is taken from the queue.</p>
         <p>5.6.2 The BCP lp user Class • unpack module data() : This method is the counterpart to the pack module data() method of the BCP tm user class. It receives the information sent by the tree manager and can be used to initialize problem-specific data in a LP module. • initialize solver interface() can be overridden to use a specific LP engine (like COIN’s own CLP solver or the commercial CPLEX solver). • initialize new search tree node() : This method is called before a new tree node is solved, providing an opportunity to tighten or fix variables. • generate heuristic solution() : When the problem’s structure allows to quickly generate feasible solutions based on the current LP solution (e.g. by clever rounding of fractional values), this method can be overridden to generate feasible solutions. By finding good solutions the search tree size can be drastically reduced.</p>
         <p>• select branching candidates() : This method decides whether to branch or not, and selects branching candidates. The default implementation uses branching on fractional variables, the local branching framework will override this method to implement local branching cuts.</p>
         <p>Chapter 6</p>
      </sec>
      <sec>
         <title>Implementation of the Framework</title>
         <p>The main intention for creating a local branching framework was to provide a clean, reliable and extendable framework for local branching metaheuristics. The main design goals of the local branching framework are:</p>
         <p>• Problem-independent functionality : The local branching framework should be usable for any binary IP problem. Therefore, it should not make problem-dependent assumptions and separate local branching logic from problem-dependent functionality. • Explicit local branching metaheuristics : It should be possible to literally write a metaheuristic function without being forced to scatter the algorithm over many COIN/BCP classes. • “Transparency” : The implementation of an algorithm for COIN/BCP should not be much different from the implementation using our local branching framework. Furthermore, existing COIN/BCP advantages such as parallelization should not be affected by local branching. • Avoid changes to COIN/BCP sourcecode : It would have been possible to embed local branching directly into the COIN/BCP source repository. However, this would tie local branching very close to COIN/BCP’s internals which are subject to change at will. Additionally, it would be much harder to integrate future bugfixes and enhancements of COIN/BCP. • Hide COIN/BCP internals : The complexity of COIN/BCP should be hidden from the local branching metaheuristic. Instead, service methods for querying the current state of local branching should be provided.</p>
         <p>These goals were met by subclassing the predefined user classes of COIN/BCP (mainly BCP lp user and BCP tm user) and embedding the local branching algorithm in those subclasses. Additionally, a local tree manager class provides handlers and parameters for controlling the flow of the local branching algorithm. This way most of the complexity and the COIN/BCP-specific implementation is hidden from the user. Ideally, enabling local branching for an existing COIN/BCP program would be done by replacing the COIN/BCP user classes with the framework’s derived classes. Of course, some additional initialization has to be performed since some classes of the framework need to be subclassed by the user again.</p>
         <p>The main classes of the framework are: • LB tm : the local branching framework’s tree manager implementation. It is responsible for managing local trees, creating and terminating local trees, and controlling the LP modules. • LB lp : the local branching framework’s LP module. It executes the tree manager’s in- structions regarding local branching, i.e. creating local cuts in the branching operations according to the given parameters (e.g. value of k ). • LB MetaHeuristic : the user’s control module. It provides methods for the user to create and terminate local trees, provides statistical data about all local trees, and handlers that are repeatedly called and are intended to be used to implement another high-level heuristic like an evolutionary algorithm. When using the framework, this is the main class the user has to care about. Unlike COIN/BCP modules, this module is not executed within its own process, but is attached to the tree manager module. • LB init : the local branching framework’s implementation of the USER initialize class. It is used to initialize instances of the tree manager (LB tm) and LP modules (LB lp). Currently LB init does not implement any initialization logic on its own, it merely exists for extension purposes. • LB cut : a simple row cut used for the Hamming distance constraint. • LocalTreeManager : an internal data manager class that is responsible for tracking all local trees, managing the found solutions for local trees, and maintaining a list of all currently active local trees. The user probably does not need to override this class except when additional data about local trees should be stored. • LocalTreeIndex : used to store information about all existing local trees. An instance of this class is shared by the LB MetaHeuristic and LocalTreeManager objects, the latter being mostly responsible for updating the information in the LocalTreeIndex, while the metaheuristic object can use the index to determine its actions (e.g. terminating a local tree that appears to be stagnating). • LocalTree : keeps information about a single local tree. Everything the tree manager (and therefore the other local branching classes) knows about a local tree is kept in a LocalTree object. For example, it contains information about the number of active nodes, the number of created nodes since the last improvement of the objective value or the current best solution found in the tree.</p>
         <p> 
            <xref id="XR274" ref-type="fig" rid="F6.1">Figure 6.1</xref> shows an UML diagram for the classes attached to the LB tm class. There are four classes that must always be subclassed for a working program. The methods to be implemented are defined by the COIN/BCP superclasses and have to be implemented anyway to get a working program (except for the LB MetaHeuristic subclass).</p>
         <p>• LB tm : pack cut algo() and unpack cut algo() must be able to pack and unpack LB cut row cuts (although other custom cut types can be supported too). initialize core() must be implemented to initialize the core matrix, ending with a call to LB tm’s version of this method to complete initialization. The user must also implement the create lbh() method to return a new instance of his implementation of the LB MetaHeuristic subclass.</p>
         <fig id="F6.1">
            <caption>
               <p>Figure 6.1: UML class diagram for the tree manager extension</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>• LB lp : pack cut algo() and unpack cut algo() have to be implemented similarly to the user’s LB tm implementation. initialize solver interface() has to create the LP engine for the LP process. generate heuristic solution() may be implemented to obtain feasible solutions from LP results (e.g. by clever rounding). • LB init : lp init() and tm init() are implemented to return new instances of the user’s LB lp and LB tm subclasses. The tm init() method is a good place to initialize the problem instance. LB tm’s initialize() method has to be called immediately after the tree manager has been created. • LB MetaHeuristic : at least initial solution() must be implemented to return the initial solution used for the very first local tree. The user probably wants to override some other methods (especially tree finished()) in order to create more local trees during the computation process. 6.1 Integrating Local Branching into COIN/BCP</p>
         <p>The main effort of implementing local branching went into the interaction between the local branching metaheuristic and the COIN Branch and Cut framework. A major goal was to exploit the existing framework as much as possible. Rewriting parts of the COIN/BCP code to implement local branching would leave the user stuck to exactly one version of COIN/BCP, without having the benefit of upcoming enhancements and bugfixes. Our implementation requires only minimal patches of existing COIN/BCP code where the original user classes did not provide the necessary flexibility. These changes are described in appendix A.  6.1.1 Identifying Local Tree Nodes COIN/BCP provides a mechanism to attach an user data object to nodes for storing individual information about each node. By creating a subclass of BCP user data and creating methods to pack and unpack objects of this subclass to a BCP buffer, the LP modules can attach any user-defined data to a single node of the search tree. The local branching framework uses this method to assign each node a LB user data object. Its main components are the node type , the unique number of its local tree if it is in a local tree and some internal information. The node type distinguishes three types of nodes:</p>
         <p>• UD LocalRoot : represents a local root node , i.e. the root node of a local tree. • UD NormalRoot : represent a normal root node , i.e. the root node for the search tree outside the local tree. • UD LocalNode : represents a node in a local tree.</p>
         <p>Note that nodes outside a local tree do not have a type, in fact they have no assigned user data objects at all since there is no extended information that might be of interest. The terms local and normal root node emerged from the way COIN/BCP handles branching: when a subproblem is branched, appropriate branching objects are created (containing the variables or cuts to be branched on), and these are used to create two or more child nodes that represent the root nodes of the new subproblems. When local branching is initiated, branching occurs on a local branching constraint, i.e. on a cut. Two children are created: one with a Hamming distance of ∆( x, x ) ≤ k , the other with a Hamming distance of ∆( x, x ) &gt; k . These child nodes represent the root nodes of the new subproblems: the first being the local root node , the second being the normal root node . The children of the local root node form a local tree and are assigned an unique identification number (ID), represented by the class LocalTreeId . In order to guarantee unique tree IDs even when the trees are created in different LP processes, a LocalTreeId consists of the internal COIN index number of its root node and the unique index number of the LP process where it was created (supplied by the tree manager). The other classes do not care about Lo- calTreeId’s internals, all they need are the pre-defined operators ( == , ! = , &lt; ) and the packing and unpacking methods for transferring LocalTreeIds between modules.  6.1.2 The LB tm Module The LB tm module provides our own implementation of COIN/BCP’s tree manager module. It is derived from BCP tm user and implements some of its methods to integrate local branching into the tree manager. It provides the LP modules with commands concerning local branching, is able to create local trees by transmitting the appropriate root nodes and keeps track of the number of created and pruned nodes for all local trees. LB tm implements several methods of the BCP tm user class:</p>
         <p>• pack module data() : sends miscellaneous initialization information to the LP process. This includes the initial solution for local branching, if local branching is enabled at all, and the value of k to be used for the first local tree. • pack user data() : this method is called by COIN/BCP when a node with an user data object is sent to a LP module. Additionally to packing the node’s LB user data object, the tree manager updates its tree statistics about pruned nodes and sends additional information when a new local tree is started (i.e. the initial solution, value of k , and some other necessary information). • unpack user data() : used to unpack an LB user data object sent from a LP process.</p>
         <p>• unpack feasible solution() : this method is invoked by COIN/BCP when a new feasible solution was sent by a LP process. This method unpacks the feasible solution (of type BCP solution generic) and updates the LocalTreeIndex’s statistics when it was found in a local tree. When a new global optimum is found, the solution is broadcasted to all active LP processes using COIN/BCP’s messaging framework. • compare tree nodes() : this binary comparison function is crucial for the local branching metaheuristic. It is called by COIN/BCP to insert a new tree node in the internal candidate queue. The nodes are ordered by priority, and the first node is the next to be sent to an idle LP process. COIN/BCP implements this method in a way that it represents a certain tree search strategy (i.e. ordering nodes by level for breadth-first or by upper bound for best bound first search). The local branching framework extends this method by always preferring local tree nodes to “normal” nodes. When both nodes are local tree nodes (or both are normal nodes), the COIN/BCP comparison function is called.</p>
         <p>6.1.3 The LB lp Module The LB lp module implements local branching for the LP module. It executes the commands sent from the tree manager (LB tm) module, and is able to create local trees when a normal root node is sent. The following methods of the BCP lp user superclass have been implemented:</p>
         <p>• unpack module data() : the counterpart to LB tm::pack module data(). Initialization of this LP process is performed, and common parameters like the value of k are set, and the initial feasible solution used for local branching is unpacked. • pack user data() : packs an LB user data object to a buffer. • unpack user data() : unpacks an user data object. This method is called by COIN/BCP when a new node with an attached user data object arrived from the tree manager. The additional information sent by LB tm::pack user data() when a new local tree should be created is also stored in the LB lp object. • unpack user message() : this function was patched into COIN/BCP to allow transmitting user-defined messages between the tree manager and LP modules. By setting a certain message tag number this method gets called when the message arrives at the process. • pack feasible solution() : packs a feasible solution to be sent to the tree manager. Additional to its predefined behavior, the local branching framework adds the current user data object if the feasible solution was generated in a local tree. This makes it possible for the tree manager to assign each received solution to the local tree where it was found. • initialize new search tree node() : this function gets called before a node is processed (i.e. before the LP relaxation is computed). This allows the user to tighten variable and cut bounds. The local branching framework performs two major tasks in this method:</p>
      </sec>
      <sec>
         <title>–</title>
         <p>When the local root node of a new tree is processed, variable fixing might occur. A given percentage of all free variables that is equal in the initial solution of the local tree and the current LP result is fixed to its value.</p>
      </sec>
      <sec>
         <title>–</title>
         <p>As explained in section 4.2.3, the inverse constraint when fixing variables is expressed as a row cut. Depending on whether the corresponding local tree was completely solved, this row cut has to be (de-)activated when the normal root node is processed. • select branching candidates() : this method is invoked by COIN/BCP when the LP relaxation of a subproblem has been solved. This method can either decide to repro- cess the subproblem when cuts have been added, or to return one or more branching objects. When a local tree should be created, an appropriate local branching constraint (a Hamming distance cut by default) is created and a branching object is returned. The local branching cut is created by the virtual method create local constraint() that can be re-implemented by the user. • set actions for children() : when a branching object was chosen, the LP process decides for each child whether to keep it for immediate processing or to return it to the tree manager. At most one child can be kept in a LP process. The local branching framework uses this method to force processing of the local root node when a new tree was created. • set user data for children() : the user data information for the child nodes of a branching object is generated after the branching object was chosen. When a new local tree is created, this method sets the local tree identification number and other internal data about the local tree. When an existing local tree is branched, it propagates the information stored in the current user data object to its children.</p>
      </sec>
      <sec>
         <title>Creating the Hamming distance cut</title>
         <p>LB lp::create hamming constraint() generates a Hamming distance constraint used for local branching. It resembles the local branching constraint as described by Fischetti and Lodi. In its current implementation it is restricted to binary IPs. It can be used as a template for custom local cut generators. Note that create hamming constraint() is not virtual, the framework calls the virtual function create local constraint() when creating local cuts which in turn calls the Hamming distance generator. When implementing new cuts, simply override the latter virtual function. Both functions return a local branching object and get information about the current node through their input parameters:</p>
         <p>• lpres represents the current LP optimum. • vars contains all available variables at the current node. • cuts contains all current cuts. The generated local cut(s) can not be appended to this collection, they must be contained in the returned branching object. • br sol is the feasible solution sent from the tree manager to be used for local branching (the initial solution ).</p>
         <p>The creation of the Hamming distance constraint is straight-forward: first, the feasible solution br sol is unpacked to a local array for easier access. Then the variable coefficients of the cut are generated. Each variable that flips its value in any feasible solution must be</p>
         <p>detected, and the sum of all changed variables must not be greater than k . Thus, the coefficients for the cut are:</p>
         <p>where x is the initial solution for the local tree. The local cut is then described as</p>
         <p>j ∈ S 0 j ∈ S 1 for any feasible solution x . Simple transformation leads to the row constraint as it will be passed to COIN/BCP:</p>
         <p>After creating a row cut with the bounds for the local tree and the rest of the search tree, some variables are selected for fixing if the tree manager requested variable fixing for this local tree. The indices of all free (non-fixed) variables equal in both the initial and the current LP solution are stored in an array, which in turn is used to randomly pick the requested number of variables. An additional constraint for the normal tree is added as described in section 4.2.3. The variables of the local tree will be fixed when initialize new search tree node() is called for the local root node. The list of picked variables can be kept inside the LB lp module since the local root node is processed immediately after branching by the same process.  In order to provide a clean separation between the low-level local branching implementation represented by the LB tm and LB lp classes and global local branching state information, the LocalTreeManager class was introduced. It encapsulates all methods for tracking local trees, such as maintaining active node numbers and assigning found solutions to local trees. A LocalTreeIndex object is used for storing the data, which is shared with the LB MetaHeuristic objects that is responsible for controlling the local branching algorithm. To emphasize the different purposes of these classes, a quick overview of the control distribution follows. All classes below (except LB lp) are effectively singletons inside the LB tm process.</p>
         <p>6.2 Managing Local Trees</p>
         <p>• The LB lp module(s) process individual subproblems. • The LB tm module assigns subproblems to LB lp modules and receives all new subproblems and solutions. When it receives information about a local tree (e.g. a new solution was found), the appropriate LocalTreeManager method is called. It also polls the LB MetaHeuristic object for commands, e.g. creation or termination of local trees. • The LocalTreeManager is mostly a passive module that provides data manage- ment routines concerning local trees. Its only active part lies in the activation of LB MetaHeuristic routines on certain events, e.g. calling a method to notify the meta heuristic of a new solution or a terminated tree.</p>
         <p>• The LB MetaHeuristic object is mainly responsible for controlling local branching – that is, creating new local trees or terminating existing ones. Additionally, the initial solution for the first local tree is generated inside this class. • The LocalTreeIndex serves as shared data pool for the LocalTreeManager and LB MetaHeuristic classes. The former is responsible for keeping the information up to date, the latter uses it mainly as decision source (e.g. to terminate all trees with more than 50.000 nodes).</p>
         <p>6.2.1 The LocalTreeIndex The LocalTreeIndex gathers information about all local trees and provides miscellaneous statistical information, e.g. the number of active trees or the number of created nodes per tree.</p>
      </sec>
      <sec>
         <title>The LocalTree class</title>
         <p>A local tree is represented by a LocalTree object. This class provides several get and set methods. The latter are called by the LocalTreeManager, but the former can be also used in some LB MetaHeuristic methods to query miscellaneous information about the given local tree. The most significant properties of a local tree are:</p>
         <p>• get nodes created() returns the total number of created nodes in this local tree. • get nodes deleted() returns the total number of deleted nodes in this subtree. This includes pruned nodes (because of their upper bound), infeasible nodes, and nodes deleted by the tree manager when a local tree was terminated. • get active nodes() returns the number of active (i.e. not processed and not deleted) nodes, in other words the difference between created and deleted nodes. A local tree with no active nodes is terminated, all active local tree must have at least one node that has not been deleted. • get nodes created since improvement() returns the number of created nodes since the last improvement of the best solution in this local tree . • get nodes deleted since improvement() is the equivalent number for deleted nodes. • get terminated() returns true when this local tree has been explicitly terminated. This is a helper function for the user to avoid terminating the same tree several times (since tree termination might not occur immediately, depending on the number of active nodes.)</p>
      </sec>
      <sec>
         <title>The LocalTreeIndex class</title>
         <p>The LocalTreeIndex class stores all local trees in an associative container (using the previously introduced LocalTreeIds as key and LocalTree as value type). The local tree manager is responsible for creating new entries when necessary, so the LocalTreeIndex actually provides a single service method: find() accepts a LocalTreeId as parameter and returns the corresponding LocalTree object, or throws an exception when the local tree does not exist.  Additionally, miscellaneous statistical information about all local trees (e.g. the best global solution found so far, or the number of nodes created since the last tree was terminated) is provided through getter methods similar to those in the LocalTree class. For a complete description, refer to the autogenerated class documentation. 6.2.2 The LocalTreeManager The LocalTreeManager is instantiated by the LB tm object and assumes control over the LocalTreeIndex data storage. It provides a clean interface for all tasks concerning local tree information, such as assigning solutions to local trees, adding created nodes or removing deleted nodes. Additionally, it maintains the lists for active, terminated and aborted trees in the LocalTreeIndex. The user probably does not want to interfere with the LocalTreeManager’s methods, they are automatically called from LB tm when needed. Instead, the LocalTreeManager delegates control to the LB MetaHeuristic object, which will be implemented by the user and is responsible for local tree control. The LB MetaHeuristic class provides a clean and simple encapsulation of the local branching algorithm. Its main goal is to provide an interface to the local branching framework without requiring the user to deal with the internals of COIN and the framework. LB MetaHeuristic is an abstract class that does not implement any local branching functionality. However, it takes little to implement the standard local branching algorithm. LB MetaHeuristic operates asynchronously in the sense that its actions, for example local tree creation, are not immediately executed. Instead, internal flags indicate the action to be taken by the tree manager when it is possible. This limitation is caused by the internal structure of COIN which imposes certain limits on direct control of the computation in exchange for performance, efficiency and parallelism. This should be taken into account when advanced tasks, such as creating multiple local trees at once, do not work as expected. Local branching control is basically performed by two operations: creating local trees and terminating local trees. Additional parameters influence local branching, such as the value of k , or the amount of variables to be fixed. LB MetaHeuristic offers the following methods and data fields for local branching control:</p>
         <p>6.3 Controlling Local Branching</p>
         <p>• create tree() sets internal variables to tell the tree manager that a new tree should be created. The initial solution can be passed as a parameter, or the current incumbent solution will be used. Note that subsequent calls to this function have no effect since the actual tree creation is executed asynchronously by the tree manager. For creating multiple trees at once, the calls to create tree() have to be synchronized, for example using the tree created() handler described below. • terminate tree() tells the tree manager to terminate the local tree with the given LocalTreeId. • terminate active trees() terminates all active trees. This may be especially useful for terminating local branching. • lb k contains the value of k to be used for new local trees.</p>
         <p>• lb fixvars contains the amount of variables to be fixed relative to the total number of variables.</p>
         <p>The local branching algorithm is determined by LB MetaHeuristic’s reaction to certain events. These event handlers are called by the LocalTreeManager and offer great degrees of freedom for creating own local branching metaheuristics. The event handlers are:</p>
         <p>• initial solution() is called to obtain the initial solution for the very first local tree. The parameters lb k and lb fixvars might also be set in this procedure. • tree created() is called when a new tree was generated. The corresponding tree identification and the tree object are passed as references. After creating a tree with create tree(), this is the first occasion to create another local tree. • tree finished() is called when a tree is finished, i.e. it has no active nodes remaining. • new node generated() is called whenever a new node was generated in a local tree. Since this method gets called regularly, time-related tasks (such as terminating local trees above a certain node limit) can be implemented in this method.</p>
         <p>6.3.1 Implementing a Basic Local Branching Algorithm In order to emphasize how the LB MetaHeuristic object can be used for implementing local branching, consider the following task. Standard local branching should be implemented, i.e. one active tree at any given time, with a node limit of 10.000 nodes per local tree. This is accomplished by implementing LB MetaHeuristic’s event handlers in the following way:</p>
         <p>• initial solution() returns a heuristically generated solution and sets lb k and lb fixvars to the desired values. • tree terminated() creates a new local tree by calling create tree(). This single function call implements the standard sequential local branching algorithm and ensures that there is always only one active local tree. • new node generated() uses the LocalTreeIndex object (index) to fetch the number of created nodes for the active tree and calls terminate tree() when the node limit was exceeded.</p>
         <p>Chapter 7</p>
      </sec>
      <sec>
         <title>Multidimensional Knapsack Problems</title>
         <p>7.1 Introduction</p>
         <p>Given a knapsack of fixed capacity c and n items with profits p j and weights w j for j = 1 . . . n , the task is to find the most valuable subset of items that fits into the knapsack. We assume that p 1 . . . p n and w 1 . . . w n are positive integers. The unbounded knapsack problem does not limit the number of times each item type can be used. In the binary or 0-1 knapsack problem , the number of items is constrained to be 0 or 1. The multiple-choice knapsack problem requires the items to be chosen from disjoint classes. In the multiple knapsack problem , several knapsacks are to be filled simultaneously. The rest of this chapter will be based on binary knapsack problems. Formally, a binary knapsack problem can be stated as:</p>
         <p>In this formulation, x j is 1 when item j is included in the knapsack and 0 otherwise. p 1 . . . p n contains the profit (or value) for each item, and w 1 . . . w n the weight or resource usage. Note that it is trivial to obtain a (poor) feasible solution x j = 0 for all j . This section is based on the book on knapsack problems by Pisinger et al. [ <xref id="XR342" ref-type="bibr" rid="R21">21</xref>] which provides a thorough reference for the family of knapsack problems. 7.1.1 Algorithms for Knapsack Problems All knapsack problems are NP-hard, therefore it is highly unlikely to find an optimal algorithm with a polynomial worst-case time complexity. Despite this, there are algorithms that achieve reasonable solution times also for large instances. The following overview of exact and approximate algorithms is based on [<xref id="XR344" ref-type="bibr" rid="R32">32</xref>]. Note that the multidimensional knapsack problem is more complex and thus usually not covered by highly effective approaches like the dynamic programming approach.</p>
         <p>• Branch and Bound : A Branch and Bound implementation for knapsack problems was first proposed by P. J. Kolesar in 1967 [<xref id="XR346" ref-type="bibr" rid="R23">23</xref>].</p>
         <p>• Dynamic programming : Basically an enumeration algorithm which can achieve excel- lent performance on some families of knapsack problems, especially those bounded by relatively low integer capacity. For these it is possible to obtain an optimal solution in Θ( nc ) with n being the number of items and c the knapsack capacity. • State space relaxation : A dynamic programming relaxation where the coefficients are scaled by a fixed value. The complexity of an algorithm may be decreased, but the optimal solution may no longer be found. This is an interesting approach for efficient approximate algorithms. • Preprocessing : Some variables may be fixed at their optimal values by using bounding tests. • Fully polynomial time approximation schemes ( FPTAS ): These are heuristics that can find a solution z with a relative error bounded by any constant value ε , i.e. z − z ∗ z ∗ ≤ ε , where z ∗ is the optimal solution value, in polynomial time bounded by the length of the input and 1 ε . A fully polynomial approximation scheme for the binary knapsack problem was presented by Ibarra and Kim in 1975 [<xref id="XR349" ref-type="bibr" rid="R19">19</xref>]. 7.1.2 Multidimensional Knapsack Problems The generalization of the knapsack problem to more than a single constraint is the multidimensional knapsack problem , also known as d-dimensional knapsack problem ( d-KP ) or multiconstraint knapsack problem . It is defined as an integer program with the following structure:</p>
         <p>An equivalent formulation using vectors is: maximize c T x</p>
         <p>There are two main characteristics of integer programs that describe multidimensional knapsack problems: First, they are particular difficult instances of integer programming because the constraint matrix W is unusually dense, while most other relevant classes are defined by sparse constraint matrices. But analogously to other knapsack problems it is also particularly easy to find a feasible solution: x j = 0 for all j , whereas in general integer programming finding feasible solutions might be as hard as finding an optimal solution. Typically the number of items n exceeds the number of constraints d . A rough bound for computing optimal solutions of multidimensional knapsacks with todays algorithms and computers is n = 500 and d = 10 . It has been shown by Korte and Schrader in 1979 [ <xref id="XR356" ref-type="bibr" rid="R24">24</xref>] that the existence of a fully polynomial time approximation scheme for a multidimensional knapsack problem even with only two constraints ( d = 2 ) would imply P = NP , i.e. that every NP-hard problem could be solved in polynomial time. However, there exists a polynomial time approximation scheme ( PTAS ) with a running time of Θ( n d/ε − d ) [<xref id="XR359" ref-type="bibr" rid="R4">4</xref>]. Compared to a FPTAS, a PTAS has the drawback of an exponential increase in running time with respect to the accuracy, i.e. its running time is polynomial only with respect to the input length, but not to the required ε value. The enormous complexity of multidimensional knapsack problems motivated extensive research in heuristic algorithms. 7.2.1 Greedy Heuristics Greedy heuristics work by inserting all items that do not violate any resource constraints ( primal greedy heuristics ) or by first putting all items into the knapsack and then removing items until the solution becomes feasible ( dual greedy heuristics ). Since the order in which the items are inserted or removed matters and some items are more valuable than others (i.e. offer a relatively high profit for relatively low resource usage), items are sorted by an arbitrary efficiency e j before inserting them into the knapsack. The most obvious efficiency measure for a one-dimensional binary knapsack problem is the relative profit e j = w p j j . Since there is more than one resource constraint in multidimensional knapsack problems, there is no such trivial method of determining the efficiency of an item. The nearest counterpart would be the aggregation of all d constraints, i.e.</p>
         <p>7.2 Heuristic Algorithms</p>
         <p>where j would be the efficiency for item . The main drawback is that it does not work well when the resource constraints are of different orders of magnitude. In this case, one constraint may completely dominate all others. This can be avoided by taking the relative weight for each constraint and define</p>
         <p>Senju and Toyoda [<xref id="XR369" ref-type="bibr" rid="R38">38</xref>] proposed a different way to incorporate the relative distribution of weights by including the difference between the capacity and the total resource usage of all items for a given constraint.</p>
         <p>A generalized formulation of these efficiency measures was proposed by Fox and Scudder [<xref id="XR373" ref-type="bibr" rid="R37">37</xref>] by introducing a relevance value r i for every constraint.</p>
         <p>Advanced adaptive algorithms adjust the relevance values when an item was inserted, an early version of such an algorithm can be found in [ <xref id="XR378" ref-type="bibr" rid="R38">38</xref>]. 7.2.2 Relaxation-Based Heuristics Heuristics based on the LP relaxations of integer programs can also be used for multidimensional knapsack problems with little or no adaptation. A simple and fast approach was given by Bertsimas and Demir in 2002 [<xref id="XR380" ref-type="bibr" rid="R3">3</xref>]. It starts by fixing variables in the LP solution depending on a parameter γ ∈ [0 , 1] :</p>
         <p>In the second phase the subproblem defined by the undecided variables γ ≤ x j LP &lt; 1 is solved again, and further variables are fixed:</p>
         <p>0 for j = argmin { x LP j | 0 &lt; x LP j &lt; 1 } . The last assignment fixes the variable with the least fractional value to zero. This second phase is repeated until all variables are fixed to zero or one. Small values for γ lead to better solution values but longer running times, while bigger values offer better performance. Setting γ = 1 is equivalent to rounding down the first LP solution. The authors proposed setting γ = 0 . 25 when performance is more important than solution quality.  7.2.3 Hybrid Algorithms More advanced algorithms combine different approaches to the multidimensional knapsack problem. They are often more complex and require more running time, but can yield near- optimal solutions in many cases where simpler heuristics fail. One such approach was proposed by Lee and Guignard in 1988 [<xref id="XR388" ref-type="bibr" rid="R27">27</xref>]. Their algorithm starts with a modified version of Toyoda’s primal greedy heuristic [<xref id="XR389" ref-type="bibr" rid="R40">40</xref>]. Instead of deciding on each item separately, they decide on several items at once before recomputing the relevance values, leading to better performance. Based on this feasible solution, the LP relaxation is solved. A comparison between the feasible solution and the LP solution in combination with the reduced costs of the LP solution is used to fix some variables and reduce the problem size. These steps are iterated, the number of iterations is controlled by a parameter. A more recent heuristic was given by Vasquez and Hao in 2001 [<xref id="XR390" ref-type="bibr" rid="R41">41</xref>]. They combine linear programming and tabu search to search binary areas around continuous solutions. This is facilitated by additional constraints that limit the search space around a solution, like a sphere constraint that geometrically isolates the search space. 7.2.4 Evolutionary Algorithms Evolutionary algorithms (EAs) were inspired by biological evolution and try to mimic the evolutionary process. An evolutionary algorithm keeps one or more populations of solutions for a given problem, and tries to improve these solutions by imitating evolutionary procedures like selection , recombination and mutation . Several evolutionary algorithms exist for the multidimensional knapsack problem. Major differences between algorithms concern varying operators for recombination and mutation, and also different representations of the solutions themselves. Raidl [<xref id="XR394" ref-type="bibr" rid="R33">33</xref>, <xref id="XR395" ref-type="bibr" rid="R34">34</xref>, <xref id="XR396" ref-type="bibr" rid="R35">35</xref>] proposed different approaches for the multidimensional knapsack problem, also combining evolutionary algorithms with local improvement heuristics. A particularly effective approach is based on an EA by Chu and Beasley [<xref id="XR397" ref-type="bibr" rid="R6">6</xref>]. It uses a direct representation using bit vectors ∈ { 0 , 1 } n for representing solutions. Recombination is done by uniform crossover, i.e. a child solution is created by randomly picking bits from one of its parents. Bit-wise mutation can be used to increase the diversity of solutions. Both crossover and mutation can produce infeasible solutions, so a repair algorithm is required. A two-phase heuristic is used for repairing and local improvement: the items are ordered by an utility ratio similar to the efficiency measures described in section 7.2.1. For repairing solutions, the least promising items are removed until the solution becomes feasible. The local improvement algorithm processes items not present in the current solution by decreasing utility ratio and inserts them if no constraints are violated. Decoder-based EAs replace the direct representation of a solution with an encoding scheme. Recombination and mutation operate on the encoded solutions, implicitly exploring the original search space. The choice of the encoding scheme can greatly influence the effectivity of recombination and mutation and the convergence of the overall search algorithm. A possible encoding scheme for knapsack problems uses permutations . Instead of storing the value for each variable, a permutation π : J → J with J = { 1 , . . . , n } is used to represent a solution. In order to get a direct representation for a solution, decoding starts with the feasible solution x = (0 , . . . , 0) . Then the variables are visited in the sequence described by the permutation, and variables that do not violate constraints are set to 1. Mutation randomly exchanges two different positions in a permutation, recombination is done using uniform order based crossover [<xref id="XR398" ref-type="bibr" rid="R9">9</xref>] which keeps the ordering (but not necessarily the positions) of the parent solutions. A permutation based EA for the knapsack problem has been proposed by Hinterding [<xref id="XR399" ref-type="bibr" rid="R18">18</xref>] and it has also been applied to the multidimensional knapsack problem by Gottlieb [<xref id="XR400" ref-type="bibr" rid="R17">17</xref>], Raidl [<xref id="XR401" ref-type="bibr" rid="R33">33</xref>], Thiel and Voss [<xref id="XR402" ref-type="bibr" rid="R39">39</xref>].</p>
         <p>Chapter 8</p>
      </sec>
      <sec>
         <title>A Sample Application: MD-KP</title>
         <p>This chapter guides through a sample application for the local branching framework, a Branch and Cut solver for the multidimensional knapsack problem as described in chapter 7. The goal is to optimize the following integer program:</p>
         <p>Since the application actually will be embedded into the COIN/BCP main program, it makes sense to adhere to COIN/BCP’s directory structure. COIN/BCP provides makefiles for building custom applications where only the added user files have to be defined. The application source is grouped into the following directories:</p>
         <p>• include/ contains all header files for the application’s classes. • LP/ contains the implementation of this program’s LP module. • TM/ contains the tree manager module. • Member/ contains the other classes, in our case the initialization class (descendant of LB init) and the metaheuristic.</p>
         <p>First of all, defining a class to hold a problem instance simplifies the further operations. In case of the multidimensional knapsack problem, we basically need some arrays to hold the coefficients of the constraints and the objective function, and the corresponding bounds. This class is based on the BranchAndCut example from the COIN source and can be easily adopted to other integer programming problems. It is defined as follows:</p>
         <p>class KS prob { public: int nItems; /// &lt; Total number of items (= columns) int nConstraints; /// &lt; Total number of constraints (= rows) double optimalknown; /// &lt; Known optimal solution, − DBL MAX if unknown double * clb; /// &lt; Lower bound for each core variable (usually 0.0)</p>
         <p>double * cub; /// &lt; Upper bound for each core variable (usually 1.0) double * val; /// &lt; Objective value (profit) for each core variable double ** res; /// &lt; Resource demands double * rescons; /// &lt; Resource constraints CoinPackedMatrix* core; /// &lt; Core matrix double * rlb core; /// &lt; Lower bounds in the core matrix (usually 0.0) double * rub core; /// &lt; Upper bounds in the core matrix (usually rescons[rownum]) } ; Next, the tree manager class will be implemented. 8.1 KS tm implementation</p>
         <p>The tree manager implementation is responsible for reading the problem data from a file, initializing the core matrix of COIN/BCP, and providing methods to pack and unpack cuts.  8.1.1 Test File Format Our program will read test instances taken from Chu and Beasley’s paper on a genetic algorithm for the multidimensional knapsack problem [<xref id="XR417" ref-type="bibr" rid="R6">6</xref>]. These test files contain 270 different instances, ranging from very easy to very complex problems. The problems are described in plain text, each file contains 30 instances of the same dimension (i.e. the same number of variables and constraints). The format of the text file is:</p>
         <p>• The number of instances (should be 30). • For each problem:</p>
         <p>n d</p>
      </sec>
      <sec>
         <title>– – – –</title>
         <p>Number of variables , number of constraints , optimal solution (if known). The coefficients of the objective function p j for j = 1 . . . n . For each constraint i : the coefficients of the constraint w ij . The upper bounds of the constraints c i for i = 1 . . . d .</p>
         <p>KS tm::read input() reads the data from the given file name using the given instance number into a KS prob object which is stored in KS tm. Since it is just a very simple line parser the implementation is omitted here.  8.1.2 Setting up the Core Matrix The raw core matrix was set up by KS tm::read input() and stored into KS prob::core, but until now COIN/BCP is not aware of the problem data. To do this, we must override LB tm::initialize core() to create all core variables and core cuts (i.e. all IP constraints). The core matrix is put together using the coefficients and upper and lower bounds loaded by read input(). In the end, we call the implementation of the superclass because the local branching framework might have to do some setup actions on its own. 8.1.3 Packing and Unpacking of Cuts We do not create own cuts in our sample application, but we still have to provide means to pack and unpack LB cut objects. These cuts will be used for local branching cuts and inverse variable fixing constraints. Since LB cut already provides methods for packing and unpacking those cuts, the corresponding implementations in LB tm are very compact: 8.1.4 Sending the Problem Description to the LP Module The LP module will need the problem description stored in the KS prob object for heuristically finding feasible solutions. KS tm::pack module data() allows to send any information to an LP process when it is created, thus we simply append our problem description to the given buffer. The local branching framework appends data on its own, so the implementation of the superclass is called too. Actually only a pointer to the problem description is passed. This is possible since the focus of the sample application does not lie on parallel execution, but on simplicity. However, it is a rather trivial task to write packing and unpacking routines for the KS prob class (especially since the core matrix does not need to be transmitted). 8.1.5 Creating a KS MetaHeuristic Object The KS MetaHeuristic implements the abstract LB MetaHeuristic class and contains the user- defined local branching control methods. Since the tree manager does not know the meta heuristic’s type a priori, we instantiate a KS MetaHeuristic object by overriding LB tm’s abstract method create lbh(): ltm is the tree manager’s LocalTreeManager object, ltm → index is the shared LocalTreeIndex, and prob contains the problem definition initialized by KS tm::read input(). The implementation of the tree manager is now complete, the next task is to implement the LP module. In the LP module implementation, the main effort goes into cut generation and heuristically finding feasible solutions. First, we need the counterpart to pack module data, or the local branching framework will use the wrong buffer values. Additionally, methods for packing and unpacking cuts are also required. We also need to setup the LP solver. Here we will instantiate COIN’s own CLP solver, the complete version of the sample application also supports CPLEX. 8.2.1 Generating Feasible Solutions COIN/BCP provides a method where the user can generate feasible solutions from solved LP relaxations. By generating good feasible solutions early in the computation, the search tree size can be reduced. However, the heuristic should not be too complex since this method is called for every solved LP relaxation. For the multidimensional knapsack problem, we chose a simple greedy heuristic as described in section 7.2.1. The efficiency measure is not based on the resource usage of each variable, but on its value in the LP result. Thus, after sorting the variables by descending LP value, all variables that do not violate the resource constraints are added to the solution, which is then returned to the LP module. 8.2.2 Generating Cuts The second main task for the LP module is cut generation. When a LP relaxation was solved, one can either try to generate cuts violated by the LP result, or the subproblem is branched. COIN/CGL offers several generic cut generators. If one chooses to generate cuts, it can be either done for every node, or for nodes meeting a certain condition. In this example, we choose to generate cuts for nodes at every eighth level of the search tree. We try to create generic knapsack cover cuts and Gomory cuts . The cut generators are first instantiated and stored in a list. We also set the maximal number of items for the knapsack cut generator. Higher numbers lead to higher computational complexity, but also higher chances of finding cuts. Then every cut generator is invoked to generate cuts and store them in cutlist . These cuts are then appended to the new cuts output parameter. When using cut generation, we have to implement COIN/BCP’s cuts to rows method. It is used to realize the abstract cut representations to actual rows of the LP relaxation. COIN/BCP provides the USER initialize class to instantiate custom tree manager and LP module implementations. KS init implements LB init, which in turn was derived from USER initialize. KS init::lp init() creates a new KS lp object, and KS init::tm init() instantiates a new KS tm object. While the former is called once for every created LP process, the latter is only called on program startup. Thus it is used to process command line parameters and initialize the tree manager by calling KS tm::read input() for the given file name. Additionally, the global function BCP user init() has to be implemented to return a new KS init() object. With these three classes, the basic COIN/BCP implementation is finished. For the standard COIN/BCP classes, the implemented methods are sufficient for an executable Branch and Cut algorithm for the multidimensional knapsack problem. The local branching framework requires the implementation of a fourth class, the local branching metaheuristic. All local branching related logic is contained in KS MetaHeuristic, our implementation of the LB MetaHeuristic class. Our local branching controller accomplishes three main tasks:</p>
         <p>void KS tm::initialize core(BCP vec &lt; BCP var core* &gt; &amp; vars, BCP vec &lt; BCP cut core* &gt; &amp; cuts, BCP lp relax*&amp; matrix) {</p>
         <p>// initialize core variables for ( int i = 0; i &lt; prob.nItems; ++i) if (0.0 == prob.clb[i] &amp;&amp; 1.0 == prob.cub[i]) vars.push back( new BCP var core(BCP BinaryVar, prob.val[i], 0, 1)); // initialize core cuts for ( int i = 0; i &lt; prob.core − &gt; getNumRows(); ++i) cuts.push back( new BCP cut core(prob.rlb core[i], prob.rub core[i])); // create LP relaxation matrix = new BCP lp relax; matrix − &gt; copyOf(*prob.core, prob.val, prob.clb, prob.cub, prob.rlb core, prob.rub core); LB tm::initialize core(vars, cuts, matrix); // execute LB’s initializiation }</p>
         <p>void KS tm::pack cut algo( const BCP cut algo* cut, BCP buffer&amp; buf) { const LB cut* lb cut = dynamic cast &lt; const LB cut* &gt; (cut); if (!lb cut) throw BCP fatal error(”pack cut algo(): unknown cut type! \ n”); lb cut − &gt; pack(buf); } BCP cut algo* KS tm::unpack cut algo(BCP buffer&amp; buf) { return new LB cut(buf); }</p>
         <p>void KS tm::pack module data(BCP buffer&amp; buf, BCP process t ptype) { if (BCP ProcessType LP == ptype) buf.pack(&amp;prob); LB tm::pack module data(buf, ptype); }</p>
         <p>virtual LB MetaHeuristic* create lbh() { return new KS MetaHeuristic(ltm − &gt; index, &amp;prob); }</p>
         <p>8.2 KS lp Implementation</p>
         <p>void KS lp::unpack module data(BCP buffer&amp; buf) { buf.unpack(pprob); LB lp::unpack module data(buf); } void KS lp::pack cut algo( const BCP cut algo* cut, BCP buffer&amp; buf) { const LB cut* lb cut = dynamic cast &lt; const LB cut* &gt; (cut); if (!lb cut) throw BCP fatal error(”LB lp::pack cut algo: unknown cut type! \ n”); lb cut − &gt; pack(buf); } BCP cut algo* KS lp::unpack cut algo(BCP buffer&amp; buf) { return new LB cut(buf); }</p>
         <p>OsiSolverInterface* KS lp::initialize solver interface() { OsiClpSolverInterface *clp = new OsiClpSolverInterface; clp − &gt; messageHandler() − &gt; setLogLevel(0);</p>
         <p>return clp; }</p>
         <p>BCP solution* KS lp::generate heuristic solution( const BCP lp result&amp; lpres, const BCP vec &lt; BCP var* &gt; &amp; vars, const BCP vec &lt; BCP cut* &gt; &amp; cuts) { const double * x = lpres.x(); BCP solution generic* sol = new BCP solution generic( false ); // sort variables by LP result value, then insert them in reversed order (best first) multimap &lt; double , int &gt; sorted; for ( unsigned int i = 0; i &lt; vars.size(); ++i) sorted.insert(pair &lt; double , int &gt; (x[i], i)); // track current resource usage double * myres = new double [pprob − &gt; nConstraints]; for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) myres[j] = 0.0; multimap &lt; double , int &gt; ::reverse iterator it; for (it = sorted.rbegin(); it != sorted.rend(); ++it) { int i = (*it).second; // number of the variable to be inserted bool ok = true ; for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) { // check if any resource constraint is violated if (myres[j] + pprob − &gt; res[j][i] &gt; pprob − &gt; rescons[j]) ok = false ; } if (ok) { // insert item into knapsack for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) myres[j] += pprob − &gt; res[j][i]; sol − &gt; add entry(vars[i], 1); } } delete [ ] myres; return sol; }</p>
         <p>void KS lp::generate cuts in lp( const BCP lp result&amp; lpres, const BCP vec &lt; BCP var* &gt; &amp; vars, const BCP vec &lt; BCP cut* &gt; &amp; cuts, BCP vec &lt; BCP cut* &gt; &amp; new cuts, BCP vec &lt; BCP row* &gt; &amp; new rows) { vector &lt; CglCutGenerator* &gt; cgs; // generate nodes at every 8th level if (current level() % 8 == 0) { CglKnapsackCover* kc = new CglKnapsackCover; kc − &gt; setMaxInKnapsack(pprob − &gt; nItems); cgs.push back(kc); cgs.push back( new CglGomory); } if (cgs.size() &gt; 0) { OsiSolverInterface* si = getLpProblemPointer() − &gt; lp solver; for ( int i = vars.size() − 1; i &gt; = 0; −− i) si − &gt; setInteger(i); OsiCuts cutlist; for ( int i = cgs.size() − 1; i &gt; = 0; −− i) { cgs[i] − &gt; setAggressiveness(100); cgs[i] − &gt; generateCuts(*si, cutlist); delete cgs[i]; cgs[i] = 0; } for ( int i = cutlist.sizeRowCuts() − 1; i &gt; = 0; −− i) { LocalTreeId id = in localbranching() ? get ks user data() − &gt; id : LocalTreeId(); new cuts.push back( new LB cut(cutlist.rowCut(i), id)); } } }</p>
         <p>void KS lp::cuts to rows( const BCP vec &lt; BCP var* &gt; &amp; vars, BCP vec &lt; BCP cut* &gt; &amp; cuts, BCP vec &lt; BCP row* &gt; &amp; rows, const BCP lp result&amp; lpres, BCP object origin origin, bool allow multiple) {</p>
         <p>const int cutnum = cuts.size(); for ( int i = 0; i &lt; cutnum; ++i) { const OsiRowCut* bcut = dynamic cast &lt; const LB cut* &gt; (cuts[i]); if (bcut) rows.push back( new BCP row(bcut − &gt; row(), bcut − &gt; lb(), bcut − &gt; ub())); else throw BCP fatal error(”Unknown cut type in cuts to rows. \ n”); } } 8.3 KS init Implementation</p>
         <p>8.4 KS MetaHeuristic Implementation</p>
         <p>• initial solution() creates a heuristic solution used for the first local branching tree. • new node generated() is called regularly by the framework and monitors local branching progress. When the given node limits are exceeded, it restarts local branching. • tree finished() starts a new local tree when the last active tree finished, effectively implementing the sequential local branching algorithm.</p>
         <p>8.4.1 Configuring Local Branching Before the actual local branching implementation is described, we need a way to adjust certain parameters of our heuristic without recompiling the whole program. COIN/BCP offers a convenient, generic parameter parser that can be used to load user-defined parameters from the command line or from a file, perform type checking and sanity checks, and define default values. In order to utilize COIN/BCP’s parser, we start by defining a parameter class, KS parameters. It is not derived from any other class, instead for each parameter type (integer, double, string, ...) it defines enumerations containing the parameters’ names. This class is  used as a generic type parameter for COIN/BCP’s BCP parameter set::create keyword list() and set default entries. The first method assigns actual string labels to the user-defined parameters, the second methods initializes all parameters with default values. For example, the following code defines a couple of double parameters and then implements COIN/BCP’s methods for using them: BCP parameter set also provides methods for reading parameter from the command line, from files, or from input streams. It also offers methods for accessing parameter values (get entry() and set entry()) and packing or unpacking of parameter sets. Since the parameters are only important for the local tree metaheuristic, we do not need to pass the parameters to other modules. 8.4.2 Setting up Local Branching Before the local branching heuristic takes over control, we have to tell the framework if local branching should be enabled at all - and which parameters to use. The event handlers depend on an already running local branching algorithm (e.g. a new node was generated, or a tree was terminated). The decision whether to create a local tree or to use standard branching takes place when the first LP process is initialized in LB tm::pack module data(). LB MetaHeuristic:: lb maxpasses sets the maximum number of local trees. If it is zero, no local trees will be generated at all and standard branching will be used. This information is also used in tree creation methods, which will fail when the number of created local trees exceeds lb maxpasses. The KS MetaHeuristic class sets lb maxpasses in its read parameters() method, where the command line is parsed using the parameter methods described in the previous section, effectively disabling local branching when either the maximum number of local trees or the value of k has been set to 0. The value of k for the very first local tree is also set in this method, which is called by KS init::tm init() after the tree manager and the KS MetaHeuristic objects were created. 8.4.3 Creating the Initial Solution The initial solution is retrieved by the local tree manager when the first LP process is initialized. To demonstrate the use of pseudo-concurrent tree exploration, we use three different heuristics to start local branching with three (partially) disjunct local trees. The solution derived from the first LP result is used for the first local tree, and two greedy heuristics with different efficiency measures provide the other two solutions. Since the metaheuristic class does not know the first LP result (it is local to the LP process), a small workaround forces the local branching framework to actually use the first LP result: initial solution() returns an empty solution (i.e. with all variables set to 0) and tells the framework to use the feasible solution derived from the first LP result (if it is better). The first LP-derived solution is certainly better than the empty solution for any feasible problem instance, thus it will always be used. The other initial solutions are stored in the metaheuristic object and are sent to the tree manager as soon as the first local tree is created. To summarize the steps above, the initial local trees are created in the following way:</p>
         <p>class KS parameters { public: enum dbl params { LB FixVarsInitial, LB FixVarsIncrement, LB FixVarsMax, LB MultiK, end of dbl params } ; } ; template &lt;&gt; void BCP parameter set &lt; KS parameters &gt; ::create keyword list() { keys.push back(make pair(BCP string(”LB FixVarsInitial”), BCP parameter(BCP DoublePar, LB FixVarsInitial))); keys.push back(make pair(BCP string(”LB FixVarsIncrement”), BCP parameter(BCP DoublePar, LB FixVarsIncrement))); keys.push back(make pair(BCP string(”LB FixVarsMax”), BCP parameter(BCP DoublePar, LB FixVarsMax))); keys.push back(make pair(BCP string(”LB MultiK”), BCP parameter(BCP DoublePar, LB MultiK))); } template &lt;&gt; void BCP parameter set &lt; KS parameters &gt; ::set default entries() { set entry(LB FixVarsInitial, 0.0); set entry(LB FixVarsIncrement, 0.0); set entry(LB FixVarsMax, 0.0); set entry(LB MultiK, 1.0); }</p>
         <p>1. initial solution() calls greedy() to generate two initial solutions using two different efficiency measures. These solutions are stored in the KS MetaHeuristic object for later use. 2. initial solution() returns an empty solution to force the framework to use the solution derived from the first LP result as initial solution for the first tree. 3. tree created() issues create tree() to create the next two local branching trees.</p>
         <p>The implementation of greedy() is similar to the feasible solution generator described in section 8.2.1. A greedy heuristic inserts the items ordered by an efficiency value determined by one of the following formulas as described in section 7.2.1:</p>
         <p>The profit p j for item j is divided by the relative resource usage, i.e. the sum of all relative weights. The relative weight concerning a given resource i and an item j is the absolute weight w ij divided by the resource limit c i . This way, weights are scaled to [0 . . . 1] regardless of their absolute value, leading to a fair consideration of all weights.</p>
         <p>The second efficiency measure takes the weight distribution into account, i.e. it emphasizes scarce resources.  The code for sorting the items follows below, the greedy heuristic for inserting the items is the same as in section 8.2.1. The parameter fun determines which efficiency measure should be used. The implementation of initial solution() is simple. Two solutions are generated, and an empty solution is returned to force the LP process to use the feasible solution derived from the first LP result. The tree created() handler is called when a new tree was created by the tree manager. Here we can create the two other local trees from the initial solutions created in initial solution(). 8.4.4 Imposing Node Limits on Local Trees While the framework provides all necessary information for terminating trees above a certain node count, the criteria for aborting local trees have to be checked in the meta heuristic. In this metaheuristic, we will implement a rather simple node-based tree termination scheme that has the following characteristics:</p>
         <p>vector &lt; double &gt; resource usages(pprob − &gt; nConstraints, 0.0); if (weight distribution == fun) { // calculate total resource usages for all resources for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) { for ( int i = 0; i &lt; pprob − &gt; nItems; ++i) resource usages[j] += pprob − &gt; res[j][i]; } } multimap &lt; double , int &gt; relval; for ( int i = 0; i &lt; pprob − &gt; nItems; ++i) { double allres = 0.0; if (relative weight == fun) { // relative weight of each item for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) allres += pprob − &gt; res[j][i] / pprob − &gt; rescons[j]; } else if (weight distribution == fun) { // weight distribution according to Senju and Toyoda for ( int j = 0; j &lt; pprob − &gt; nConstraints; ++j) allres += pprob − &gt; res[j][i] * (resource usages[j] − pprob − &gt; rescons[j]); } relval.insert(pair &lt; double , int &gt; (pprob − &gt; val[i] / allres, i)); }</p>
         <p>BCP solution generic* KS MetaHeuristic::initial solution( BCP vec &lt; BCP var core* &gt; &amp; corevars, std::map &lt; BCP IndexType, BCP var* &gt; &amp; vars, bool &amp; allow lp result) { solution relative weight = greedy(corevars, vars, relative weight); solution weight distribution = greedy(corevars, vars, weight distribution); allow lp result = true ; return new BCP solution generic( false ); }</p>
         <p>void KS MetaHeuristic::tree created( const LocalTreeId&amp; id, LocalTree&amp; tree) { if (1 == index.trees.size()) create tree(solution relative weight); else if (2 == index.trees.size()) { create tree(solution weight distribution); }</p>
         <p>• Trees will be aborted when their total number of created nodes exceeds a given limit. • Trees will be aborted when the number of created nodes since the last improvement of the best feasible solution found inside the local tree exceeds a given limit. • When a tree is aborted and the maximum number of local trees is not reached, a new local tree is created with the current best global solution. Based on the result of the last local tree, the new tree will be eventually modified:</p>
      </sec>
      <sec>
         <title>– –</title>
         <p>When a better solution was found since the last tree was created, local branching is restarted with this new solution and the initial local branching parameters. When no new solutions were found, the new local tree is tightened (if the corresponding parameters are set): the number of variables to be fixed is increased, and/or the value of k gets modified.</p>
         <p>Written as a handler using LB MetaHeuristic::new node generated(), the following code implements node limits for all local trees. Some additional safety checks occurs, such as checking if local branching is enabled ( lb enabled ). Due to COIN/BCP’s asynchronous design, nodes may be added to a tree event after the tree was terminated. Thus it is also checked if the current tree has not already been terminated ( ltree.get terminated() ). The last expres- sions of the outer if clause formulate the node limits described above using the statistical data of the LocalTreeIndex.  8.4.5 Handling Terminated Local Trees LB MetaHeuristic::tree finished is called when a formerly active tree has no more active nodes remaining. It may be called when the tree was solved completely, or when it was aborted, e.g. by our new node generated() implementation. When the tree was aborted, the tree’s get terminated() function returns true. In both cases, a new tree has to be created. LB MetaHeuristic::create tree() takes care of the limit on the total number of local trees by setting lb enabled to false if necessary. Our implementation additionally tracks the number of retries for the current incumbent solution (i.e. the number of local trees spawned with the last incumbent solution), disabling local branching when a given number of retries has been exceeded. The Branch and Cut solver for multidimensional knapsack problems is now complete. For compiling the application, a properly patched installation of COIN/BCP is needed (see appendix A), and an adapted makefile. A makefile template can be taken from one of the COIN/BCP examples, found under Examples/BAC or Examples/BranchAndCut in the COIN directory. Note that all of the framework’s sources have to be added to the makefile. The related part of the Makefile.bc file should look like the following:</p>
         <p>void KS MetaHeuristic::new node generated( const LocalTreeId&amp; id, LocalTree&amp; tree) { int maxnodes without improvement = params.entry(KS parameters::LB MaxNodesWithoutImprovement); int maxnodes = params.entry(KS parameters::LB MaxNodes); if (lb k &gt; 1 &amp;&amp; lb enabled &amp;&amp; !tree.get terminated() &amp;&amp; ((maxnodes without improvement &gt; 0 &amp;&amp; tree.get nodes created since improvement() &gt; maxnodes without improvement) (maxnodes &gt; 0 &amp;&amp; tree.get nodes created() &gt; maxnodes))) {</p>
         <p>terminate active trees(); if (index.nodes created since improvement &lt; index.nodes created since newtree) // an improved solution was found, restart with this solution lb fixvars = params.entry(KS parameters::LB FixVarsInitial); lb k = params.entry(KS parameters::LB K); } else if (params.entry(KS parameters::LB FixVarsInitial) &gt; 0.0 | | params.entry(KS parameters::LB MultiK) != 1.0) { // the old tree did not yield a better solution, // so fix some variables and/or modify k value. lb fixvars += params.entry(KS parameters::LB FixVarsIncrement); lb fixvars = min(lb fixvars, params.entry(KS parameters::LB FixVarsMax)); lb k = ( int ) round(params.entry(KS parameters::LB MultiK) * lb k); lb k = max(lb k, params.entry(KS parameters::LB MinK));</p>
         <p>lb k = min(lb k, params.entry(KS parameters::LB MaxK)); lb tightening = true ; // do it only once } } }</p>
         <p>void KS MetaHeuristic::tree finished( const LocalTreeId&amp; id, LocalTree&amp; tree) { LB MetaHeuristic::tree finished(id, tree); if (0 == index.active trees.size() &amp;&amp; lb enabled) { if (index.nodes created since improvement &lt; index.nodes created since newtree) { create tree(); lb tightening = false ; lb retries = 0; } else if (lb tightening &amp;&amp; lb retries &lt; params.entry(KS parameters::LB MaxRetries)) { create tree(); ++lb retries; } else lb enabled = false ; } } 8.5 Finishing Touches</p>
         <p>USER SRC = USER SRC += KS init.cpp USER SRC += KS lp.cpp</p>
         <p>USER SRC += KS tm.cpp USER SRC += KS metaheuristic.cpp # LB sources USER SRC += LB tm.cpp USER SRC += LB lp.cpp USER SRC += LB cut.cpp USER SRC += LB user data.cpp USER SRC += LB init.cpp USER SRC += localtree.cpp USER SRC += localtreeid.cpp USER SRC += localtreemanager.cpp USER SRC += LB metaheuristic.cpp</p>
         <p>Chapter 9</p>
      </sec>
      <sec>
         <title>Test Results</title>
         <p>The multidimensional knapsack solver described in the previous chapter was used to under- take extensive testing of local branching performance. The test instances were taken from J.E. Beasley’s OR Library [ <xref id="XR502" ref-type="bibr" rid="R2">2</xref>] which provides 270 instances for the multidimensional knapsack problem. They are grouped by problem dimension into nine files, each containing 30 instances. The smallest problem size contains 100 variables and 5 constraints, the largest 500 variables subject to 30 constraints. Each file contains three groups of instances with tightness ratios of α = 0 . 25 , 0 . 5 , and 0 . 75 . The tightness ratio defines how “tight” the resource limits are set, lower ratios define tighter resource limits. Additional problems were taken from the Hearin Center for Enterprise Science [<xref id="XR503" ref-type="bibr" rid="R14">14</xref>]. This dataset contains eleven test instances originally used for the multiple knapsack problem, ranging from two instances with 100 variables and 15 constraints to an extremely large test instance with 2500 variables and 100 constraints. All tests were executed on an Intel Pentium 4 with 2.8 GHz and 2 GB of RAM, running Linux with a 2.4.21 kernel. The LP engine used for testing was CPLEX 8.1. The test application was configured to output status information at regular intervals, allowing a reporting tool described in appendix B to generate tables and plots comparing different configurations. The running time is always given in CPU time for the COIN/BCP process as recorded by COIN/BCP’s own timing statistics. This also includes time spent solving the LPs using the CPLEX solver. When analyzing the results with respect to time (i.e. determining when a solution was found during the computation), the number of processed nodes is taken instead of the CPU time. As described in chapter 6, it is much easier for COIN/BCP to track the number of nodes during a computation than the CPU time spent so far. This simplification relies on the fact that the number of processed nodes does not vary significantly for a given test instance, even when using different parameters for local branching. Summarizing the detailed results that will be presented in this chapter, three major observations were made:</p>
         <p>9.1 Test Environment</p>
         <p>9.2 Test Results Overview</p>
         <p>• Local branching can find better solutions than standard branching early in the computation especially for large, complex test instances. • For smaller, easier test instances local branching did not show such advantages or was inferior to standard branching. • The settings for local branching, especially the value of k , the number of variables to be fixed, and the maximum number of nodes depend on the problem size. It was not possible to find a parameter configuration that delivers good results for all problem sizes (or even a rule of thumb to account for problem complexity).</p>
         <p>Since local branching showed its benefits primarily with larger test instances, most of the detailed results will concentrate on larger knapsack problems.  9.2.1 Final Objective Comparison The first method for comparing two configurations is by comparing the best feasible solution found in a given timespan. When both configurations found the same feasible solution (or at least two solutions with the same objective value), the configuration which processed less nodes to find this solution is considered better. 9.2.2 Online Performance As an artificial measure for determining the efficiency of the algorithm an online performance rating was introduced. The basic idea is to plot the best objective value over time, and then calculate the sum of the area below:</p>
         <p>with objval ( x ) being the interpolated final objective value for the number of processed nodes x and nodes max the total number of processed nodes. By adding a monotonically decreasing weight function, the online performance favors algorithms that find good solutions early in the computation, even if the final result is the same. We chose a simple inverse exponential weight function,</p>
         <p>The online performance rating is calculated by summing up objval ( x ) w ( x ) for x = 1 . . . nodes max and scaling the result by nodes 1 max . When comparing different configurations, all results have to be processed over the same range. We set nodes max to the maximum number of processed nodes for the last improvement in the considered configurations. <xref id="XR521" ref-type="fig" rid="F9.1">Figure 9.1</xref> shows a plot of the objective value for two different configurations on the left, and the corresponding online performance weight function on the right. While both configurations ultimately find almost equally good solutions, configuration 6 yields better solutions earlier in the computation, so its online performance score is greater than that of configuration 1 (the former configuration is actually using local branching, while the latter is not.) As explained in the previous chapters, there are several key parameters that influence the performance of local branching. In short, the user has to decide on the following parameters:</p>
         <p>mknapcb9.txt/18 mknapcb9.txt/18 217400 1 [<xref id="XR524" ref-type="bibr" rid="R1">1</xref>] [<xref id="XR525" ref-type="bibr" rid="R1">1</xref>] 217200 [<xref id="XR526" ref-type="bibr" rid="R2">2</xref>] 0.95 value 217000 0.9 0.85 216800 objective 216600 w(x) 0.75 0.8 216400 0.7 final 216200 0.65 216000 0.6 215800 0.55 0 3350 6700 10050 0 3350 6700 10050 processed nodes processed nodes</p>
         <fig id="F9.1">
            <caption>
               <p>Figure 9.1: Final objective value and the corresponding online performance weights for a test instance.</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>9.3 Local Branching Configurations</p>
         <p>• The value of k for the Hamming distance constraint determines how many binary variables can flip inside a single local tree. • Fixing some variables helps to reduce the problem complexity inside the tree defined by the chosen k value. • Defining node limits on local trees avoids stagnation. • By adapting the values of k and the numbers of variables to be fixed at run-time, the search can be narrowed or broadened at will.</p>
         <p>The variables used for controlling these parameters are described in chapter 8. In the test results, the following abbreviated notation will be used:</p>
         <p>• k = k initial , k scaling , k { min | max } defines the initial k value, the factor k is multiplied with when a tree is aborted, and the minimum (when the factor is less than 1) or maximum (when the factor is greater than 1) value for k . When only one value is given, k remains constant throughout the computation. • Variable fixing: f initial , f increment , f max contains the number of variables fixed when a local tree using a new incumbent solution is started, and the increment and maximum values to be used when a tree is aborted and restarted with the same initial solution. • Maximum nodes is the maximum number of nodes for a local tree, no node limit is used when this parameter is omitted.</p>
         <p>• Maximum nodes without improvement declares the maximum number of nodes to be created in a local tree without finding an improved feasible solution. When this value is omitted, no such node limit is imposed on local trees.</p>
         <p>All parameters depend on the size of the test instance, so no reasonable default values can be provided.  The objective of the following tests is to accelerate the process of finding good solutions early in the computation, while the final objective is not of paramount importance. For these test runs, a time limit of 10 minutes per instance was used. Since there was no single configuration that succeeded in all presented problem instances, the results are separated by problem size. For the initial solution a greedy heuristic generated two solutions, the first using relative weights as in equation (7.5) as efficiency measure and the second using the first LP optimum. The better solution (regarding the objective value) was used as initial solution. 9.4.1 Local Branching and Node Limits The first set of test runs uses standard local branching and for some instances node limits, but no cut generation. We start with examining the moderately sized mknapcb7 test instance collection from the OR Library [<xref id="XR540" ref-type="bibr" rid="R2">2</xref>].</p>
         <p>9.4 Short-Time Tests</p>
      </sec>
      <sec>
         <title>Mknapcb7: 100 variables, 30 constraints</title>
         <p>The following configurations have been tested for all 30 test instances of mknapcb7:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 13 , variable fixing: 0 . 1 , no node limit. k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 10000 .</p>
         <p>For these test instances, the local tree search without a node limit was superior to a local tree search with the same parameters, but with a limit on the total number of nodes per local tree. When comparing final objectives with the conditions described earlier in this chapter, configuration (2) wins against (1) with a clear advantage of 27 : 10 (the numbers do not add up to 30 because of some ties.) For configuration (3), the direct comparison yields only a slight advantage of 19 : 17 for local branching. Introducing the online performance rating, (2) loses some of its advantage, but still showing a distinct advantage of 22 : 14 when compared to (1). Configuration (3) stays at 19 : 17 , not showing a significant benefit from local branching. Comparing the number of local trees, (2) mostly used only a single local tree with an average of 1 . 2 local trees, (3) created an average of 27 . 1 trees per test instance. <xref id="XR546" ref-type="fig" rid="F9.2">Figure 9.2</xref> shows two sample graphs from this test series.</p>
         <p>mknapcb7.txt/2 mknapcb7.txt/3 20800 21500 [<xref id="XR549" ref-type="bibr" rid="R1">1</xref>] [<xref id="XR550" ref-type="bibr" rid="R1">1</xref>] 20700 [<xref id="XR551" ref-type="bibr" rid="R2">2</xref>] 21400 [<xref id="XR552" ref-type="bibr" rid="R2">2</xref>] value 20600 value 21300 20500 21200 objective 20400 20300 objective 21100 21000 20200 20900 final 20100 final 20800 20000 20700 19900 20600 0 425 850 1275 0 2750 5500 8250 processed nodes processed nodes</p>
         <fig id="F9.2">
            <caption>
               <p>Figure 9.2: Two sample graphs for mknapcb7 showing a benefit for local branching on the left, and an advantage for normal Branch and Cut on the right.</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>mknapcb8.txt/15 mknapcb8.txt/26 107300 147480 [<xref id="XR555" ref-type="bibr" rid="R1">1</xref>] [<xref id="XR556" ref-type="bibr" rid="R1">1</xref>] 107200 [<xref id="XR557" ref-type="bibr" rid="R2">2</xref>] 147460 [<xref id="XR558" ref-type="bibr" rid="R2">2</xref>] [<xref id="XR559" ref-type="bibr" rid="R3">3</xref>] [<xref id="XR560" ref-type="bibr" rid="R3">3</xref>] value 107100 value 147440 147420 objective 107000 106900 objective 147400 147380 final 106800 final 147360 106700 147340 106600 147320 0 8475 16950 25425 0 7300 14600 21900 processed nodes processed nodes</p>
         <fig id="F9.3">
            <caption>
               <p>Figure 9.3: Two sample graphs for mknapcb8 showing the benefit of node limits for configuration (3) against the same configuration without node limit (2) and standard Branch and Cut (1).</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
      </sec>
      <sec>
         <title>Mknapcb8: 250 variables, 30 constraints</title>
         <p>For the larger test instances of mknapcb8, imposing node limits proved to be more beneficial. A local branching setup similar to the previous configurations proved to be superior to standard Branch and Cut, especially when considering the online performance rating. The following configurations delivered the best results compared to standard Branch and Cut:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 13 , variable fixing: 0 . 1 , no node limit k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 5 , maximum nodes per tree: 5000</p>
         <p>Comparing the final objective values, both (2) and (3) showed an advantage of 18 : 14 against (1). When comparing the online performance rating, (2) exhibited a slight disadvantage of 15 : 17 , while (3) apparently benefited from the node limit and won clearly by 21 : 11 . <xref id="XR567" ref-type="fig" rid="F9.3">Figure 9.3</xref> again shows two sample plots comparing the three parameter settings.</p>
         <table-wrap id="Tx570">
            <caption>
               <p>Table 9.1: Summary table for the tested mknapcb problems. Each row represents 30 different instances with the given dimensions. The ratios given for final objective value and online performance compare the best local branching configuration to standard Branch and Cut.</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> n</td>
                     <td> d</td>
                     <td> best</td>
                     <td> final objective</td>
                     <td> online performance</td>
                     <td> Wilcoxon</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> mknapcb7</td>
                     <td> 100</td>
                     <td> 30</td>
                     <td> (2)</td>
                     <td> 27 : 10</td>
                     <td> 22 : 14</td>
                     <td> &lt; 0 . 01%</td>
                  </tr>
                  <tr>
                     <td> mknapcb8</td>
                     <td> 250</td>
                     <td> 30</td>
                     <td> (3)</td>
                     <td> 18 : 14</td>
                     <td> 21 : 11</td>
                     <td> 15 . 4%</td>
                  </tr>
                  <tr>
                     <td> mknapcb9</td>
                     <td> 500</td>
                     <td> 30</td>
                     <td> (3)</td>
                     <td> 19 : 11</td>
                     <td> 17 : 13</td>
                     <td> 2 . 1%</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
      </sec>
      <sec>
         <title>Mknapcb9: 500 variables, 30 constraints</title>
         <p>The largest class of test instances from the OR Library, mknapcb9, exhibited similar behavior regarding local branching. Compared to standard branch and cut, two configurations exhibited similar performance.</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 10 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 5000 k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 1000</p>
         <p>Regarding the final objective values, configuration (2) achieves a ratio of 18 : 12 against standard Branch and Cut, reducing the node limit leads to a further minor improvement of 19 : 11 . The online performance ratings are a bit less favorable, showing a 17 : 13 advantage for both local branching configurations. Configuration (2) created an average of 12.2 local trees per computation, while (3) used an average of 67.8 local trees. <xref id="XR577" ref-type="table" rid="T9.1">Table 9.1</xref> summarizes the results and also contains the result of a Wilcoxon rank sum test . It represents the error probability for the assumption that the first method performs on average better than the second. It is generated by creating two columns, one for each configuration, and setting a 1 where a configuration achieved the best result for a given test instance, and 0 otherwise. We do not compare the final objective values, because even when two configurations achieved the same value, we prefer the configuration that reached it with fewer processed nodes. A Wilcoxon score close to 0 means that the the local branching configuration is very likely to be better than the standard Branch and Cut algorithm, a value close to 0.5 means that no significant difference exists. For mknapcb7 and mknapcb9 the Wilcoxon test clearly indicates that the local branching results are better than the standard Branch and Cut results, while the result for mknapcb8 (0.15) is less clear. The detailed test results for all test instances are given in tables 9.2, 9.3, and 9.4.</p>
      </sec>
      <sec>
         <title>MK-gk11: 2500 variables, 100 constraints</title>
         <p>The huge eleventh test instance from the problems taken from the Hearin Center for Enterprise Science [<xref id="XR580" ref-type="bibr" rid="R14">14</xref>] shows clear advantages for most local branching configurations. The tested configurations are:</p>
      </sec>
      <sec>
         <title>(1)</title>
         <p>Standard Branch and Cut.</p>
         <p>MK-gk11/0 95220 [<xref id="XR585" ref-type="bibr" rid="R1">1</xref>] 95210 [<xref id="XR586" ref-type="bibr" rid="R2">2</xref>] [<xref id="XR587" ref-type="bibr" rid="R3">3</xref>] value 95200 [<xref id="XR588" ref-type="bibr" rid="R4">4</xref>] 95190 objective 95180 95170 final 95160 95150 95140 75 150 225 processed nodes</p>
         <fig id="F9.4">
            <caption>
               <p>Figure 9.4: Clear advantages for local branching in test instance MK-gk11.</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
      </sec>
      <sec>
         <title>(2) (3) (4)</title>
         <p>k = 5 , variable fixing: 0 . 05 , 0 . 2 , 0 . 5 , maximum nodes per tree: 250 k = 20 , no variable fixing, no node limit. k = 10 , no variable fixing, maximum number of nodes: 1000</p>
         <p>All local branching configurations both showed faster convergence and better final objective values. The plot of the objective value is given in <xref id="XR593" ref-type="fig" rid="F9">figure 9.4</xref>. 9.4.2 Cut Generation The idea of cut generation is to find valid inequalities that are violated by the current LP optimum as described in section 2.3, thus decreasing the LP optimum and increasing the chances to prune a subproblem. The size of the search tree can be reduced significantly at the expense of computationally expensive cut generation. In our test results, cut generation did not lead to a significant advantage for local branching in comparison to the standard Branch and Cut algorithm, instead it compensated the advantage of local branching. We used COIN/CGL’s generic cut generators, the most efficient proved to be the knapsack cover cut generator. Others, like the Gomory cut generator, did not improve the results but slowed down the computation.</p>
      </sec>
      <sec>
         <title>Mknapcb7: 100 variables, 30 constraints</title>
         <p>The following parameter configurations have been tested:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 13 , variable fixing: 0 . 1 , no node limit. k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 1000 .</p>
         <p>Without cut generation, configuration (2) beat standard Branch and Cut by 27 : 10 . With cut generation, the standard algorithm reaches a tie result of 18 : 18 both against (2) and (3). Regarding the online performance rating, local branching gains a slight advantage of 18 : 16 for configuration (2) and 19 : 15 for (3).</p>
         <table-wrap id="Tx602">
            <caption>
               <p>Table 9.2: All results for mknapcb7 using a single initial solution ( n = 100 , d = 30 .)</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td/>
                     <td> final</td>
                     <td> objective</td>
                     <td> value</td>
                     <td> online performance</td>
                     <td> number of</td>
                     <td> local trees</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> instance</td>
                     <td> (1)</td>
                     <td> (2)</td>
                     <td> (3)</td>
                     <td> (1) (2) (3)</td>
                     <td> (1) (2)</td>
                     <td> (3)</td>
                  </tr>
                  <tr>
                     <td> 0</td>
                     <td> 21946</td>
                     <td> 21946</td>
                     <td> 21946</td>
                     <td> 13719.11 13719.11 13719.11</td>
                     <td> 0 2</td>
                     <td> 29</td>
                  </tr>
                  <tr>
                     <td> 1</td>
                     <td> 21716</td>
                     <td> 21716</td>
                     <td> 21716</td>
                     <td> 13700.76 13710.87 13699.94</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 2</td>
                     <td> 20754</td>
                     <td> 20754</td>
                     <td> 20754</td>
                     <td> 13096.68 13102.44 13102.44</td>
                     <td> 0 1</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 3</td>
                     <td> 21464</td>
                     <td> 21464</td>
                     <td> 21464</td>
                     <td> 13532.09 13511.85 13524.59</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 4</td>
                     <td> 21844</td>
                     <td> 21844</td>
                     <td> 21844</td>
                     <td> 13797.81 13809.01 13809.01</td>
                     <td> 0 1</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 5</td>
                     <td> 22176</td>
                     <td> 22176</td>
                     <td> 22176</td>
                     <td> 14005.53 14005.90 14006.11</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 6</td>
                     <td> 21799</td>
                     <td> 21799</td>
                     <td> 21799</td>
                     <td> 13718.36 13723.17 13723.17</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 7</td>
                     <td> 21397</td>
                     <td> 21327</td>
                     <td> 21327</td>
                     <td> 13493.88 13472.05 13473.10</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 8</td>
                     <td> 22471</td>
                     <td> 22475</td>
                     <td> 22482</td>
                     <td> 14187.96 14201.18 14210.38</td>
                     <td> 0 1</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 9</td>
                     <td> 20983</td>
                     <td> 20983</td>
                     <td> 20983</td>
                     <td> 13230.11 13230.08 13223.38</td>
                     <td> 0 1</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 10</td>
                     <td> 40691</td>
                     <td> 40691</td>
                     <td> 40767</td>
                     <td> 25722.46 25723.66 25742.90</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 11</td>
                     <td> 41308</td>
                     <td> 41308</td>
                     <td> 41304</td>
                     <td> 26108.12 26108.20 26106.94</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 12</td>
                     <td> 41630</td>
                     <td> 41630</td>
                     <td> 41630</td>
                     <td> 26304.63 26304.63 26304.63</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 13</td>
                     <td> 41041</td>
                     <td> 41041</td>
                     <td> 41041</td>
                     <td> 25937.69 25937.69 25937.69</td>
                     <td> 0 1</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 14</td>
                     <td> 40889</td>
                     <td> 40889</td>
                     <td> 40889</td>
                     <td> 25842.11 25842.93 25838.43</td>
                     <td> 0 1</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 15</td>
                     <td> 41028</td>
                     <td> 41058</td>
                     <td> 41022</td>
                     <td> 25934.05 25921.21 25915.76</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 16</td>
                     <td> 41062</td>
                     <td> 41038</td>
                     <td> 41038</td>
                     <td> 25957.11 25936.80 25935.54</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 17</td>
                     <td> 42719</td>
                     <td> 42719</td>
                     <td> 42719</td>
                     <td> 26976.12 26979.48 26979.01</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 18</td>
                     <td> 42230</td>
                     <td> 42230</td>
                     <td> 42230</td>
                     <td> 42230.00 42230.00 42230.00</td>
                     <td> 0 2</td>
                     <td> 49</td>
                  </tr>
                  <tr>
                     <td> 19</td>
                     <td> 41700</td>
                     <td> 41700</td>
                     <td> 41700</td>
                     <td> 41700.00 41700.00 41700.00</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 20</td>
                     <td> 57494</td>
                     <td> 57494</td>
                     <td> 57494</td>
                     <td> 36319.24 36320.82 36322.32</td>
                     <td> 0 1</td>
                     <td> 28</td>
                  </tr>
                  <tr>
                     <td> 21</td>
                     <td> 60027</td>
                     <td> 60027</td>
                     <td> 60026</td>
                     <td> 37944.30 37943.28 37943.50</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 22</td>
                     <td> 58052</td>
                     <td> 58015</td>
                     <td> 58052</td>
                     <td> 36677.22 36670.90 36688.60</td>
                     <td> 0 1</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 23</td>
                     <td> 60776</td>
                     <td> 60776</td>
                     <td> 60776</td>
                     <td> 38415.55 38415.96 38415.96</td>
                     <td> 0 2</td>
                     <td> 34</td>
                  </tr>
                  <tr>
                     <td> 24</td>
                     <td> 58884</td>
                     <td> 58884</td>
                     <td> 58884</td>
                     <td> 37214.71 37214.71 37214.71</td>
                     <td> 0 2</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 25</td>
                     <td> 60011</td>
                     <td> 60011</td>
                     <td> 60011</td>
                     <td> 37919.29 37910.45 37910.45</td>
                     <td> 0 2</td>
                     <td> 30</td>
                  </tr>
                  <tr>
                     <td> 26</td>
                     <td> 58132</td>
                     <td> 58132</td>
                     <td> 58132</td>
                     <td> 36737.55 36737.63 36741.17</td>
                     <td> 0 1</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 27</td>
                     <td> 59064</td>
                     <td> 59064</td>
                     <td> 59064</td>
                     <td> 37325.45 37333.21 37333.21</td>
                     <td> 0 2</td>
                     <td> 29</td>
                  </tr>
                  <tr>
                     <td> 28</td>
                     <td> 58975</td>
                     <td> 58975</td>
                     <td> 58975</td>
                     <td> 37277.47 37278.91 37276.15</td>
                     <td> 0 1</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 29</td>
                     <td> 60603</td>
                     <td> 60603</td>
                     <td> 60603</td>
                     <td> 38295.77 38299.39 38299.39</td>
                     <td> 0 1</td>
                     <td> 27</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <p>instance 0 1 2 3 4 56601 56629 35748.73 35773.20 5 6 56206 56180 35513.51 35510.10 7 56392 56413 35636.01 35644.80 8 57413 57429 36271.66 36272.94 9 56447 35658.13 35658.13 10 107746 107732 68106.39 68088.89 11 108335 108335 68469.79 68459.16 12 106375 106415 67244.95 67256.61 13 106780 106766 67498.51 67490.14 14 107349 107414 67856.07 67880.51 15 107177 107246 67729.09 67731.84 16 106294 106241 67167.78 67144.31 17 103998 103977 65727.41 65724.06 18 106736 106751 67452.37 67460.47 19 105675 105681 66792.13 66798.56 20 150081 150097 94866.09 94863.58 21 149881 149854 94729.76 94706.55 22 152971 152960 96692.10 96687.17 23 153177 153177 96824.54 96824.86 24 150287 150287 94938.43 94942.16 25 148520 148544 93871.33 93889.45 26 147454 147471 93209.11 93206.87 27 152817 152877 96585.86 96635.54 28 149570 149554 94533.41 94538.37 29 149586 149586 94553.56 94554.05</p>
         <p>final objective value online performance number of local (1) (2) (3) (1) (2) (3) (1) (2) 56824 56824 35910.64 35910.64 0 1 58310 58332 36842.34 36860.57 0 1 56498 56493 35706.18 35701.40 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1</p>
      </sec>
      <sec>
         <title>56824 35914.54 58364 36878.71 56508 35706.59 56930 56930 56930 35989.96 35989.96 35989.96 56629 35783.77 57146 57146 57146 35983.42 35983.42 35983.42 56219 35524.30 56413 35650.11 57429 36278.42 56447 56447 35659.16 107746 68108.02 108336 68469.89 106442 67257.55 106786 67499.44 107414 67885.23 107246 67736.79 106297 67181.55 103998 65727.95 106758 67468.22 105716 66814.53 150097 94866.84 149907 94742.37 152973 96697.38 153190 96825.11 150287 94947.21 148544 93891.89 147471 93218.07 152912 96656.17 149570 94541.43 149595 94554.53</title>
         <table-wrap id="T9.3">
            <caption>
               <p>Table 9.3: All results for mknapcb8 using a initial solution ( n = 250 , d = 30 .)</p>
            </caption>
         </table-wrap>
         <p>final objective value online performance number of</p>
         <p>0 115841 115809 73184.48 73192.49 0 12 1 114623 114667 72420.52 72456.21 0 12 2 116541 116607 73647.29 73693.17 0 14 3 115096 115128 72753.14 72760.71 0 11 4 116266 116316 73495.78 73519.30 0 14 5 115563 115584 73032.02 73048.46 0 12 6 113928 113928 72006.14 72006.60 0 11 7 114190 114137 72161.72 72126.01 0 14 8 115133 115419 72771.30 72822.16 0 12 9 116891 116929 73881.07 73888.93 0 12 10 217925 217995 137747.72 137797.39 0 12 11 214517 214626 135580.86 135589.10 0 12 12 215835 215844 136399.86 136372.69 0 12 13 217827 217805 137664.20 137665.81 0 11 14 215515 215535 136212.21 136221.81 0 10 15 215697 215717 136337.13 136351.87 0 11 16 215772 215780 136388.68 136381.53 0 12 17 216366 216341 136763.70 136744.46 0 11 18 217290 217196 137312.79 137278.83 0 11 19 214624 214592 135632.98 135649.19 0 11 20 301643 301627 190656.90 190667.01 0 12 21 299957 299945 189579.82 189577.88 0 14 22 304985 304985 192790.68 192790.07 0 14 23 301854 301891 190787.31 190803.87 0 12 24 304411 304350 192420.91 192368.46 0 12 25 296891 296891 187672.97 187664.53 0 13 26 303261 303262 191663.09 191671.50 0 14 27 306890 306892 193976.43 193981.49 0 12 28 303088 303083 191588.20 191565.66 0 15 29 300479 300439 189896.47 189905.03 0 11</p>
      </sec>
      <sec>
         <title>115850 73215.35 114701 72473.47 116661 73729.35 115152 72784.41 116385 73553.33 115600 73065.10 113982 72038.81 114190 72170.82 115419 72943.75 116988 73908.53 217995 137801.07 214626 135609.00 215844 136411.96 217827 137671.45 215559 136263.63 215722 136366.40 215780 136398.82 216419 136784.92 217290 137348.63 214633 135652.21 301643 190667.63 299987 189618.86 304994 192792.68 301955 190809.87 304413 192423.62 296959 187705.63 303270 191687.73 306937 193995.36 303111 191592.30 300499 189930.56</title>
         <table-wrap id="T9.4">
            <caption>
               <p>Table 9.4: All results for mknapcb9 using a single initial solution ( n = 500 , d = 30 .)</p>
            </caption>
         </table-wrap>
         <p>9.4.3 Multiple Initial Solutions Creating multiple local trees in the beginning of the computation can help to improve the initial performance of the local branching algorithm. The processor time is distributed over several local trees, preferring those with better nodes (according to the tree search strategy, i.e. those with better bounds.) As described in chapter 8, three different initial solutions were used:</p>
         <p>• The feasible solution generated heuristically from the first LP result. • A solution returned by a greedy heuristic using the relative weight as an efficiency measure. • A solution returned by the same heuristic including the weight distribution as an efficiency measure.</p>
         <p>Compared to local branching with a single initial solution the results improved considerably. For the mknapcb7 test instances, the three initial local trees contributed equally to the best found solution, that is, the initial trees were of roughly the same size. For the mknapcb8 and mknapcb9 instances, the tree based on the first LP result was most often superior to the trees based on efficiency measures, meaning that the latter two trees were often completed after a few nodes. Apparently the greedy heuristics with efficiency values worked better for the smaller mknapcb7 instances than for the more complex mknapcb8 and mknapcb9 instances.</p>
      </sec>
      <sec>
         <title>Mknapcb7: 100 variables, 30 constraints</title>
         <p>The following configurations have been tested:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 13 , variable fixing: 0 . 1 , no node limit. k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 10000 .</p>
         <p>Regarding the final objective values, both local branching configurations showed an 24 : 10 advantage to standard Branch and Cut. While this result is similar to what local branching with a single initial solution achieved, the pseudo-concurrent tree exploration shows more benefits when looking at the online performance rating. Both local branching configurations achieved a clear advantage of 25 : 9 compared to local branching, which is considerably better than the results using a single initial solution.</p>
      </sec>
      <sec>
         <title>Mknapcb8: 250 variables, 30 constraints</title>
         <p>As in section 9.4.1, the following configurations have been tested for mknapcb8:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 13 , variable fixing: 0 . 1 , no node limit k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 5 , maximum nodes per tree: 5000</p>
         <table-wrap id="Tx633">
            <caption>
               <p>Table 9.5: Summary table for the tests using multiple initial solutions, including a Wilcoxon probability score for the assumption that the final objective values of standard Branch and Cut are better than the given local branching configuration.</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> n</td>
                     <td> d</td>
                     <td> best</td>
                     <td> final objective online</td>
                     <td> performance</td>
                     <td> Wilcoxon</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> mknapcb7</td>
                     <td> 100</td>
                     <td> 30</td>
                     <td> (2)</td>
                     <td> 24 : 10</td>
                     <td> 25 : 9</td>
                     <td> 0 . 02%</td>
                  </tr>
                  <tr>
                     <td> mknapcb8</td>
                     <td> 250</td>
                     <td> 30</td>
                     <td> (2)</td>
                     <td> 22 : 8</td>
                     <td> 24 : 6</td>
                     <td> 0 . 02%</td>
                  </tr>
                  <tr>
                     <td> mknapcb9</td>
                     <td> 500</td>
                     <td> 30</td>
                     <td> (3)</td>
                     <td> 22 : 8</td>
                     <td> 25 : 5</td>
                     <td> 0 . 02%</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <p>Comparing the final objective values, configuration (2) showed an advantage of 22 : 8 against standard Branch and Cut, (3) had an advantage of 21 : 9 . Regarding online performance, (2) showed a clear advantage of 24 : 6 and (3) an advantage of 26 : 4 compared to standard Branch and Cut.</p>
      </sec>
      <sec>
         <title>Mknapcb9: 500 variables, 30 constraints</title>
         <p>The following configurations were tested:</p>
      </sec>
      <sec>
         <title>(1) (2) (3)</title>
         <p>Standard Branch and Cut. k = 10 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 5000 . k = 13 , variable fixing: 0 . 1 , 0 . 1 , 0 . 8 , maximum nodes per tree: 1000 .</p>
         <p>Both (2) and (3) showed clearly superior results to (1), with (2) showing a slight advantage of 17 : 13 and (3) beating standard Branch and Cut by 22 : 8 . The online performance ratings clearly favor the local branching configurations: (2) beats (1) by 24 : 6 , (3) beats (1) by 25 : 5 . Compared with the same configuration without multiple initial solutions, (2) exhibited an advantage of 19 : 11 for the final objective value and 27 : 3 for the online performance rating. <xref id="XR641" ref-type="table" rid="T9.5">Table 9.5</xref> summarizes the results for these test runs. The detailed results for all test instances of mknapcb7, mknapcb8 and mknapcb9 are given in tables 9.6, 9.7, and 9.8. Bold values indicate the best result for a single instance. Note that it is possible for more than one configuration to achieve the “best” result. The test runs described in the last section are useful for testing the short-time heuristical behavior a large variety of local branching configurations. Testing the instances of the OR library [<xref id="XR644" ref-type="bibr" rid="R2">2</xref>] with longer running times (up to one hour) did not reveal significantly different behavior. However, the very large eleventh instance of the second set of test instances [<xref id="XR645" ref-type="bibr" rid="R14">14</xref>] with 2500 variables and 100 constraints was an interesting target for examining long-run behavior. The huge core matrix dramatically slows down the LP solver, affecting the significance of the results of a 10 minute test run. Increasing the CPU time to 2 hours showed interesting results: local branching extended its lead (with unmodified parameters), standard branch and cut was clearly inferior to all tested configurations. <xref id="XR646" ref-type="fig" rid="F9.5">Figure 9.5</xref> shows the final objective plots for the following configurations:</p>
         <p>9.5 Long Runs</p>
         <table-wrap id="Tx649">
            <caption>
               <p>Table 9.6: All results for mknapcb7 using multiple initial solutions ( n = 100 , d = 30 .)</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td/>
                     <td> final</td>
                     <td> objective</td>
                     <td> value</td>
                     <td> online performance</td>
                     <td> number of</td>
                     <td> local trees</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> instance</td>
                     <td> (1)</td>
                     <td> (2)</td>
                     <td> (3)</td>
                     <td> (1) (2) (3)</td>
                     <td> (1) (2)</td>
                     <td> (3)</td>
                  </tr>
                  <tr>
                     <td> 0</td>
                     <td> 21946</td>
                     <td> 21946</td>
                     <td> 21946</td>
                     <td> 13719.11 13719.11 13719.11</td>
                     <td> 0 3</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 1</td>
                     <td> 21716</td>
                     <td> 21716</td>
                     <td> 21716</td>
                     <td> 13698.34 13720.04 13720.04</td>
                     <td> 0 3</td>
                     <td> 28</td>
                  </tr>
                  <tr>
                     <td> 2</td>
                     <td> 20754</td>
                     <td> 20754</td>
                     <td> 20754</td>
                     <td> 13096.68 13114.77 13114.77</td>
                     <td> 0 3</td>
                     <td> 31</td>
                  </tr>
                  <tr>
                     <td> 3</td>
                     <td> 21464</td>
                     <td> 21464</td>
                     <td> 21464</td>
                     <td> 13517.23 13566.96 13566.96</td>
                     <td> 0 3</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 4</td>
                     <td> 21844</td>
                     <td> 21814</td>
                     <td> 21814</td>
                     <td> 13803.20 13792.24 13792.50</td>
                     <td> 0 3</td>
                     <td> 20</td>
                  </tr>
                  <tr>
                     <td> 5</td>
                     <td> 22176</td>
                     <td> 22176</td>
                     <td> 22176</td>
                     <td> 14005.53 14018.62 14018.62</td>
                     <td> 0 3</td>
                     <td> 21</td>
                  </tr>
                  <tr>
                     <td> 6</td>
                     <td> 21799</td>
                     <td> 21799</td>
                     <td> 21799</td>
                     <td> 13718.36 13758.35 13758.35</td>
                     <td> 0 3</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 7</td>
                     <td> 21397</td>
                     <td> 21327</td>
                     <td> 21397</td>
                     <td> 13493.88 13483.27 13503.57</td>
                     <td> 0 3</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 8</td>
                     <td> 22471</td>
                     <td> 22493</td>
                     <td> 22525</td>
                     <td> 14187.96 14218.78 14229.36</td>
                     <td> 0 3</td>
                     <td> 16</td>
                  </tr>
                  <tr>
                     <td> 9</td>
                     <td> 20983</td>
                     <td> 20983</td>
                     <td> 20983</td>
                     <td> 13223.38 13262.97 13262.97</td>
                     <td> 0 3</td>
                     <td> 29</td>
                  </tr>
                  <tr>
                     <td> 10</td>
                     <td> 40691</td>
                     <td> 40767</td>
                     <td> 40767</td>
                     <td> 25722.66 25737.27 25745.08</td>
                     <td> 0 3</td>
                     <td> 19</td>
                  </tr>
                  <tr>
                     <td> 11</td>
                     <td> 41308</td>
                     <td> 41308</td>
                     <td> 41304</td>
                     <td> 26108.12 26110.50 26109.40</td>
                     <td> 0 3</td>
                     <td> 23</td>
                  </tr>
                  <tr>
                     <td> 12</td>
                     <td> 41630</td>
                     <td> 41630</td>
                     <td> 41630</td>
                     <td> 26304.63 26313.70 26313.70</td>
                     <td> 0 3</td>
                     <td> 24</td>
                  </tr>
                  <tr>
                     <td> 13</td>
                     <td> 41041</td>
                     <td> 41041</td>
                     <td> 41041</td>
                     <td> 25909.87 25925.38 25925.38</td>
                     <td> 0 3</td>
                     <td> 30</td>
                  </tr>
                  <tr>
                     <td> 14</td>
                     <td> 40889</td>
                     <td> 40889</td>
                     <td> 40872</td>
                     <td> 25830.92 25842.90 25838.46</td>
                     <td> 0 3</td>
                     <td> 23</td>
                  </tr>
                  <tr>
                     <td> 15</td>
                     <td> 41028</td>
                     <td> 41058</td>
                     <td> 41058</td>
                     <td> 25934.27 25937.79 25931.24</td>
                     <td> 0 3</td>
                     <td> 23</td>
                  </tr>
                  <tr>
                     <td> 16</td>
                     <td> 41062</td>
                     <td> 41038</td>
                     <td> 41062</td>
                     <td> 25957.11 25942.97 25956.31</td>
                     <td> 0 3</td>
                     <td> 37</td>
                  </tr>
                  <tr>
                     <td> 17</td>
                     <td> 42719</td>
                     <td> 42719</td>
                     <td> 42719</td>
                     <td> 26972.46 27001.34 27001.34</td>
                     <td> 0 3</td>
                     <td> 21</td>
                  </tr>
                  <tr>
                     <td> 18</td>
                     <td> 42230</td>
                     <td> 42230</td>
                     <td> 42230</td>
                     <td> 42230.00 42230.00 42230.00</td>
                     <td> 0 4</td>
                     <td> 30</td>
                  </tr>
                  <tr>
                     <td> 19</td>
                     <td> 41700</td>
                     <td> 41700</td>
                     <td> 41700</td>
                     <td> 41700.00 41700.00 41700.00</td>
                     <td> 0 3</td>
                     <td> 22</td>
                  </tr>
                  <tr>
                     <td> 20</td>
                     <td> 57494</td>
                     <td> 57494</td>
                     <td> 57494</td>
                     <td> 36319.24 36343.09 36343.09</td>
                     <td> 0 3</td>
                     <td> 29</td>
                  </tr>
                  <tr>
                     <td> 21</td>
                     <td> 60027</td>
                     <td> 60026</td>
                     <td> 60026</td>
                     <td> 37944.30 37943.18 37943.18</td>
                     <td> 0 3</td>
                     <td> 48</td>
                  </tr>
                  <tr>
                     <td> 22</td>
                     <td> 58052</td>
                     <td> 58052</td>
                     <td> 58025</td>
                     <td> 36682.74 36681.73 36671.01</td>
                     <td> 0 3</td>
                     <td> 19</td>
                  </tr>
                  <tr>
                     <td> 23</td>
                     <td> 60776</td>
                     <td> 60776</td>
                     <td> 60776</td>
                     <td> 38415.55 38418.18 38418.18</td>
                     <td> 0 4</td>
                     <td> 25</td>
                  </tr>
                  <tr>
                     <td> 24</td>
                     <td> 58884</td>
                     <td> 58884</td>
                     <td> 58884</td>
                     <td> 37214.71 37214.71 37214.71</td>
                     <td> 0 4</td>
                     <td> 28</td>
                  </tr>
                  <tr>
                     <td> 25</td>
                     <td> 60011</td>
                     <td> 60011</td>
                     <td> 60011</td>
                     <td> 37919.29 37931.77 37931.77</td>
                     <td> 0 4</td>
                     <td> 27</td>
                  </tr>
                  <tr>
                     <td> 26</td>
                     <td> 58132</td>
                     <td> 58132</td>
                     <td> 58132</td>
                     <td> 36737.55 36748.63 36748.63</td>
                     <td> 0 3</td>
                     <td> 30</td>
                  </tr>
                  <tr>
                     <td> 27</td>
                     <td> 59064</td>
                     <td> 59064</td>
                     <td> 59064</td>
                     <td> 37325.45 37336.68 37336.68</td>
                     <td> 0 3</td>
                     <td> 26</td>
                  </tr>
                  <tr>
                     <td> 28</td>
                     <td> 58975</td>
                     <td> 58975</td>
                     <td> 58975</td>
                     <td> 37277.47 37280.14 37279.40</td>
                     <td> 0 3</td>
                     <td> 20</td>
                  </tr>
                  <tr>
                     <td> 29</td>
                     <td> 60603</td>
                     <td> 60603</td>
                     <td> 60603</td>
                     <td> 38295.77 38307.77 38307.77</td>
                     <td> 0 3</td>
                     <td> 24</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <p>final objective value online performance number of local</p>
         <p>0 56824 56824 35912.01 35912.54 0 3 1 58310 36842.34 0 3 2 56498 56493 35708.82 35713.55 0 3 3 56930 35989.96 0 3 4 56601 56629 35783.77 35782.33 0 3 5 57146 35983.42 0 3 6 56206 56253 35513.51 35543.21 0 3 7 56392 56448 35636.01 35672.34 0 3 8 57429 57429 36271.66 36293.59 0 3 9 56447 35659.16 0 3 10 107746 68108.02 0 3 11 108336 108335 68469.79 68482.54 0 3 12 106440 106415 67257.55 67265.87 0 3 13 106780 106790 67498.51 67507.29 0 3 14 107349 107374 67856.07 67874.43 0 3 15 107177 67729.09 0 3 16 106294 106283 67167.78 67181.78 0 3 17 103998 103995 65727.95 65737.10 0 3 18 106736 106758 67452.37 67480.32 0 3 19 105675 105723 66792.13 66827.20 0 3 20 150073 150096 94866.09 94865.84 0 3 21 149907 149862 94742.37 94720.42 0 3 22 152971 152971 96685.57 96690.15 0 3 23 153177 153177 96824.54 96830.61 0 3 24 150287 94938.43 0 3 25 148520 93871.33 0 3 26 147454 147454 93209.11 93204.43 0 3 27 152817 152877 96585.86 96623.47 0 3 28 149568 149565 94545.42 94542.68 0 3 29 149586 94553.56 0 3</p>
      </sec>
      <sec>
         <title>56824 35914.54 58520 58520 36990.73 36990.73 56553 35719.57 56930 56930 35990.17 35990.17 56629 35796.56 57146 57146 36058.77 36058.77 56253 35553.50 56457 35674.56 57433 36299.70 56447 56447 35683.55 35683.55 107746 107746 68110.74 68110.74 108352 68489.44 106442 67280.00 106806 67516.62 107414 67893.45 107246 107246 67789.77 67789.77 106305 67189.00 103998 65739.76 106800 67501.87 105742 66838.93 150097 94866.62 149907 94748.96 152973 96697.38 153190 96838.22 150287 150287 95000.95 95000.95 148544 148544 93901.04 93901.04 147471 93216.50 152877 96629.50 149570 94546.38 149595 149595 94565.92 94565.92</title>
         <table-wrap id="T9.7">
            <caption>
               <p>Table 9.7: All results for mknapcb8 using multiple initial solutions ( n = 250 , d = 30 .)</p>
            </caption>
         </table-wrap>
         <p>final objective value online performance number of</p>
         <p>0 115824 115838 73176.36 73214.09 0 8 1 114623 114699 72420.13 72488.98 0 8 2 116541 116661 73647.29 73737.49 0 7 3 115096 115180 72752.50 72799.13 0 7 4 116266 116321 73495.78 73530.94 0 8 5 115563 115604 73026.80 73060.22 0 7 6 113928 113928 72006.14 72019.41 0 8 7 114168 114174 72161.72 72163.73 0 8 8 115198 115419 72817.93 72918.72 0 8 9 116952 116886 73908.53 73891.15 0 8 10 217925 217983 137742.72 137792.81 0 8 11 214517 214626 135580.86 135641.70 0 9 12 215835 215854 136383.17 136439.97 0 8 13 217827 217769 137664.20 137654.34 0 8 14 215557 215548 136258.66 136252.59 0 8 15 215697 215718 136337.13 136364.35 0 8 16 215772 215791 136386.68 136398.33 0 9 17 216419 216403 136777.53 136788.67 0 8 18 217290 217290 137312.79 137340.43 0 8 19 214624 214581 135652.21 135641.87 0 8 20 301627 301583 190667.63 190643.19 0 9 21 299957 299984 189558.15 189583.03 0 9 22 304985 305002 192790.68 192803.92 0 9 23 301854 302001 190787.31 190862.57 0 8 24 304380 304398 192406.11 192397.45 0 9 25 296891 296892 187672.97 187673.13 0 8 26 303261 303240 191663.09 191689.34 0 8 27 306890 306910 193995.36 194001.03 0 9 28 303092 303090 191583.54 191594.76 0 9 29 300479 300444 189930.56 189920.25 0 8</p>
      </sec>
      <sec>
         <title>115850 73227.44 114701 72497.70 116661 73744.23 115206 72816.38 116385 73565.28 115741 73095.36 113979 72044.76 114190 72166.85 115419 72943.75 116988 73921.92 218042 137831.38 214626 135671.85 215885 136454.78 217827 137697.27 215559 136263.27 215726 136370.18 215792 136407.75 216419 136808.10 217312 137350.16 214633 135661.50 301643 190668.82 299987 189630.07 305002 192804.37 302004 190896.06 304411 192419.64 296986 187727.07 303285 191710.94 306911 194005.21 303111 191600.27 300488 189945.99</title>
         <table-wrap id="T9.8">
            <caption>
               <p>Table 9.8: All results for mknapcb9 using multiple initial solutions ( n = 500 , d = 30 .)</p>
            </caption>
         </table-wrap>
         <p>MK-gk11/0 95230 [<xref id="XR669" ref-type="bibr" rid="R1">1</xref>] 95220 [<xref id="XR670" ref-type="bibr" rid="R2">2</xref>] value 95210 [<xref id="XR671" ref-type="bibr" rid="R3">3</xref>] [<xref id="XR672" ref-type="bibr" rid="R4">4</xref>] 95200 [<xref id="XR673" ref-type="bibr" rid="R5">5</xref>] objective 95190 95180 95170 final 95160 95150 95140 0 757 1514 2271 3028 processed nodes</p>
         <fig id="F9.5">
            <caption>
               <p>Figure 9.5: The final objective value for MK-gk11, using a time limit of 2 hours.</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
      </sec>
      <sec>
         <title>(1) (2) (3) (4) (5)</title>
         <p>Standard Branch and Cut. k = 5 , variable fixing: 0 . 05 , 0 . 2 , 0 . 5 , maximum nodes per tree: 250 k = 5 , variable fixing: 0 . 05 , 0 . 2 , 0 . 5 , maximum nodes per tree: 500 k = 10 , variable fixing: 0 . 05 , 0 . 2 , 0 . 5 , maximum nodes per tree: 500 k = 20 , variable fixing: 0 . 05 , 0 . 2 , 0 . 5 , maximum nodes per tree: 1000</p>
         <p>Chapter 10</p>
      </sec>
      <sec>
         <title>Summary and Outlook</title>
         <p>This thesis described the implementation of a generic local branching framework based on the open source COIN/BCP Branch, Cut and Price library. Local branching is a local search heuristic that is well suited for integration in existing integer programming solvers. The framework provides the possibility to augment COIN/BCP programs with local branching search capabilities. Several extensions to the standard local branching algorithm were implemented: pseudo-concurrent exploration of multiple local trees, aborting local trees, and search space tightening through variable fixing. An encapsulated metaheuristic class offers means for a clean implementation of local branching metaheuristics without touching COIN/BCP’s internals. Rich statistical data about the current state of the local branching algorithm is provided by the framework. Methods for creating new trees, terminating existing trees or modifying the local branching search parameters are also provided. As a sample application, a Branch and Cut solver for the multidimensional knapsack problem was used to demonstrate the application of the local branching framework and to research the effects of local branching. The results for the multidimensional knapsack problem were promising: local branching showed better convergence, especially in the early stages of the computation, and showed significant benefits for large, complex test instances. By guiding the Branch and Cut solver through neighborhood search and fixing of variables, local branching allows to find better results earlier in the computation, which also leads to a reduction of the search tree complexity in the later stages. However, the benefit for relatively small test instances was less clear. The local branching framework was designed in a way that facilitates embedding local branching as a local search metaheuristic in another, higher-level search algorithm. This is the main area where future work could be expected, to use the heuristical characteristics of local branching to improve other search algorithms not based on Branch and Cut. In general, any algorithm that involves some kind of local neighborhood search lends itself to the integration of local branching. For example, an evolutionary algorithm could use the framework to generate new, better solutions based on especially promising candidates.</p>
         <p>Appendix A</p>
      </sec>
      <sec>
         <title>COIN/BCP patches</title>
         <p>The local branching framework requires some small patches to the COIN/BCP source. The patches add the following functionality to COIN/BCP:</p>
         <p>• Support for user-defined messages between LP and TM modules has been added. • A special slot for the normal tree root node is provided in the candidate queue. This is necessary because the normal root node must be used whenever a new local tree is started. • There is no way for the COIN/BCP user classes to catch all pruned nodes. Pruned nodes are now added to a list that is available to the framework’s tree manager. • When a local tree is aborted, all nodes must be removed from the candidate list. Since the nodes are potentially scattered over the candidate list, and the candidate list can contain millions of nodes, explicitly deleting all nodes may be ineffective. Instead, a list of local tree identification numbers is kept and nodes from these trees are pruned immediately instead of being returned to the tree manager. All filenames in this section are relative to the COIN/BCP root directory. A.1 Adding User-Defined Messages</p>
         <p>In order to support user-defined messages between LP and TM modules, we have to add an unique message tag for user messages and stubs for packing and unpacking routines. We start by adding two new message tags to the BCP message tag enumeration in include/BCP message tag.hpp (written in bold face):  Then we add virtual method declarations of unpack user message() to BCP tm user and BCP lp user by adding the following lines to the corresponding class definitions in include/BCP lp user.hpp and include/BCP tm user.hpp :</p>
         <p>(. . .) /** The message contains the description of a variable. */ BCP Msg VarDescription, // VG / VP - &gt; LP /** No more (improving) variables could be found. (Message body is empty.) */ BCP Msg NoMoreVars, // VG / VP - &gt; LP BCP Msg UserMessageToLp , BCP Msg UserMessageToTm } ;</p>
         <p>virtual void unpack user message(BCP lp prob&amp; prob, BCP buffer&amp; buf); We also provide a default implementation that throws an exception when called in LP/BCP lp user.cpp and TM/BCP tm user.cpp : void BCP tm user::unpack user message(BCP tm prob&amp; prob, BCP buffer&amp; buf) { throw BCP fatal error( ”BCP tm user::unpack user message() invoked but not overridden! \ n”); }</p>
         <p>The implementation for BCP lp user is identical except for the class name. The last step to be taken is to call these handlers from the tree manager and LP module message processing functions. For the tree manager, we add the following block to the switch statement of BCP tm prob::process message() in TM/BCP tm msgproc.cpp :</p>
         <p>case BCP Msg UserMessageToTm: user − &gt; unpack user message(* this , msg buf); msg buf.clear(); break ; Similarly, we add the following code to the switch statement of BCP lp prob::process message() in LP/BCP lp msgproc.cpp : case BCP Msg UserMessageToLp: user − &gt; unpack user message(* this , msg buf); msg buf.clear(); break ; A.2 Extending the Candidate List</p>
         <p>When local trees can be spawned before the previous tree terminated, the normal root node (the sibling of the local tree root) has to be extracted from the candidate list. The easiest and fastest way to achieve this goal is to store the normal root node in an extra variable and modify the methods to insert and retrieve items.  A.2.1 include/BCP tm node.hpp We start with modifying include/BCP tm node.hpp . We have to add two member variables to BCP node queue which is used to store the candidate list. Then we add a new parameter to BCP node queue::insert that permits to insert a normal root node without using the extra slot. This is used when the normal root node is the last remaining node and should be returned to the candidate list. By setting a default value, existing calls to this function do not need to be modified.</p>
         <p>/** root node of the “normal” tree, to be used when all local trees are processed or a new tree should be opened */ BCP tm node* normal root node;</p>
         <p>/** use normal root node instead of a candidate from the queue the next time top() is called */ bool use normal root node;</p>
         <p>/** Insert a new node into the queue. */ void insert(BCP tm node* node, bool replace normal root node = true ); We also slightly modify the inline functions empty() and top() to account for the normal root variable. /** Return whether the queue is empty or not */ inline bool empty() const { return !normal root node &amp;&amp; pq.size() == 1; } /** Return the top member of the queue */ BCP tm node* top() const { return ((normal root node &amp;&amp; use normal root node) | | (normal root node &amp;&amp; pq.size() == 1)) ? normal root node : pq[<xref id="XR700" ref-type="bibr" rid="R1">1</xref>]; } The last modification correctly initializes the new variables in the constructor. BCP node queue(BCP tm prob&amp; p): p(q), pq(), normal root node(0), use normal root node( false ) { pq.push back(NULL); }</p>
         <p>A.2.2 include/BCP tm node.cpp We also have to change the implementations of the insert and and pop methods of the BCP node queue class. Since we have to access user data objects, we have to include the framework’s user data header. By using a compile-time flag for applications that use the local branching framework, the COIN source remains usable for other applications.  The pop() method removes a node from the head of the priority queue. When the queue is has only one element left, the normal root node is re-inserted to the list. If the normal root node has been used (by setting use normal root node to true), it is deleted when pop() is called. In the insert() method, we have to detect normal root nodes and store them in the extra variable instead of the normal candidate list. By using the COIN LB flag again, the LB user data cast does not conflict with other COIN/BCP applications. A.2.3 TM/BCP tm functions.cpp Another small modification is necessary in the static helper function BCP tm start one node() . When the normal root node should be returned, it is returned without further checking (e.g. if it should be pruned). This way the tree manager can recognize when the normal root node is pruned without further modifications (in this case, the node would be pruned by a LP process). Also, when a node was pruned because of the global upper bound, it is added to the pruned nodes list that is described in the next section. We start by adding a new public member to the BCP tm prob class. It is used to store nodes that have been pruned. Since even pruned nodes are never deleted from memory, the tree manager can access this list without further restrictions. The tree manager can also empty the list when the nodes have been processed.</p>
         <p>#ifdef COIN LB #include ”LB user data.hpp” #endif</p>
         <p>void BCP node queue::pop() { if (use normal root node &amp;&amp; normal root node) { normal root node = 0; return ;</p>
         <p>} if (normal root node &amp;&amp; pq.size() &lt; = 2) { // reinsert normal root node when the last element is popped insert(normal root node, false ); normal root node = 0; use normal root node = false ; } (. . .)</p>
         <p>void BCP node queue::insert(BCP tm node* node, bool replace normal root node) { #ifdef COIN LB if (node − &gt; user data() &amp;&amp; replace normal root node) { const LB user data* ud = dynamic cast &lt; const LB user data* &gt; (node − &gt; user data()); if (ud &amp;&amp; LB user data::UD NormalRoot == ud − &gt; type) { normal root node = node; return ; } } #endif (. . .)</p>
         <p>(. . .) p.ub() * (1 − p.param(BCP tm par::TerminationGap Relative))) process this = false; if ( p . candidates . use normal root node ) { process this = true ; p . candidates . use normal root node = 0 ; } if (process this) break; if (desc − &gt; indexed pricing.get status() == BCP PriceNothing | | p.current phase colgen == BCP DoNotGenerateColumns Fathom) { next node − &gt; status = BCP PrunedNode OverUB; p . pruned nodes . push back ( next node );</p>
         <p>(. . .) A.3 Counting Pruned Nodes</p>
         <p>/** Pruned nodes are stored in this list - may be cleared when no longer needed */ BCP vec &lt; BCP tm node* &gt; pruned nodes; There are two more places where nodes may be pruned inside the tree manager.</p>
         <p>A.3.1 TM/BCP tm msg node rec.cpp Among other things, the method BCP tm unpack branching info() prunes child nodes generated from a branching object when necessary. We add those nodes to our pruned nodes() list.  A.3.2 TM/BCP tm msgproc.cpp The tree manager also receives pruned nodes from LP processes. These nodes are also added to pruned nodes. With these modifications, the tree manager is able to track the number of active nodes for all local trees. It uses the local tree identification number stored in the user data of the pruned nodes to update the node numbers of the corresponding local tree. For aborting local trees, we store a set of local tree identification numbers in the candidate list. In the tree manager method responsible for finding a new subproblem for a LP process, we simply discard nodes that are in this set of terminated trees.</p>
         <p>(. . .) case BCP FathomChild: child − &gt; status = BCP PrunedNode Discarded; p . pruned nodes . push back ( child ); break; (. . .)</p>
         <p>(. . .) case BCP Msg NodeDescription Discarded: case BCP Msg NodeDescription OverUB Pruned: case BCP Msg NodeDescription Infeas Pruned: node = BCP tm unpack node no branching info(*this, msg buf); pruned nodes . push back ( node ); (. . .)</p>
         <p>A.4 Aborting Local Trees</p>
         <p>A.4.1 include/BCP tm node.hpp We have to include two additional headers, again wrapped in a precompiler conditional. #ifdef COIN LB #include ”localtreeid.hpp” #include &lt; set &gt; #endif Then we add a new public member to BCP node queue: #ifdef COIN LB /** LocalTreeId values of trees to be terminated (= to be pruned by BCP node queue::pop and BCP node queue::top) */ std::set &lt; LocalTreeId &gt; terminate ids; #endif A.4.2 TM/BCP tm functions In BCP tm start one , we modify the head of the main loop, the updates marked with bold face. (. . .) while (true) { if (p.candidates.empty()) return BCP NodeStart NoNode; next node = p.candidates.top(); p.candidates.pop(); desc = next node − &gt; desc; bool process this = true; #ifdef COIN LB const LB user data * lb ud = dynamic cast &lt; const LB user data * &gt; ( next node − &gt; user data ()); if ( lb ud &amp;&amp; p . candidates . terminate ids . end () != p . candidates . terminate ids . find ( lb ud − &gt; id )) process this = false ; else #endif if (! p.has ub()) // if no UB yet or lb is lower than UB then go ahead break; (. . .)</p>
         <p>Appendix B</p>
      </sec>
      <sec>
         <title>Test Scripts</title>
         <p>The results of chapter 9 were retrieved using test scripts written in Bash and Python code. The printstats.py script expects a file containing the output of a set of test runs, usually covering more than one instance and testing several configurations. The results are grouped by filename and configuration, and miscellaneous statistical data can be extracted. For example, tables containing the final objective values or the online performance rating. Additionally, plots of the final objective value can be created. The gnuplot program is used to generate these cuts which can be viewed on screen or written to a postscript file. Since the test logs are usually rather large and take some seconds for processing, a simple interactive command line interface was implemented to shorten user response times.  To simplify testing different configurations on many different files, a short Bash script is available. The configurations to be tested are entered as an array, which is then applied to every file supplied. Since the instances of the OR Library [<xref id="XR730" ref-type="bibr" rid="R2">2</xref>] contain 30 test instances per file, the instance numbers to be tested can be specified in an array. The script has to be executed from the main knapsack application directory and stores the results in the file specified by outfile . The instance numbers are stored in instances (in this case { 0 . . . 29 } ), the configurations are stored in testcases . The test files are supplied on the command line, possibly using wildcards. The printstats.py script parses the log file given on the command line and offers a simple line-based interactive interface to query the results. The most important commands are:</p>
         <p>B.1 Generating Log Files</p>
         <p># !/bin/bash outfile=testall.log rm $outfile instances=”‘seq 0 29‘” testcases=(”LB K 0” ”LB K 10 LB MaxNodes 5000” ”LB K 20”) for file in $* do echo − e ”Processing” $file ”. . . \ n” for inst in $instances do for opts in ”$ { testcases[@] } ” do date &gt;&gt; $outfile echo − e ”Processing” $file ”, instance” $inst ”, params =” $opts ”. . . \ n” &gt;&gt; $outfile nice Linux − O/bcps $opts $ { file } :$ { inst } &gt;&gt; $outfile echo &gt;&gt; $outfile done done done</p>
         <p>B.2 Analyzing Log Files</p>
         <p>• help returns a list of all commands. • help [command] returns a short description and possible parameters of the given command. • table [configuration]* prints a table containing all tested instances as rows and the given configurations (or all, if none are supplied) as columns. The index numbers of the configurations correspond with the log file and are also displayed below the table. • columns [parameter] sets the displayed values. Possible parameters are:</p>
      </sec>
      <sec>
         <title>– –</title>
      </sec>
      <sec>
         <title>– – – –</title>
         <p>finalobjective : the final objective value. finalobjective delta : the final objective value, and the number of processed nodes relative to the best configuration in a row (when two configurations found the same result.) onlineperformance : the online performance rating. localtrees : the number of (created) local trees. localtime : time spent in local branching relative to the total computation time. finalbinary : a binary comparison function for the final objective value, useful for executing Wilcoxon rank sum tests. • showfilename [true/false] enables or disables the file name column in the table view. • plot [filename] [configurations]* executes gnuplot to plot the final objective values of the given configurations (or all if none are given). • outputformat [screen/postscript] sets the output format of the plots generated by the plot commands. screen uses gnuplot to display the diagram on the screen, postscript writes the output to a postscript file. • outputdir [directory] sets the directory where the postscript files are stored (default: current working directory.)</p>
      </sec>
      <sec>
         <title>Bibliography</title>
      </sec>
   </body>
   <back>
      <ref-list>
         <ref id="R1">
            <mixed-citation>[1] E. Balas, S. Ceria, G. Cornuéjols, and N. Natraj. Gomory cuts revisited. Operations Research Letters , 19:1–9, 1996.</mixed-citation>
         </ref>
         <ref id="R2">
            <mixed-citation>[2] J. E. Beasley. Operation research library. <ext-link ext-link-type="uri"
                         href="http://www.brunel.ac.uk/depts/ma/research/jeb/info.html">http://www.brunel.ac.uk/depts/ma/research/jeb/info.html</ext-link> .</mixed-citation>
         </ref>
         <ref id="R3">
            <mixed-citation>[3] D. Bertsimas and R. Demir. An approximate dynamic programming approach to multidimensional knapsack problems. Management Science , 48(4):550–565, 2002.</mixed-citation>
         </ref>
         <ref id="R4">
            <mixed-citation>[4] A. Caprara, H. Kellerer, U. Pferschy, and D. Pisinger. Approximation algorithms for knapsack problems with cardinality constraints. European Journal of Operational Research , 123:333–345, 2000.</mixed-citation>
         </ref>
         <ref id="R5">
            <mixed-citation>[5] S. Ceria, G. Cornuejols, and M. Dawande. Combining and strengthening gomory cuts. In E. Balas and J. Clausen, editors, Integer Programming and Combinatorial Optimization: Proc. of the 4th International IPCO Conference , pages 438–451. Springer, Berlin, Heidelberg, 1995.</mixed-citation>
         </ref>
         <ref id="R6">
            <mixed-citation>[6] P. C. Chu and J. E. Beasley. A genetic algorithm for the multidimensional knapsack problem. Journal of Heuristics , 4(1):63–86, 1998.</mixed-citation>
         </ref>
         <ref id="R7">
            <mixed-citation>[7] V. Chvátal. Edmonds polytopes and a hierarchy of combinatorial problems. Discrete Mathematics , (4):305–337, 1973.</mixed-citation>
         </ref>
         <ref id="R8">
            <mixed-citation>[8] E. Danna, E. Rothberg, and C. L. Pape. Exploring relaxation induced neighborhoods to improve mip solutions. Mathematical Programming , 2004.</mixed-citation>
         </ref>
         <ref id="R9">
            <mixed-citation>[9] L. Davis. A genetic algorithm tutorial. In Handbook of Genetic Algorithms , pages 1–101, New York, 1991.</mixed-citation>
         </ref>
         <ref id="R10">
            <mixed-citation>[10] F. Eisenbrand. On the chvátal rank of polytopes in the 0/1 cube. Discrete Applied Mathematics , 98:21–27, 1999.</mixed-citation>
         </ref>
         <ref id="R11">
            <mixed-citation>[11] F. Eisenbrand. Gomory-Chvátal cutting planes and the elementary closure of polyhedra . PhD thesis, 2000.</mixed-citation>
         </ref>
         <ref id="R12">
            <mixed-citation>[12] M. Esö, L. Ladányi, T. K. Ralphs, and L. Trotter. Fully parallel generic branch-and- cut. Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientific Computing , 1997.</mixed-citation>
         </ref>
         <ref id="R13">
            <mixed-citation>[13] M. Fischetti and A. Lodi. Local branching. Mathematical Programming , 98:23–47, 2002.</mixed-citation>
         </ref>
         <ref id="R14">
            <mixed-citation>[14] H. C. for Enterprise Science. Benchmarks for the multiple knapsack problem. <ext-link ext-link-type="uri" href="http://hces.bus.olemiss.edu/tools.html">http://hces.bus.olemiss.edu/tools.html</ext-link> .</mixed-citation>
         </ref>
         <ref id="R15">
            <mixed-citation>[15] R. E. Gomory. Outline of an algorithm for integer solutions to linear programs. Bulleting of the American Mathematical Society , (64):275–278, 1958.</mixed-citation>
         </ref>
         <ref id="R16">
            <mixed-citation>[16] R. E. Gomory. An algorithm for integer solutions to linear programs. Recent Advances in Mathematical Programming , pages 269–302, 1963.</mixed-citation>
         </ref>
         <ref id="R17">
            <mixed-citation>[17] J. Gottlieb. Permutation-based evolutionary algorithms for multidimensional knapsack problems. Proceedings of 2000 ACM Symposium on Applied Computing , 2000.</mixed-citation>
         </ref>
         <ref id="R18">
            <mixed-citation>[18] R. Hinterding. Mapping, order-independent genes and the knapsack problem. Proceedings of the 1st IEEE International Conference on Evolutionary Computation , pages 13– 17, 1994.</mixed-citation>
         </ref>
         <ref id="R19">
            <mixed-citation>[19] O. H. Ibarra and C. E. Kim. Fast approximation algorithms for the knapsack and sum of subset problems. J. ACM , 22(4):463–468, 1975.</mixed-citation>
         </ref>
         <ref id="R20">
            <mixed-citation>[20] N. Karmarkar. A new polynomial-time algorithm for linear programming. Combinator- ica , 4:373–395, 1984.</mixed-citation>
         </ref>
         <ref id="R21">
            <mixed-citation>[21] H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems . Springer, 2004.</mixed-citation>
         </ref>
         <ref id="R22">
            <mixed-citation>[22] L. G. Khachian. A polynomial algorithm for linear programming. Doklady Akad. Nauk USSR , 224:1093–1096, 1979.</mixed-citation>
         </ref>
         <ref id="R23">
            <mixed-citation>[23] P. Kolesar. A branch and bound algorithm for the knapsack problem. Management Science , 13:723–735, 1967.</mixed-citation>
         </ref>
         <ref id="R24">
            <mixed-citation>[24] B. Korte and R. Schrader. On the existence of fast approximation schemes. In O. L. Mangasarian, R. R. Meyer, and S. Robinson, editors, Nonlinear Programming 4 , pages 415–437. Academic Press, 1981.</mixed-citation>
         </ref>
         <ref id="R25">
            <mixed-citation>[25] A. H. Land and A. G. Doig. An automatic method for solving discrete programming problems. Econometrica , 28:497–520, 1960.</mixed-citation>
         </ref>
         <ref id="R26">
            <mixed-citation>[26] E. K. Lee and J. E. Mitchell. Branch-and-bound methods for integer programming. In Encyclopedia of Optimization , volume 2, pages 509–519. Kluwer Academic Publishers, 2001.</mixed-citation>
         </ref>
         <ref id="R27">
            <mixed-citation>[27] J. S. Lee and M. Guignard. An approximate algorithm for multidimensional zero-one knapsack problems. Management Science , 34(3):402–410, 1988.</mixed-citation>
         </ref>
         <ref id="R28">
            <mixed-citation>[28] R. Lougee-Heimer. The common optimization interface for operations research. IBM Journal of Research and Development , 47:57–66, 2003.</mixed-citation>
         </ref>
         <ref id="R29">
            <mixed-citation>[29] J. E. Mitchell. Branch-and-cut algorithms for integer programming. In Encyclopedia of Optimization , volume 2, pages 519–525. Kluwer Academic Publishers, 2001.</mixed-citation>
         </ref>
         <ref id="R30">
            <mixed-citation>[30] J. E. Mitchell. Cutting plane algorithms for integer programming. In Encyclopedia of Optimization , volume 2, pages 525–533. Kluwer Academic Publishers, 2001.</mixed-citation>
         </ref>
         <ref id="R31">
            <mixed-citation>[31] M. Padberg and G. Rinaldi. A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. SIAM Rev. , 33(1):60–100, 1991.</mixed-citation>
         </ref>
         <ref id="R32">
            <mixed-citation>[32] D. Pisinger. Algorithms for Knapsack Problems . PhD thesis, University of Copenhagen, Dept. of Computer Science, Feb. 1995.</mixed-citation>
         </ref>
         <ref id="R33">
            <mixed-citation>[33] G. Raidl. An improved genetic algorithm for the multiconstrainted 0-1 knapsack problem. Proceedings of the 5th IEEE International Conference on Evolutionary Computation , pages 207–211, 1998.</mixed-citation>
         </ref>
         <ref id="R34">
            <mixed-citation>[34] G. R. Raidl. Weight-codings in a genetic algorithm for the multiconstraint knapsack problem. Proceedings of the 1999 IEEE Congress on Evolutionary Computation , pages 596–603, 1999.</mixed-citation>
         </ref>
         <ref id="R35">
            <mixed-citation>[35] G. R. Raidl and J. Gottlieb. Empirical analysis of locality, heritability and heuristic bias in evolutionary algorithms: A case study for the multidimensional knapsack problem. Accepted for publication in the Evolutionary Computation Journal , 2004.</mixed-citation>
         </ref>
         <ref id="R36">
            <mixed-citation>[36] T. K. Ralphs and L. Ladányi. COIN/BCP User’s Manual . <ext-link ext-link-type="uri" href="http://www.coin-or.org/Presentations/bcp-man.pdf">http://www.coin-or.org/Presentations/bcp-man.pdf</ext-link> , 2001.</mixed-citation>
         </ref>
         <ref id="R37">
            <mixed-citation>[37] G. D. Scudder and G. Fox. A heuristic with tie breaking for certain 0-1 integer programming models. Naval Research Logistics Quarterly , 32:613–623, 1985.</mixed-citation>
         </ref>
         <ref id="R38">
            <mixed-citation>[38] S. Senju and Y. Toyoda. An approach to linear programming with 0-1 variables. Management Science , 11:B196–B207, 1967.</mixed-citation>
         </ref>
         <ref id="R39">
            <mixed-citation>[39] J. Thiel and S. Voss. Some experiences on solving multiconstraint zero-one knapsack problems with genetic algorithms. INFOR 32 , pages 226–242, 1994.</mixed-citation>
         </ref>
         <ref id="R40">
            <mixed-citation>[40] Y. Toyoda. A simplified algorithm for obtaining approximate solution to zero-one programming problems. Management Science , 21:1417–1427, 1975.</mixed-citation>
         </ref>
         <ref id="R41">
            <mixed-citation>[41] M. Vasquez and J.-K. Hao. A hybrid approach for the 0-1 multidimensional knapsack problem. Proceedings of IJCAI 01 , 2001.</mixed-citation>
         </ref>
         <ref id="R42">
            <mixed-citation>[42] L. A. Wolsey. Integer Programming . John Wiley and Sons, 1998.</mixed-citation>
         </ref>
      </ref-list>
   </back>
</article>