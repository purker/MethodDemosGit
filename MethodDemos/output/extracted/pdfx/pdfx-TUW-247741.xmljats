<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  SYSTEM "http://dtd.nlm.nih.gov/archiving/3.0/archivearticle3.dtd">
<article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xlink="http://www.w3.org/1999/xlink">
   <front>
      <journal-meta>
         <journal-id/>
         <journal-title-group>
            <journal-title/>
         </journal-title-group>
         <issn/>
         <publisher>
            <publisher-name/>
         </publisher>
      </journal-meta>
      <article-meta>
         <title-group>
            <article-title>A FORMAL METHOD FOR SELECTING EVALUATION METRICS FOR IMAGE SEGMENTATION.</article-title>
         </title-group>
         <supplement>
            <p>Abdel Aziz Taha, Allan Hanbury Vienna University of Technology</p>
         </supplement>
         <abstract>
            <sec>
               <p>Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmentations with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the segmentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert. Index Terms — image segmentation; evaluation metrics; selection</p>
            </sec>
         </abstract>
      </article-meta>
   </front>
   <body>
      <sec>
         <title>1 Introduction</title>
      </sec>
      <sec>
         <title>2 Related Work</title>
         <p>Jin et al. [ <xref id="XR32" ref-type="bibr" rid="R12">12</xref>] established a formal method for comparing two different measures and introduced two criteria for formal comparison of the goodness of evaluation metrics, namely the degree of consistency (DoC) and degree of discriminancy (DoD). Applying these criteria, they showed theoretically and empirically that AUC is a better measure than accuracy in evaluating the performance of classifiers. [<xref id="XR33" ref-type="bibr" rid="R13">13</xref>] [<xref id="XR34" ref-type="bibr" rid="R14">14</xref>] applied formal constraints based on axiometry to compare and judge evaluation metrics depending on the grade of satisfaction of these constraints. Busin et al. [<xref id="XR35" ref-type="bibr" rid="R15">15</xref>] used axiometrics to define a formal and general notation that fits any effectiveness metric. Based on this notation, they proposed several axioms that should be satisfied by an effectiveness metric. They used these axioms as criteria to evaluate metrics. All these papers deal with the problem only from a theoretical axiometrical point of view without taking into account the classification goal and the nature and properties of data being classified. Sakai [<xref id="XR36" ref-type="bibr" rid="R11">11</xref>] proposed a method for evaluating evaluation metrics by measuring their sensitivity using Boot- strap Hypothesis Tests, and used this method in com- paring seven evaluation metrics. They negate the belief that commonly used evaluation measures are equally reliable. Fatourechi et al. [<xref id="XR38" ref-type="bibr" rid="R2">2</xref>] proposed a framework based on Desired Region of Operation (DROP) for selecting the best evaluation metric for evaluating imbalanced classifications. Sakai [<xref id="XR39" ref-type="bibr" rid="R16">16</xref>] provided comparisons between metrics depending on the sensitivity and sta- bility using the Voorhees/Buckley swap method [<xref id="XR40" ref-type="bibr" rid="R17">17</xref>]. All these papers lack generality because they are methods designed either for specific metrics or for specific metric properties.</p>
      </sec>
      <sec>
         <title>3 Proposed Methods</title>
         <p>We propose a method for choosing the most suitable metric for evaluating image segmentation. In Sections 3.1 to 3.3, the method is described and discussed formally. Then this formal description is explained in a step by step demonstration with a real example in Section 4, which also provides an experimental evaluation of the method. Given a set of effectiveness metrics M and a set of segmentations C , each of the segmentations is evaluated against its ground truth segmentation using all metrics to obtain a ranking per metric. Now, choosing the most suitable metric goes in two main steps: (i) Constructing different partitions on the segmentation set C and ranking the subsets of each partition according to their average quality regarding each metric. (ii) Inferring the metric bias from the rank correlations across all partitions and all metrics. In the following, each of the steps is described in more details.</p>
         <sec>
            <title>3.1 Constructing partitions and rank structure:</title>
            <p>1. For each metric m ∈ M , evaluate each of the segmentations x ∈ C against its ground truth segmentation to get the score matrix s where s ( x , m ) is the score of segmentation x measured by metric m . 2. For each metric m ∈ M , assign each segmentation x ∈ C a rank depending on its score to get the rank matrix r where r ( x , m ) is the rank of segmentation x measured by metric m . 3. Define a set F of t segmentation properties. These can be any features thought to impact metrics e.g. class imbalance, number of segments, segment size, noise, deviation, shape signatures, sphereness, boundary smoothness, resolution, moments, etc. Furthermore, features can also be score-dependent e.g. precision and recall for utilizing trade-off i.e. for evaluation that tends to reward precision on cost of recall and vice versa. If no features are known to impact metrics, simply use all available features.  4. Construct t different partitions on the segmentation set C , each partition according to one feature from F , i.e. according to the grade of occurrence of the feature in the segmentations. One gets the set of partitions P = { P 1 , .., P t } . Each partition should have the same number of subsets s . The function P ij ( x ) assigns the segmentation x to the subset j according to partition i .</p>
            <p>5. Construct t random partitions P ˇ = { P ˇ 1 , .., P ˇ t } by randomly assigning segmentations to s equal subsets in each partition. The function P ˇ ij ( x ) that assigns a segmentation x to the subset j of the random partition i is defined by  P ˇ ij ( x ) = 1 0 otherwise x ∈ subset j of random partition i (2) 6. For each metric m ∈ M , for each partition i ∈ P , rank the subsets j according to the average of the individual ranks in each subset using the rank function</p>
            <p>where x ∈ C are the individual segmentations and n ij is the number of segmentations in the subset j of Partition i . Now, use the rank averages from Equation 3 to compute the rank structure R = R ( i , j , m ) that gives the rank of subset j of partition i measured by metric m according to descending rank average. Analogously, R ˇ = R ˇ ( i , j , m ) gives the rank of subset j of the random partition i measured by metric m . Note that ranking the subsets using the averages of the individual ranks in each subset is a ranking method inspired by the Mann-Whitney-Wilcoxon (MWW) test [<xref id="XR55" ref-type="bibr" rid="R18">18</xref>]. This is because straightforwardly computing the ranks from score averages is sensitive to outliers and may produce unreasonable rankings if the scores are not normally distributed [<xref id="XR56" ref-type="bibr" rid="R19">19</xref>].</p>
            <p>3.2 Inferring metric bias: Now, the rank structures R ˇ (rankings of the random partitions) and R (rankings of the non-random partitions) provide a statistical ba- sis to infer metric bias by analyzing how rankings of the different metrics and different partitions are corre- lated. The analysis is primarily based on comparing two correlations: the average of the rank correlations given the random partitions R ˇ (we will call this correlation the base correlation K ˇ ) and the rank correlation  given a particular partition R (we will call this correlation the biased correlation K ). They are given by</p>
            <p>where m t is the metric being evaluated, p is a given partition, and r ( x 1, x 2 ) is the Pearson’s correlation co- efficient between the rankings x 1 and x 2 (the point de- notes all possible values, e.g. R ( p , ., m ) means all possible subset ranks in partition p measured by metric m ) Now, we define the overall bias of metric m t to be the average of the absolute correlation change B ( m t ) which is given by</p>
            <p>Finally, the metrics in M are ranked according to their overall bias, where the metric with the lowest bias is the most suitable.  3.3 Discussion: To understand the key idea, let’s think about the following two cases: Case 1, partitioning the segmentations randomly. Case 2, partitioning the segmentations according to a particular property (e.g. class imbalance in the underlying segmentations). Given a particular metric m , the base correlation K ˇ ( m ) given by Equation 4 (related to Case 1) depends on the nature of the metric and is not affected by the properties of the segmentations, since the partition is random. Now, if this correlation changes when we consider Case 2 (i.e. biased correlation K given by Equation 5), then the change is caused by the impact of the property used for partitioning (in this case class imbalance) on the metric and therefore it characterizes the bias of the metric towards/against this property. If many partitions (many properties) are used, then the sum of the correlation differences is a measure of the overall bias for the given metric.</p>
         </sec>
      </sec>
      <sec>
         <title>4 Experiments</title>
         <p>In this section, the proposed method is demonstrated and tested with a real example, namely a set of 229 automatic brain tumor segmentations (MRI 3D volumes) from the BRATS2012 challenge 1 . The segmentations correspond to 47 medical cases and were produced by 1 MICCAI 2012 Challenge on Multimodal Brain Tumor Segmenta- tion, www2.imm.dtu.dk/projects/BRATS2012  five different algorithms participating in BRATS challenge. To build the rank structure (described in Section 3.1), all segmentations were evaluated against their ground truth segmentations using 18 metrics (listed in <xref id="XR77" ref-type="table" rid="T1">Table 1</xref>) to get the score matrix s (Step 1). Then global ranks were calculated from scores to get a ranking per metric r (Step 2). A set of 7 properties, namely segment size, noise, class imbalance, connected compo- nent count, point variance, sphereness, and recall, was defined (Step 3). Now, 7 partitions of the segmentations were constructed each time using one of the defined properties. Each consists of 10 subsets with 229 10 ≈ 22 segmentations (Step 4). A random partition with 10 equal subsets was constructed (Step 5). For each partition, the subsets were ranked using the sum of individual global ranks r to get 18 rankings per partition (126 rankings in total). The random partition was also ranked to get 18 rankings (Step 6). To infer metric bias, Equations 4, 5, and 6 in Section 3.2 were applied to the resulting rank structure. The result of this step is a metric list ranked according to bias. To validate the suitability ranking produced by the proposed approach, a manual ranking done by a ra- diology expert was used: for each medical case, the five corresponding segmentations were ranked by their quality from a medical point of view (we call these the manual rankings). Analogously, for each medical case, 18 rankings of the five segmentations were produced each time using one of metrics (we call these the metric rankings). The average correlation between manual rankings and metric rankings was computed for each metric and finally the metrics were sorted according to this average correlation. The resulting metric ranking (<xref id="XR78" ref-type="table" rid="T1">Table 1</xref> column ’manual’) was used as a ground truth suitability ranking of the metrics to validate the automatic ranking. <xref id="XR79" ref-type="table" rid="T1">Table 1</xref> column ’automatic’ contains for each metric the bias (Equation 6) and the corresponding suitability rank computed according to ascending bias. A moderate to strong correlation between the two rankings can be observed. The six best metrics are the same in both rankings. This correlation shows that metrics with low bias produce rankings that are more corre- lated to manual rankings than others.</p>
      </sec>
      <sec>
         <title>5 Conclusion and future work</title>
         <p>For evaluating segmentations, metrics can be chosen according to their bias (Equation 6) toward/against the properties of the segmentations being evaluated. Test results show that the ranking produced by metrics with low bias generally have higher correlation with manual ranking than rankings produced by other metrics. In future work, the method will be tested with seg-  mentations of other types and validated against rankings from different experts. A further issue to be investigated in future work is the influence of weighting the properties in Equation 6 on the metric suitability ranking, if it is known that particular properties are more/less important for the segmentation goal. For example, the manual ranking used to validate this method is done by a radiologist who may emphasise recall on cost of precision to assure that the tumor is completely removed. In this case weighting the recall and precision properly could improve the result.</p>
         <table-wrap id="Tx83">
            <caption>
               <p>Table 1 . Manual and automatic metric suitability rankings. In column ’manual’, the average correlation between metric rankings and the manual rankings as well as corresponding suitability ranks according to decend- ing correlation. In column ’automatic’, the metric bias calculated automatically by the proposed method as well as the ranks according to ascending bias (detailed data and results available in [<xref id="XR85" ref-type="bibr" rid="R20">20</xref>])</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td/>
                     <td> correl.</td>
                     <td> rank</td>
                     <td> bias</td>
                     <td> rank</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> Cohen’s Kappa</td>
                     <td> 0.818</td>
                     <td> 1</td>
                     <td> 33.5</td>
                     <td> 2</td>
                  </tr>
                  <tr>
                     <td> Adjusted Rand Index</td>
                     <td> 0.818</td>
                     <td> 1</td>
                     <td> 33.1</td>
                     <td> 1</td>
                  </tr>
                  <tr>
                     <td> Interclass Correlation</td>
                     <td> 0.818</td>
                     <td> 1</td>
                     <td> 33.5</td>
                     <td> 2</td>
                  </tr>
                  <tr>
                     <td> Probabilistic distance</td>
                     <td> 0.802</td>
                     <td> 2</td>
                     <td> 34.7</td>
                     <td> 5</td>
                  </tr>
                  <tr>
                     <td> Dice</td>
                     <td> 0.800</td>
                     <td> 3</td>
                     <td> 33.6</td>
                     <td> 3</td>
                  </tr>
                  <tr>
                     <td> Average Distance</td>
                     <td> 0.798</td>
                     <td> 4</td>
                     <td> 33.9</td>
                     <td> 4</td>
                  </tr>
                  <tr>
                     <td> Accuracy</td>
                     <td> 0.791</td>
                     <td> 5</td>
                     <td> 64.0</td>
                     <td> 14</td>
                  </tr>
                  <tr>
                     <td> Rand Index</td>
                     <td> 0.791</td>
                     <td> 5</td>
                     <td> 64.0</td>
                     <td> 14</td>
                  </tr>
                  <tr>
                     <td> Variation of Inform.</td>
                     <td> 0.791</td>
                     <td> 6</td>
                     <td> 62.0</td>
                     <td> 13</td>
                  </tr>
                  <tr>
                     <td> Mutual Information</td>
                     <td> 0.753</td>
                     <td> 7</td>
                     <td> 46.5</td>
                     <td> 12</td>
                  </tr>
                  <tr>
                     <td> Mahalanobis Distance</td>
                     <td> 0.701</td>
                     <td> 8</td>
                     <td> 37.7</td>
                     <td> 7</td>
                  </tr>
                  <tr>
                     <td> Global Consistency Err.</td>
                     <td> 0.670</td>
                     <td> 9</td>
                     <td> 69.8</td>
                     <td> 15</td>
                  </tr>
                  <tr>
                     <td> Hausdorff Distance</td>
                     <td> 0.663</td>
                     <td> 10</td>
                     <td> 35.5</td>
                     <td> 6</td>
                  </tr>
                  <tr>
                     <td> Area u. curve (AUC)</td>
                     <td> 0.647</td>
                     <td> 11</td>
                     <td> 42.0</td>
                     <td> 8</td>
                  </tr>
                  <tr>
                     <td> Sensitivity</td>
                     <td> 0.615</td>
                     <td> 12</td>
                     <td> 44.4</td>
                     <td> 10</td>
                  </tr>
                  <tr>
                     <td> Precision</td>
                     <td> 0.608</td>
                     <td> 13</td>
                     <td> 44.5</td>
                     <td> 11</td>
                  </tr>
                  <tr>
                     <td> Volumetric Similarity</td>
                     <td> 0.590</td>
                     <td> 14</td>
                     <td> 43.6</td>
                     <td> 9</td>
                  </tr>
                  <tr>
                     <td> Specificity</td>
                     <td> 0.398</td>
                     <td> 15</td>
                     <td> 78.6</td>
                     <td> 16</td>
                  </tr>
                  <tr>
                     <td> Correl. btw. manual &amp;</td>
                     <td> automatic automatic</td>
                     <td> ranking</td>
                     <td/>
                     <td> 0.607</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
      </sec>
      <sec>
         <title>6 Acknowledgments</title>
         <p>Thanks to Dr. Bjoern H. Menze, ETH Zurich for pro- viding the MRI brain segmentations from MICCAI 12 BRATS challenge to be used as test data. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement 318068 (VISCERAL).</p>
      </sec>
      <sec>
         <title>7 References</title>
      </sec>
   </body>
   <back>
      <ref-list>
         <ref id="R1">
            <mixed-citation>[1] T.H.J.M. Peeters, P.R. Rodrigues, A. Vilanova, and B.M ter Haar Romeny, “Analysis of dis- tance/similarity measures for diffusion tensor imaging,” in Visualization and Processing of Tensor Fields: Advances and Perspectives . Springer, Berlin, 2008.</mixed-citation>
         </ref>
         <ref id="R2">
            <mixed-citation>[2] Mehrdad Fatourechi, Rabab K. Ward, Steven G. Mason, Jane Huggins, Alois Schloegl, and Gary E. Birch, “Comparison of evaluation metrics in classification applications with imbalanced datasets,” in ICMLA , 2009, pp. 777–782.</mixed-citation>
         </ref>
         <ref id="R3">
            <mixed-citation>[3] Guido Gerig, Matthieu Jomier, and Miranda Chakos, “Valmet: A new validation tool for as- sessing and improving 3D object segmentation,” in Proceedings of the 4th International Conference on Medical Image Computing and Computer-Assisted In- tervention , 2001, pp. 516–523.</mixed-citation>
         </ref>
         <ref id="R4">
            <mixed-citation>[4] Daniel B. Russakoff, Carlo Tomasi, Torsten Rohlf- ing, Calvin R. Maurer, and Jr., “Image similarity using mutual information of regions,” in 8th European Conference on Computer Vision, ECCV , 2004, pp. 596–607.</mixed-citation>
         </ref>
         <ref id="R5">
            <mixed-citation>[5] Nguyen Xuan Vinh, Julien Epps, and James Bai- ley, “Information theoretic measures for cluster- ings comparison: is a correction for chance nec- essary?,” in Proceedings of the 26th Annual International Conference on Machine Learning . 2009, pp. 1073–1080, ACM.</mixed-citation>
         </ref>
         <ref id="R6">
            <mixed-citation>[6] David M. W. Powers, “Evaluation: From precision, recall and F-factor to ROC, informedness, marked- ness correlation,” Journal of Machine Learning Tech- nologies , vol. 2, pp. 37–63, 2011.</mixed-citation>
         </ref>
         <ref id="R7">
            <mixed-citation>[7] Chris Buckley and Ellen M. Voorhees, “Evaluating evaluation measure stability,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval . 2000, pp. 33–40, ACM.</mixed-citation>
         </ref>
         <ref id="R8">
            <mixed-citation>[8] Halim Benhabiles, Guillaume Lavoue, Jean Phillipe Vandeborre, and Mohamed Daoudi, “A subjective experiment for 3d-mesh segmentation evaluation,” in IEEE International Workshop on Multimedia Signal Processing (MMSP) , 2010.</mixed-citation>
         </ref>
         <ref id="R9">
            <mixed-citation>[9] Filip Radlinski and Nick Craswell, “Comparing the sensitivity of information retrieval metrics,” in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval , 2010.</mixed-citation>
         </ref>
         <ref id="R10">
            <mixed-citation>[10] R. Blanco and H. Zaragoza, “Beware of relatively large but meaningless improvements,” Tech. Rep., Yahoo! Research 2011-001, 2011.</mixed-citation>
         </ref>
         <ref id="R11">
            <mixed-citation>[11] Tetsuya Sakai, “Evaluating evaluation metrics based on the bootstrap,” in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval . 2006, pp. 525–532, ACM.</mixed-citation>
         </ref>
         <ref id="R12">
            <mixed-citation>[12] Jin Huang and Charles X. Ling, “Using AUC and accuracy in evaluating learning algorithms,” IEEE Transactions on Knowledge and Data Engineering , vol. 17, pp. 299–310, 2005.</mixed-citation>
         </ref>
         <ref id="R13">
            <mixed-citation>[13] Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa Verdejo, “A comparison of extrinsic clus- tering evaluation metrics based on formal con- straints,” Inf. Retr , vol. 12, no. 4, pp. 461–486, Au- gust 2009.</mixed-citation>
         </ref>
         <ref id="R14">
            <mixed-citation>[14] Nguyen Xuan Vinh, Julien Epps, and James Bai- ley, “Information theoretic measures for cluster- ings comparison: Variants, properties, normaliza- tion and correction for chance,” J. Mach. Learn. Res. , vol. 9999, pp. 2837–2854, December 2010.</mixed-citation>
         </ref>
         <ref id="R15">
            <mixed-citation>[15] Luca Busin and Stefano Mizzaro, “Axiometrics: An axiomatic approach to information retrieval effectiveness metrics,” in Proceedings of the 2013 Conference on the Theory of Information Retrieval , New York, NY, USA, 2013, pp. 8:22–8:29.</mixed-citation>
         </ref>
         <ref id="R16">
            <mixed-citation>[16] Tetsuya Sakai, “On the reliability of information retrieval metrics based on graded relevance,” Information Processing Management , 2007.</mixed-citation>
         </ref>
         <ref id="R17">
            <mixed-citation>[17] Ellen M. Voorhees and Chris Buckley, “The effect of topic set size on retrieval experiment error,” in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval , 2002, pp. 316–323.</mixed-citation>
         </ref>
         <ref id="R18">
            <mixed-citation>[18] H. B. Mann and D. R. Whitney, “On a test of whether one of two random variables is stochas- tically larger than the other,” The Annals of Mathe- matical Statistics , vol. 18, no. 1, pp. 50–60, 1947.</mixed-citation>
         </ref>
         <ref id="R19">
            <mixed-citation>[19] Janez Demsar, “Statistical comparisons of classifiers over multiple data sets,” J. Mach. Learn. Res. , vol. 17, pp. 30, 2006.</mixed-citation>
         </ref>
         <ref id="R20">
            <mixed-citation>[20] Abdel Aziz Taha, Allan Hanbury, and Oscar Jimenez, “Test data and results of the automatic metric selection method,” Tech. Rep., Vienna University of Technology, <ext-link ext-link-type="uri" href="http://publik.tuwien.ac.at/files/">http://publik.tuwien.ac.at/files/</ext-link> PubDat 229008.pdf, 2014.</mixed-citation>
         </ref>
      </ref-list>
   </back>
</article>