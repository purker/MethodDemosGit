<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  SYSTEM "http://dtd.nlm.nih.gov/archiving/3.0/archivearticle3.dtd">
<article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xlink="http://www.w3.org/1999/xlink">
   <front>
      <journal-meta>
         <journal-id/>
         <journal-title-group>
            <journal-title/>
         </journal-title-group>
         <issn/>
         <publisher>
            <publisher-name/>
         </publisher>
      </journal-meta>
      <article-meta>
         <title-group>
            <article-title>Eine generische Bibliothek f ür Metaheuristiken und ihre Anwendung auf das Quadratic Assignment Problem</article-title>
         </title-group>
         <supplement>
            <p>DIPLOMARBEIT</p>
            <p>ausgeführt am Institut f ür Computergraphik und Algorithmen 186 der Technischen Universit ät Wien unter Anleitung von a.o. Univ.-Prof. Dipl.-Ing. Dr.techn. G ünther Raidl durch Daniel Wagner Schauleithenstraße 9 3363 Ulmerfeld-Hausmening Datum Unterschrift</p>
            <p>Abstract</p>
            <p>In this master thesis a generic libray of efficient metaheuristics for combinatorial optimization is presented. In the version at hand classes that feature local search, simulated annealing, tabu search, guided local search and greedy randomized adaptive search procedure were implemeted. Most notably a generic implementation features the advantage that the problem dependent classes and methods only need to be realized once without targeting a specific algorithm because these parts of the sourcecode are shared among all present algorithms contained in EAlib. This main advantage is then exemplarily demonstrated with the quadratic assignment problem. The sourcecode of the QAP example can also be used as an commented reference for future problems. Concluding the experimental results of the individual metaheuristics reached with the presented implementation are presented.</p>
         </supplement>
      </article-meta>
   </front>
   <body>
      <sec>
         <title>Kurzfassung</title>
         <p>In dieser Diplomarbeit wird eine generische Bibliothek von effizienten Metaheuristiken für kombinatorische Optimierungsprobleme vorgestellt. In der vorliegenden Version enthält sind lokale Suche, Simulated Annealing, Tabusearch, Guided Local Search und Greedy Randomized Adaptive Search Procedure implementiert worden. Eine generische Implementierung bietet vorallem den Vorteil das bei einem neuen zu lösendem Problem nur einige bestimmte problemabhängige Klassen und Methoden realisiert werden müssen ohne sich schon im Vorhinein einen speziellen Algorith- mus festzulegen, da diese Klassen und Methoden von allen in der EAlib vorhanden Metaheuristiken verwendet werden. Die Vorteile dieser Bibliothek werden anschließend anhand des Quadratic Assignment Problems ausführlich dargestellt. Dieses Beispiel dient zusätzlich auch noch als kommentierte Referenz für zukünftige Problemimplentierungen. Abschließend werden die Resulate der Experimente mit den verschiedenen Metaheuristiken präsentiert.</p>
      </sec>
      <sec>
         <title>Danksagung</title>
         <p>An dieser stelle möchte ich mich bei allen Menschen bedanken die zum Gelingen dieser Diplomarbeit beigetragen haben. Dieser Dank gilt meinem Betreuer Prof. Raidl, der mich mit großer Geduld am Weg zum Abschluß begleitet hat und mit mir in den vielen Treffen oft nützliche Ideen entwickelt hat. Meinen Eltern und meinem Bruder Ronald danke ich für ein sorgloses Studium und die moralische Unterstützung wenn die Motivation einmal nicht so groß war. Bei meinen Studienkollegen, besonders bei Harry und Zamb, bedanke ich mich für die Freundschaft, den Spaß und die gegenseitige Unterstützung. Last but not least möchte ich mich auch bei meinen Mitbewohnern Sic0 und Leo bedanken, die mir während meiner Arbeit die nötige Ruhe zukommen ließen, aber natürlich auch ab und zu für willkommene Ablenkung gesorgt haben. Natascha danke ich für die schöne gemeinsame Zeit.</p>
      </sec>
      <sec>
         <title>Table of Contents</title>
         <p>1 Introduction 5 1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Combinatorial Optimization and Metaheuristics . . . . . . . . . . . . 5 1.3 Guide to the thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Quadratic Assignment Problem 7 2.1 Problem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2.1 Permutation Formulation . . . . . . . . . . . . . . . . . . . . . 9 2.2.2 Integer Linear Programming . . . . . . . . . . . . . . . . . . . 9 2.2.3 Trace Formulation . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.4 Solution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.4.1 Exact Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.4.2 Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.4.3 Metaheuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.4.4 Research Trends . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.5 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.5.1 Steinberg Wiring Problem . . . . . . . . . . . . . . . . . . . . 14 2.5.2 Antenna Assembly Sequence Problem . . . . . . . . . . . . . . 16 3 Metaheuristics 18 3.1 Basic Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.3 Tabu Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.4 Guided Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.5 Greedy Randomized Adaptive Search Procedure . . . . . . . . . . . . 27 4 Requirements 31 4.1 Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.2 Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.3 Usability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5 Implementation 35 5.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.2 Class reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2.1 Class chromosome . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2.2 Class ea advbase . . . . . . . . . . . . . . . . . . . . . . . . . 38</p>
         <p>5.2.3 Class lsbase . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.2.4 Class localSearch . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.2.5 Class simulatedAnnealing . . . . . . . . . . . . . . . . . . . . 39 5.2.6 Class tabuSearch . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.2.7 Class guidedLS . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.2.8 Class GRASP . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.2.9 Class feature . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.2.10 Class tabuAttribute . . . . . . . . . . . . . . . . . . . . . . . . 42 5.2.11 Class tabulist . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.2.12 Class move and childs . . . . . . . . . . . . . . . . . . . . . . 43 5.2.13 Class qapChrom . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.2.14 Class qapInstance . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.2.15 Class qapFeature . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.2.16 Class qapTabuAttribute . . . . . . . . . . . . . . . . . . . . . 45 5.2.17 Parameter handling . . . . . . . . . . . . . . . . . . . . . . . . 45 5.3 Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.3.1 Interface aObjProvider . . . . . . . . . . . . . . . . . . . . . . 47 5.3.2 Interface tabulistProvider . . . . . . . . . . . . . . . . . . . . 47 5.3.3 Interface featureProvider . . . . . . . . . . . . . . . . . . . . . 48 5.3.4 Interface gcProvider . . . . . . . . . . . . . . . . . . . . . . . 48 5.3.5 Interface tabuProvider . . . . . . . . . . . . . . . . . . . . . . 48 5.3.6 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6 Experimental Results 52 6.1 Test Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.2 Test Setup and Procedure . . . . . . . . . . . . . . . . . . . . . . . . 53 6.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 7 Conclusions 66 List of Algorithms 67 List of Figures 68 List of Tables 69 Bibliography 70</p>
         <p>All men by nature desire knowledge. Aristotle</p>
      </sec>
      <sec>
         <title>Chapter 1 Introduction</title>
      </sec>
      <sec>
         <title>1.1 Motivation</title>
         <p>Metaheuristics are a popular approach to handle computationally intractable optimization problems. In the course of this master thesis an existing library dedicated to evolutionary algorithms was extended substantially by several common known and used metaheuristics. These metaheuristics are implemented in a generic manner so that their application to a widespread variety of combinatorial optimization problems is supported. A generic implementation of metaheuristics is desirable because common por- tions of many metaheuristics can be implemented problem independent and also a significant amount of problem dependent sourcecode can be shared between the metaheuristics, e.g. efficient evaluation of the objective value or neighborhood relevant methods. The basis for the implementation of the metaheuristics is the EAlib library which is developed at the Vienna University of Technology, Institute of Computergraphics and Algorithms. At the beginning of this master thesis it already contained particular classes for evolutionary algorithms and some supporting infrastructure which was also useful for our project. The aim of this master thesis the was to extend this existing library while trying to keep changes to the existing parts to a minimum to maintain compatibility with present applications.</p>
      </sec>
      <sec>
         <title>1.2 Combinatorial Optimization and Metaheuristics</title>
         <p>An optimization problem can be characterized as the selection of a “best” configuration or set of parameters to achieve some objective criteria. If the entities to  be optimized are discrete, the number of feasible solutions is finite. We call such problems combinatorial optimization problems. A combinatorial optimization problem is specified formally by a set of problem instances and is either a minimization problem or a maximization problem. An instance of a combinatorial minimization problem is a pair ( X , f ), where the solution set X is the set of all feasible solutions and the cost function f is a mapping f : X ← R . The problem is to find a globally optimal solution, i.e. an x ∗ ∈ X such that f ( x ∗ ) ≤ f ( x ) for all x ∈ X . Maximization problems can be trivially transformed into minimization problems by changing the sign of the cost function f . Salient examples are the traveling sales problem and related routing and trans- portation problems, scheduling and time-tabling, cutting and packing tasks. Most of these problems are NP-hard. However NP-hardness does not necessarily mean that all practically relevant instances are not solveable within acceptable time. Vice versa, an algorithm for a polynomial-time solvable problem might be too expensive in practice. Many different algorithmic strategies exist to deal with this problems and the metaheuristics, which are the main topic of this work, are among of them. Tradi- tionally metaheuristics are considered as solution methods utilizing an interaction between local improvement procedures and higher level strategies to overcome local optima leading to a robust search process. In general metaheuristics contain are not designed for a specific optimization problem. They rather can be applied to a wide range of problems. Therefore many metaheuristics can be implemented in a generic manner straighforward. For the library at hand five initial metaheuristics were chosen for implementation which are local search, simulated annealing, tabu search, guided local search and greedy randomized adaptive search procedures.</p>
      </sec>
      <sec>
         <title>1.3 Guide to the thesis</title>
         <p>The thesis at hand describes the quadratic assignment problem in Chapter 2 which we chose as an example problem to demonstrate the application of EAlib to a new task and to illustrate the pros and cons of the implemented metaheuristics. In Chapter 3 all featured algorithms are explained. The requirements of functionality, design and usability of the targeted library are specified in Chapter 4 while the details of the implemented library are stated in Chapter 5. Finaly experimental results of solving the quadratic assignment problem using the new EAlib are presented in Chapter 6.</p>
         <p>Science is organized knowledge. Wisdom is organized life. Imanuel Kant</p>
      </sec>
      <sec>
         <title>Chapter 2 Quadratic Assignment Problem</title>
         <p>Since the quadratic assignment problem (QAP) was mentioned first by Koopmans and Beckmann [23] in 1957, they used the QAP to model economic activities, many authors contributed to it, see Loiola et al. [27] for a recent survey article about the QAP. The major attraction points of the QAP are its practical and theoretical importance and its computational complexity — it is one of the most difficult combinatorial optimization problems. In general problem instances of size n ≥ 30 can not be solved in reasonable time. Sahni and Gonzales [39] had first shown that the QAP is a member of the class of NP-hard problems and that, unless P = NP, it is not possible to find a polynomial -approximation algorithm, for a constant . Nevertheless recent results (Gutin and Yeo [20]) proved that, in the case of QAP, polynomial approximations with factorial domination number exist. For more information on the theory of NP-completeness Garey and Johnson [14] is recommended. Since the QAP is very versatile, several other NP-hard combinatorial optimization problems such as traveling salesman problem (TSP), graph partitioning, the bin-packing problem (BPP) or the max clique problem can be formulated and solved using QAPs [5, 27]. Prior to an exact definition of the QAP, a simpler related problem, the linear assignment problem (LAP), is presented as a smoother introduction assignment. After a short description of the LAP, a comprehensive explanation of the QAP, which will cover a problem definition and various mathematical formulation approaches, resolution methods and finally applications, will be provided.</p>
      </sec>
      <sec>
         <title>2.1 Problem Description</title>
         <p>Assigning objects is a common task for econimic or techinical staff. Therefore it is not a surprise that assigment problems are among the greatest challanges in the area of combinatorial optimization.  As an introduction the linear assignment problem (LAP) is presented here. As- sume there are two equal sized sets of objects, e.g. persons and jobs, and they are assigned to each other by making up pair of those objects, taking one from each set for a pair. Additionally every possible pair is given a value, which results in a n × n matrix with n 2 elements. The problem now is to find an assignment of all objects for which the sum of the values is minimized. An example application for the LAP is the assignment of persons to jobs. Mathematically this problem can be formulated as follows.</p>
         <p>where A = [ a i,π ( i ) ] is the matrix of values for assigning object i to π ( i ) and further Π is the set of all permutations of the n elements { 1 , . . . , n } . The LAP is polynomial and is easily solved by the Hungarian method [27] which was proposed by Harold W. Kuhn in 1955 [24]. Reconsidering the above description the question arises if it really true that an assignment of two objects does not have any sideeffects on other assignments. If this assumption does not hold, the quadratic assignemnt problem may give an appropriate formal description of the real-world problem. QAP is a generalization of in the linear assignment problem in a manner that assignment can affect each another. Therefore, in addition to the value matrix — when using QAPs it is called distance matrix — a flow matrix of same dimension is introduced. As an example that is related to the previous mentioned one with persons and jobs, the distance matrix can be interpreted as the distance between the offices and the flow as the amount of interaction between these persons.</p>
         <fig id="F2.1">
            <caption>
               <p>Figure 2.1: A quadratic assignment example</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
      </sec>
      <sec>
         <title>2.2 Formulations</title>
         <p>Nowadays many different formulations are used. Loiola et al. [27] and Commander and Pardalos [9] give a good survey over the existing formulations of the quadratic  assignment problem, different resolution methods, lower bound calculation and applications.</p>
         <sec>
            <p>As an introduction the popular and very intuitive formulation is based on permutations is given. Thereby the QAP can be stated as follows. Let A , B and C be n × n matrices representing flows between objects, distances between locations and costs for assigning objects to locations, further let Π be the set of all possible permutations of the n elements { 1 , . . . , n } .</p>
            <p>a i,j is the flow between objects i and j , b π ( i ) ,π ( j ) is the distance between locations π ( i ) and π ( j ) and c i,π ( i ) is the fixed cost of assigning object i to location π ( i ). The formulation given contains a linear part to model fixed assignment cost. However many authors neglect this term of the equation, since it is a LAP and thus easy to be solved, e.g. with the Hungarian method, or because they do not need this term for their considerations; the resulting formulation is stated below:</p>
            <p>In the implementation of this master thesis we used the term to be minimized in the above formula as objective function. Consequently our solution representation consists of the permutation vector π .</p>
         </sec>
         <sec>
            <p>Koopmans and Beckman [23] used a different formulation in their initial statement of the quadratic assignment problem; the so-called integer linear programming (IP) formulation. It is still of great use, since IP is a topic of ongoing research. In this formulation the reader also can see why the problem is called quadratic, which is not so obsious in some of the other formulations. The general IP formulation is as follows. Let A = [ a i,j ] be a matrix of flows between objects i and j and further B = [ b k,p ] a matrix of the distances between positions k and p and lastly C = [ c i,k ] a matrix of costs for assigning object i to</p>
            <p>position k :</p>
            <p>The actual QAP is the problem of minizing equation above, by proper choice of the permutation matrix X = [ x i,j ]. The minimand contains a term of second degree in the unknown permutation matrix X and therefor the problem is called quadratic. For the same reason as in the prior section the linear term regarding the fixed costs of assigning objects to locatinos can be neglected, leading to the following formulation:</p>
            <p>i,j =1 k,p =1 s . t . (2.5) , (2.6) and (2.7) .</p>
         </sec>
         <sec>
            <p>Since the essential information about an actual QAP instance is represented usually with matrices it is not surprising that a formulation evolved which takes advantage of this; the trace formulation is an approach to mathematically describe the QAP that n uses the trace of a matrix which is defined by trace A = i a i,i . It was introduced by Edwards [10]. Again consider A = [ a i,j ] a matrix of flows from object i to object j , B = [ b k,p ] distances of location k and p and C = [ c i,k ] costs of assigning object i to location k .</p>
            <p>repectively with the linear term of the problem omitted:</p>
            <p>where Π is the set of all n × n permutation matrices. It is often used in lower bounds related publications.</p>
         </sec>
      </sec>
      <sec>
         <title>2.3 Lower Bounds</title>
         <p>The knowledge of lower bounds is fundamental when developing optimization algorithms to solve combinatorial or other mathematical problems. This importance of lower bounds is two-fold. At first they are an essential part of exact algorithms, e.g. branch-and-bound procedures. These methods, while attempting to guarantee the global optimum, also try to avoid the total enumeration of the complete search space. Therefore the performance of such methods depends strongly on the computational quality and efficiency of the utilized lower bounding techniques. An other application of lower bounds is the evaluation of the quality of solutions obtained by some heuristic algorithms (see Section 6.1 on page 52). The quality of the lower bound can be measured by the gap between the computed bound with the known optimal solution, this referred to as the tightness of the bound, i.e. good lower bounds are closer to the global optimum. For an exact algorithm a good bounding technique, which can find the bounds quickly 1 , should be used. When used in heuristics, lower bound quality is the most important property. One of the first suggested and best known lower bounds for the quadratic assignment problem is the one presented by Gilmore [15] and Lawler [25]. The Gilmore- Lawler-Bound (GLB) is given by the solution of linear assignment problem whose cost matrix is gained by special inner products of the flow- and distance-matrix of the original QAP. The advantage of the GLB is that is simple and it can be computed efficiently. However, its drawback is that the gap to the optimal solution grows with the size of problem. For this reason the GLB is a weak bound for larger problem instances. Due to an intensive research activity many other lower bounds have been dis- coverd. Bounds based on mixed integer linear programming (MILP) relaxations, eigenvalues of the flow- and distance matrix, reformulations of the above mentioned GLB exist. Some of them, e.g. eigenvalue based bounds, really outperform the original GLB so far tightness is concerned, but they suffer from high computation requirements. The most recent and promising research trends are based on semidefinite programming (SDP), reformulation linearization. Anstreicher and Brixius [1] presented a lower bound for the QAP based on semidefinite and convex quadratic programming, a bound using the bundle method is proposed by Rendl and Sotirov [36]. 1 Up to now no bound that features both advantages, tightness and computational cheapness has been discovered.</p>
      </sec>
      <sec>
         <title>2.4 Solution Methods</title>
         <p>Since its statement, many different approaches were applied to solve the quadratic assignment problem. These can be categorized in either exact or heuristic methods. In this section we an overview about some of the most successfull or frequent used methods of these categories are presented.</p>
         <sec>
            <p>The oldest and simplest way, to resolve the quadratic assignment problem, is enumeration . This causes evaluation of the objective function for all n ! possible permutations and memorizing the best found solutions; note that there is not necesssarily only oneoptimal solution. The computational effort for evaluating the cost of a permutation requires O ( n 2 ) steps, which has to be computed O ( n !) times yielding exponentially sized computation times. Enumeration is very simple to code and has small memory requirements, on the other hand its use is very limited and not of practical relevance. Other methods include quadratic programming, which reformulates the problem as a 0–1 program (see Section 2.2.2 on page 9) and linear programming, which linearizes the QAP by introducing new variables, the resulting linear program can be solved e.g. with mixed integer linear programming methods. Many of the above methods share the same problem; they vastly examine the complete search space and therefore, as mentioned, only small problem instances can be solved within a reasonable amount time. The most successful exact resolution methods for the quadratic assignment problem incorporate branch-and-bound (BB) algorithms. Essential for BB is a good bounding technique, because this directly affects the extent to which the search space must be enumerated; the thighter the used bound, the more solutions can be excluded from the exploration. Branch-and-bound methods attract many researchers due to their potential. For example Frazer [13] and Brixius and Anstreicher [5] describe a BB implementation and Anstreicher et al. [2] describe a grid enabled BB implementation which was used to solve a problem instance of size 30 to optimality. They report the utilization of an average of 650 worker machines over a one-weekend period, which provides the equivalent of almost 7 years of computation on one single HP9000 C3000 worksta- tion. For an other instance of the same size they utilized the equivalent of 15 years on a single C3000. These examples show the potential of parallelization, which is currently one of the major fields of interest.</p>
         </sec>
         <sec>
            <p>Heuristic algorithms, contrary to exact algorithms, can not provide any guarantee of optimality for the best solution obtained. The reason for the current research on suboptimal solution methods is the fact that many of them can provide good solutions within reasonable time constraints, which is often necessary real-world application environments. Heuristic methods include the following categories: constructive , enumeration and improvement methods.</p>
            <p>Constructive methods, which are among the earliest heuristics to solve the QAP, try to complete a permutation with each iteration of the algorithm. The selection of each assignment is based on a heuristic selection criterion. For example Gilmore [15] introduced one of the first constructive algorithms. Nowadays this category of heuristics focuses new interest because metaheuristics, such as the greedy randomized adaptive search procedure (see Section 3.5 on page 27) incorporate them. Enumerative methods are motivated by the expectation that an acceptable solution can be found early during a brute force exploration of the search space. For interesting problems these methods do not enumerate the all feasible solutions and therefore different termination criteria are used. Usually the number of total iterations, or iterations between successive improvements is used, other common criteria include a limit on the total execution time or lowering the upper bound when no further improvements are possible after a number of iterations. It is important to remind that any of these termination criteria can prohibit the finding of an optimal solution. Improvement methods correspond to local search algorithms (see Section 3.1 on page 19. Most of the heuristics for the QAP are part of this category. An other worthy to mention category of methods are approximate algorithms , which are heuristics provinding quality guarantees for their solutions.</p>
         </sec>
         <sec>
            <p>Metaheuristics are, as their name suggests, heuristic algorithms too, but usually they can be adapted straighforward to a wide range of different problems; this is in general not possible for traditional heuristics. However, as the main focus of this master thesis lays on metaheuristcs we address them extensively in the next chapter.</p>
         </sec>
         <sec>
            <p>Current state of the art algorithms can be divided into two major categories, at one side the search for optimal solutions and exact algorithms which can provide them, and on the other side methods that can provide solutions that are good enough in reasonable time. Of course also theoretical developments are of interest. The main research focus for the QAP is generated by the growing interest on metaheuristics since the end of the 1980’s because it is a popular benchmark to compare algorithms. With recent generations of computer technology the QAP attracted new attention, which lead to honorable developments in parallel algorithms. Promising future developments seem to be possible through the hybridization of several algorithms, which generated some interest in the past, together with parallelization.</p>
         </sec>
      </sec>
      <sec>
         <title>2.5 Applications</title>
         <p>The initial motivation that lead to the formulation of the quadratic assignment problem was:</p>
         <p>In the light of the practical and theoretical importance of indivisi- bilities, it may seem surprising that we possess so little in the way of successful formal analysis of production problems involving indivisible resources. (Koopmans and Beckmann [23]) [...] The assumption that the benefit from an economic activity at some location does not depend on the uses of other locations is quite inade- quate to the complexities of locational decisions.</p>
         <p>As the quoted statement suggests, a main field applications is allocation of resources with complex interactions of the individual resources. Koopmans and Beckmann were economists and therefore their focus was on economic activities. Example applications are scheduling of jobs or production lines, facility organization, hospi- tal layout. Nevertheless the QAP is also of practical use where it is not so obvious like dartboard design or typewriter layout. Not to forget many engineering applications. In the remainder of this section we illustrate two applications of the quadratic assignment problem in detail.</p>
         <sec>
            <p>In a 1961 paper [40], Leon Steinberg proposed a backboard wiring problem. The problem is about the optimal placement of computer components on a backboard in  such a manner, that the total interconnecting wiring length is minimized. Improved wiring length has two main advantages, most important it increases the performance of the designed system, not less attractive are the decreased manufacturing costs. The original problem instance consisted of 34 components with a total of 2625 interconnections which were to be placed on a backboard with 36 open positions (circles in <xref id="XR111" ref-type="fig" rid="F2.2">Figure 2.2</xref>). Two dummy components, with no connections to any other components, are added so that the number of components equals the number of open positions. The use of dummy elements is a common trick to be able to formulate real-world problems as QAPs. With this addition the mathematical formulation can be given</p>
            <p>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36</p>
            <fig id="F2.2">
               <caption>
                  <p>Figure 2.2: Original Backboard of the Steinberg Wiring Problem</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>i,j,k.l s . t . x i,j = 1 i = 1 , . . . , n j x i,j = 1 j = 1 , . . . , n i x i,j ∈ 0 , 1 i, j = 1 , . . . , n</p>
            <p>where a i,k is the number of wires interconnection components i and k , b j,l is the distance between positions j and l on the backboard and x i,j = 1 if component i is placed at position j . Special attention is payed on the choice of the b j,l . In the original paper Steinberg considered using 1-norm, 2-norm or squared 2-norm distances. He further concentrated on obtaining good solutions for the 2-norm and squared 2-norm versions of the problem. However, research interest has been directed to the 1-norm version, which was also used by Brixius and Anstreicher [6] who solved the initial problem instance to optimality with an exact branch-and-bound algorithm, 40 years after its statement. The solution required approximately 186 hours of CPU time on a single Pentium III personal computer.</p>
         </sec>
         <sec>
            <p>At the National Aeronautics and Space Administration (NASA) another interesting application of the quadratic assignment problem is reported by Padula and Kincaid [33]. It is known that NASA often has to design and erect antennas (see <xref id="XR123" ref-type="fig" rid="F2.3">Figure 2.3</xref>(a)) in space for different purposes like communication with spacecrafts (Deep Space Network). Such an antenna consists of a very large number n of truss elements. For research purposes, the antenna structure is designed as a tetrahedral truss with a flat top surface, which means that all nodes in the top surface of the finite-element model are coplanar (see <xref id="XR124" ref-type="fig" rid="F2.3">Figure 2.3</xref>(b)). To minimize surface distortions and to the avoid internal forces during the assembly process of the antenna, the truss elements have to be of identical length. However, due to limitations in the manufacturing process, the length is never precisely identical. Each truss element j has a small but measurable error e j . To overcome the impact of these errors, the truss elements are assembled in such a way, that the errors offset each other. For a mathematical formulation of the described problem of arranging the truss elements first, an objective value has to be defined. The objective value of a concrete arrangement is stated as the squared L 2 norm of the surface distortion:</p>
            <fig id="Fx126">
               <caption>
                  <p/>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>(a) Antenna configuration (b) Finite element model</p>
            <fig id="F2.3">
               <caption>
                  <p>Figure 2.3: Conceptual design of a large space antenna (from [33])</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>where e is the vector of measured errors, U is the influence matrix such that u i,j gives the influence of a truss length error in element j on the surface at node i and D is a positive semidefinite weighting matrix that denotes the relative importance of each node i at which distortion is measured. The calculation of matrix U is can  be done with any structural analysis software package and the matrix D is often the identity matrix. Summarizing this, the combinatorial optimization problem for minimizing antenna distortions is stated as:</p>
            <p>where E are all possible permutations of the error vector e . Clearly the formulation above is a quadratic assignment problem, although it is not a common formulation; compare the permutation formulation in equation 2.3 on page 9. In case of the antenna assembly sequence problem simulated annealing and tabu search where applied successfully to solve the problem. Prior to this attempts a pairwise interchange heuristic was suggested, which was based on a simple basic local search algorithm. It is not very surprising that the results achieved with local search where inferior to the ones obtained by simulated annealing or tabu search. The main advantage for NASA gained by metaheuristically optimized assembling of the truss elements standard precision is adequate which decreases the overall costs since cost for truss elements increase dramatically when unusual precision in length is required. This example shows that an engineering description of a problem can lead directly to a convenient solution method; however this is not the usual case.</p>
            <p>For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled. Richard Feynman</p>
         </sec>
      </sec>
      <sec>
         <title>Chapter 3 Metaheuristics</title>
         <p>During the last decades a new kind of heuristic algorithms has emerged which tries to use lower-level heuristic approaches to build higher-level frameworks targeted at efficiently and effectively exploring a search space. The name metaheuristic, first introduced in Glover [16], stems from the composition of two Greek words. Heuristic derives from the verb heuriskein ( υρισκ ιν ) which means “to find” and the prefix meta means “beyond, in an upper level”. This category of algorithms includes 1 Evolutionary Computing (EC) and Genetic Algorithms (GA), Guided Local Search (GLS), Greedy Randomized Adaptive Search Procedure (GRASP), Iterated Local Search (ILS), Simulated Annealing (SA), Tabu Search (TS), Variable Neighborhood Search (VNS) and many more. For example, Glover and Kochenberger [19] and Blum and Roli [4] provide a survey on metaheuristics and related topics and current state of the art in the area. In this chapter we focus on the concepts and fundamental principles of the metaheuristics implemented during this master thesis. But before we start off some some terms need to be clearyfied. We consider a neighborhood structure as a function N : X → 2 X , which assigns each valid solution x ∈ X a set of neighbors N ( x ) ⊆ X . The set N ( x ) is commonly named the neighborhood of x . It is usually defined implicity through valid changes ( moves ) on the solutions x ∈ X . Furthermore we introduce a search space , i.e. a solution representation and an objective function. In other words a search space is a collection of possible solutions to the problem at hand, incorporation some notion of distance between the candidate solutions. 1 In alphabetical order.</p>
      </sec>
      <sec>
         <title>3.1 Basic Local Search</title>
         <p>Basic local search (LS) is also called iterative improvement or hill-climbing because at each iteration a move is only performed when the new solution is better than the current solution, regarding to a defined objective function. A move is defined as the selection of a solution s out of a neighborhood N ( s ) of a solution s .</p>
         <p>procedure Basic Local Search s ← GenerateInitialSolution() repeat s ← ChooseNeighbor( N ( s )) if f ( s ) ≤ f ( s ) then s ← s end if until termination conditions met end procedure Algorithm 1: Basic Local Search</p>
         <p>In Algorithm 1 the basic algorithm is outlined in pseudocode. First of all the most important task is to define a search space. This means a representation of real-world objects and an objective function f are needed. Regarding the chosen representation an appropriate neighborhood structure has to be found. A popular choice for many combinatorial optimization problems is the 2-opt 2 neighborhood because it can be applied easy to many problems. Nevertheless, despite some exemptions, 2-opt tends to get stuck in local optima. Some other neighborhoods are k -flip for binary strings where the neigborhood consits of all solutions that have a Hamming-Distance less or equal to k . A generalized 2-opt, k-opt is also known. The GenerateInitialSolution function is needed to generate an initial solution at which the search begins. This could happen simply by a completely random choice or a more sophisticated construction method. As ChooseNeighbor( N ( s ) ) function, also called step function , theoretically any function that chooses a solution s out of a neighborhood N ( s ) of solution s is possible, but it has turned out that only a few are commonly used:</p>
         <p>random neighbor picks a neighboring solution out of N ( s ) at random. first improvement systematically searches N ( s ) and chooses the first neighboring solution that is better than s . best improvement completely explores N ( s ) and takes the best neighboring solution. 2 A 2-opt move consists of removing two edges of a given solution and reconnect them in a different way.</p>
         <p>Finally the termination conditions have to be defined. In case of the latter two ChooseNeighbor( N ( s ) ) functions the simple condition stop if no further improvement is made will almost always only find a local optimum. Other possible termination conditions depend on the amount of passed CPU-Time, number of iterations, number of iterations since the last improvement or any combination of these or other conditions, which is virtually always desired. Depending on the chosen neighborhood the basic local search algorithm often only yields poor locally optimal solutions and is therefore only of limited use. To address this weakness, many advanced local search methods where proposed. Among others iterated local search [28, 29], multi-start methods [30], guided local search, greedy randomized adaptive search procedure, simulated annealing and tabu search have been developed.</p>
      </sec>
      <sec>
         <title>3.2 Simulated Annealing</title>
         <p>Simulated annealing (SA) was the first major attempt to improve basic local search, which does not perform well if caught in a local optima — as pointed out in in the last section. It was proposed independently by Kirkpatrick et al. [22] and Cerny [8] during the early 1980s and it is commonly said that SA is the oldest among the metaheuristics. Simulated annealing is inspired by the physical process of cooling crystalline matter, hence it is often referred to as a nature inspired method. The fundamental idea of simulated annealing is that in contrary to basic local search moves resulting in solutions of worse quality than the current solution are allowed with a certain probability in order to escape from local optima; these moves are referred to as uphill moves. The probability of accepting an uphill move depends on the actual deterioration and the current temperature, which is decreased during the search process. The simulated annealing metaheuristic is outlined as pseudocode in Algorithm 2 on the following page. At first the algorithm generates an initial solution either randomly or with some construction heuristic and initializes the so-called temperature parameter T and the counter t . Then at each iteration of the annealing process a solution s ∈ N ( s ) is randomly chosen and accepted as new current solution depending on f ( s ), f ( s ) and T . The solution s replaces s as new current solution if f ( s ) &lt; f ( s ) or, when f ( s ) ≥ f ( s ), with a probability which is a function of T , f ( s ) and f ( s ). Generally the probability is computed following the Boltzmann distribution . Metropolis et al. [31] have used this method when they simulated the movement of particles in cooled matter, therefore the name Metropolis-Criterion became popular for the following</p>
         <p>procedure Simulated Annealing s ← GenerateInitialSolution() t ← 0 T ← T 0 repeat repeat s ← arbitrary solution ∈ N ( s ) if f ( s ) &lt; f ( s ) then s ← s else if Z &lt; e −| f ( x ) − f ( x ) | /T then s ← s end if end if t ← t +1 until temperature-update conditions met T ← g( T , t ) until termination conditions met end procedure Algorithm 2: Simulated Annealing inequation:</p>
         <p>with Z = random number ∈ [0 , 1)</p>
         <p>The most crucial part in parameterizing simulated annealing is the selection of an appropriate cooling scheme , which strongly affects convergence speed and result quality. The idea is to decrease temperature during the search process so that at the beginning uphill moves are accepted with a high probability which decreases step-by-step with the following iterations. This is analogous to the natural process of annealing metals or glass. While temperature is relatively high the search is not biased in a strong way and uphill moves are accepted regularly, with descending temperature the search is biased towards classical iterative improvement and accepting uphill moves will become unlikely; Simulated annealing can therefore be understood as a mixture of a random walk and iterative improvement. The cooling scheme defines the temperature T at each iteration t of the annealing process. It consists of the definition of a starting temperature T 0 , a function g ( T, t ) with which the actual cooling is computed and the number of iterations between updates of the temperature. The choice of T 0 can be made upon statistical data or bounds. The number of iterations at each temperature should allow the procedure  to reach a stable state, which means that no more moves that only are allowed at this temperature should be necessary to reach a global optimal solution — physicists call this state an equilibrium. This number of iterations is usually set to a multiple of the size of the neighborhood. For updating the temperature no specific type of function is necessary, but commonly a monotone descend function is used, e.g. geometric cooling.</p>
         <p>s . t . α &lt; 1</p>
         <p>The advantages of simulated annealing are that it is one of the best studied metaheuristics existing. For example it is proven that under certain conditions, e.g. infinite runtime, simulated annealing converges to a global optimum (Henderson and Jacobson [21]). Simulated annealing is easy to implement and can be adopted to a wide range of applications, although for good results often long runtimes are needed. Simulated annealing is subject of continued research. Some of the more recent trends to improve practical performance are advanced cooling schemes including non-monotonic cooling ( reheating ), dynamic cooling, deterministic neighborhood exploration, parallelization and hybridization with for example genetic algorithms or GRASP.</p>
      </sec>
      <sec>
         <title>3.3 Tabu Search</title>
         <p>The elementary ideas of tabu search (TS) were first introduced by Glover [16] in 1986. Tabu search is one of the most cited and applied metaheuristics in the field of combinatorial optimization problems. In its basic version, described in Algorithm 3 on the next page, tabu search performs a best improvement local search (see Section 3.1 on page 19) and additionally uses a short term memory , which allows to escape from local optima and avoids cycles during exploration of the search space. This short term memory is implemented as a tabu list that remembers recently visited solutions and forbids moves towards them. The neighborhood of the current solution is restricted to solutions that do not belong to the tabu list, the resulting set is the so-called allowed set . Similar to other metaheuristic methods an initial solution is generated randomly or with a construction heuristic, the tabu list T L is initialized with the empty set. At each iteration of the search process the best solution of the allowed set of the neighborhood of the current solution is selected as new current solution and added to the tabu list; an element of the tabu list is removed from it; usually the selection of this element is based on recency, i.e. removal in FIFO order. An essential property</p>
         <p>procedure Basic Tabu Search s ← GenerateInitialSolution() x ← s TL ← ∅ repeat X ← part of N ( x ) that does not violate T L x ← best solution ∈ X add x to T L remove elements older than t L iterations from T L x ← x if f ( x ) &lt; f ( s ) then s ← x end if until termination conditions met end procedure Algorithm 3: Basic Tabu Search</p>
         <p>of this process is that it allows to select new solutions with a worse solution quality than the current solution, because the search must not stop when it finds the first local optimum. An important parameter is the length of the tabu list ( tabu tenure ). Small tabu tenures allow the process to concentrate on small areas of the search space. On the other side, large tabu tenures will forbid the process to revisit more solutions and thus a better exploration of the entire search space is enforced. The tabu tenure can be varied during the search process to improve the robustness of the algorithm and quality of results. Robust tabu search (see Taillard [41]) changes the tabu list length randomly during the search between a mininum and maximum size, while reactive tabu search (see Battiti and Tecchiolli [3]) increases the tabu tenure if there is evidence that some solutions are visited repeatedly. As a result the diversification of the process is increased, while the tabu tenure is decreased if there is no further improvement, which intensifies the search process. However, the major problem of this basic tabu search algorithm is that it stores complete solutions in its short term memory. Managing tabu lists is thus inefficient because they make exhaustive use of memory and it takes significant computational effort to deal with them. Therefore, instead of storing complete solutions only tabu attributes are typically stored. These attributes characterize a performed move. E.g. in case of the traveling salesman problem when a 2-opt move is performed the two removed edges or alternatively the two newly introduced edged may be stored as tabu attributes, and every solution that is generated using this attributes does not qualify for the allowed set, it is tabu . Because more than one attribute can be defined, a tabu list is introduced for each of these attributes.  This new type of tabu lists is much more effective, although it raises a new problem. With forbidding an attribute as tabu, typically more than one solution is declared as tabu. Some of these solutions that must now be avoided might be of excellent quality and have not yet been visited. To overcome this problem, aspiration criteria are introduced which allow to override the tabu state of a solution and thus include it in the allowed set. A commonly used aspiration criterion is to allow solutions which are better than the currently best known solution. A sketch of the procedure summarizing the above techniques is provided in Algorithm 4. procedure Tabu Search s ← GenerateInitialSolution() x ← s T L 1 . . . T L n ← ∅ repeat X ← part of N ( x ) that does not violate T L 1 . . . T L n or satisfies at least one aspiration criterion x ← best solution ∈ X add x to T L 1 . . . T L n remove elements older than t L iterations from T L 1 . . . T L n x ← x if f ( x ) &lt; f ( s ) then s ← x end if until termination conditions met end procedure</p>
         <p>Algorithm 4: Tabu Search</p>
         <p>Additionally to the above described tabu lists, which represent a short term memory, other ways of taking advantage from information about the search history are possible. Every piece of information collected during the search process can be useful. This long term memory can be structured regarding to four principles: recency , frequency , quality and influence . A recency-based memory records for each solution, or attribute, the most recent iteration it was considered in, while frequency-based memory counts how many times each solution (attribute) has been visited. This information identifies the subset of the search space where the process stayed for a longer number of iterations or where it only examined a limited amount of solutions, so it is useful to control the diversification of the search process. The information regarding quality can be used to determine good solution attributes, which can be integrated in solution construction. Finally influence, a property regarding decisions during the search process, allows to identify the most critical decisions. For further information the reader is encouraged to look at two articles by Fred Glover [17, 18], which provide a good starting-point for deeper insight into tabu search and related methods.</p>
      </sec>
      <sec>
         <title>3.4 Guided Local Search</title>
         <p>Guided local search (GLS) is a metaheuristic that sits on top of another local search procedure. It modifies the landscape of the search space to guide the underlying heuristic method away from already encountered local optima. The roots of the GLS metaheuristic are in a neural-network based method called GENET (see Tsang and Wang [43]) which is a constraint satisfaction resolution method. As mentioned, GLS modifies the landscape of the search space, to guide the underlying local search method gradually away from known local optima. To accomplish this it augments the objective function of the underlying local search procedure with penalties, which makes the known local optima less attractive (see <xref id="XR183" ref-type="fig" rid="F3.1">Figure 3.1</xref>). In Algorithm 5 on page 27 the basic guided local search procedure is described in pseudocode. Guided local search applies the penalties to solution features which have to be defined. These features may be any property or characteristic that can be used to distinguish solutions; compare the tabu attributes of tabu search. E.g. in the case of the traveling salesman problem these features could be arcs between pairs of cities and in the case of the quadratic assignment problem facility-location assigments (see Voudouris and Tsang [44] and Mills et al. [32]). For each defined feature f i the following components must be provided:</p>
         <p>function objective Solution space</p>
         <fig id="F3.1">
            <caption>
               <p>Figure 3.1: Escaping a local optimum with GLS</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>• An indicator function I i ( s ) that indicates whether the feature f i is present in the current solution or not.</p>
         <p>• A cost function c i ( s ) describes the cost of having the feature f i present in the</p>
         <p>current solution s . These costs are often defined in analogy to the objective function. • And finally p i , the penalty parameters , which are initialized with 0 for all features. The penalty parameters are used to penalize features that appear in local optima.</p>
         <p>Given an objective function g ( s ), which maps each solution of the search space to a numeric value, GLS defines a new augmented objective function h ( s ) which will be used by the underlying local search procedure.</p>
         <p>Updating the penalty values p i of the features when reaching a local optimum is the crucial task in guided local search. A common way to do this is to calculate a utility value U til ( s, i ) of a feature i at the current local optimum s :</p>
         <p>The penalty values of the features with maximimum utility value will be incre- mented. Then, local search is applied again with the updated penalties and changed augmented objective function. The higher the costs c i ( s ) the higher the utility of the feature. The costs are scaled by the penalty value to permit the search process from being totally cost driven by taking the search history into account. A problem is that during the search process, where more and more features are penalized, the landscape of the search space could be distorted too much. This will make further exploration difficult and so in addition to increasing the penalty values a multiplication rule is applied regularly, which is smoothing the landscape again. The λ parameter, also called regularization parameter , is used to specify the influence of the penalty values on the augmented objective function, which controls the diversity of the search process. With increasing λ the diversification will increase, too. The right choice of λ is crucial. This, however, must be done individually for each problem, because it is specific to the used objective function g ( s ). The difference ∆ h of the values of the augmented objective function between two consecutive moves helps to understand this.</p>
         <p>If the regularization parameter λ is large enough the inner local search procedure will solely remove the penalized features and the information regarding penalty values will fully determine the path of the search process. In contrast if, λ is to small the local search procedure will ignore the penalty values and it will not be able to escape from local optima. A good choice of λ is therefore in the same order of magnitude as ∆ g and the resulting moves will aim at the combined objective, which is to improve the solution and to remove penalized features from the generated solutions. A common solution for this problem is to introduce a α parameter which is used to tune the now dynamically computed λ parameter, taking into account information about the problem instance. The advantage of this method is that once α is tuned well enough it can be used for many problem instances (see Voudouris and Tsang [44]).</p>
         <p>procedure Guided Local Search s ← GenerateInitialSolution() for i = 1 , . . . , n do p i ← 0 end for repeat s ← LocalSearch( s , g + λ · n i =1 I i · p i ) for all features i with maximum utility U til ( s, i ) do p i ← p i + 1 end for until termination conditions met end procedure Algorithm 5: Guided Local Search</p>
         <p>Here only the main concepts of guided local search are described but many additional ideas and improvements where proposed and applied successfully in different applications such as Fast GLS. Also several other refinements of the algorithm are possible such as e.g. iterative penalty value updates (Voudouris and Tsang [42] and [45]).</p>
      </sec>
      <sec>
         <title>3.5 Greedy Randomized Adaptive Search Procedure</title>
         <p>The Greedy Randomized Adaptive Search Procedure (see Feo and Resende [11, 12]) is a simple but powerful metaheuristic that combines a constructive heuristic with local search. The basic structure of GRASP is outlined in Algorithm 6 on the following page. GRASP is an iterative multi-start procedure which consists of two phases, the construction phase builds a feasible solution, whose neighborhood is explored to  find a local optimum in the subsequent local search phase. The best solution found in any iteration is returned as final result of the search process.</p>
         <p>procedure GreedyRandomizedAdaptiveSearchProcedure repeat s ← GreedyConstructSolution() s ← LocalSearch( s ) if f ( s ) &lt; f ( s ) then s ← s end if until termination conditions met end procedure Algorithm 6: Greedy Randomized Adaptive Search Procedure</p>
         <p>The construction phase itself, outlined in Algorithm 7 on the next page, is characterized by two major properties: a dynamic constructive heuristic and random- ization. It is assumed that a solution consists of a subset of components, analogous to Section 3.4 on page 25 where these components could be used as GLS features. During the construction phase the solution is put together step-by-step, adding a new component in each iteration. The selection of the new component is done at random out of the restricted candidate list (RCL). It is essential that the construction heuristic is dynamic , which means that the score for each solution component is evaluated depending on the current partial solution. In contrast static construction heuristics assign a score to each solution component prior to the construction process. The most critical part of the GRASP construction phase is the BuildRestrict- edCandidateList procedure, since it determines the strength of the heuristic bias. An incremental cost c ( e ) is associated with the inclusion of a component e ∈ CL into the currently constructed solution. Further at each iteration let c min and c max be the smallest and the largest incremental costs and subsequently the restricted candidate list is made up by the most promising components e ∈ CL , i.e. with the best incremental costs c ( e ). An easy solution for this problem is to limit the RCL by the number of its elements. The list is made up by k components with the best incremental costs c ( e ), where k is a parameter which has to be carefully tuned. In its extremes k is either set equal to 1, resulting in a construction procedure which degenerates to a deterministic greedy heuristic, because only the best element at each iteration would be considered for the RCL. If k = n , where n is the size of CL , i.e the number of possible components, the construction is done completely at random. On the other side the restricted candidate list can be limited by the quality of the components. Therefore a threshold parameter α ∈ [0 , 1] is associated with the RCL.</p>
         <p>procedure GreedyConstructSolution s ←∅ while solution s is not complete do CL ← all possible extensions e of solution s RCL ← BuildRestrictedCandidateList(CL) e ← select an element of RCL at random s ← s ⊕ e end while end procedure Algorithm 7: GRASP construction phase</p>
         <p>All components whose costs c ( e ) are superior to the threshold value are included, so the condition c ( e ) ∈ [ c min , c min + α · ( c max − c min )] has to be fulfilled by each element of the RCL. In analogy to the previous RCL selection method the extreme cases exist, too, with α = 1 resulting in a pure greedy heuristic and α = 0 equivalent to a pure random construction. In both cases k and respectively α are important parameters which strongly determine the sampling of the search space and hence the quality of the resulting solutions. It is essential to the success of GRASP that the most promising regions of the solution space are sampled during the construction phase. Also it is important that the constructed solutions belong to basins of attraction of different local optima to ensure sufficient diversification. The first condition can be achieved by a good choice of the construction heuristic and its parameters. For the second condition an appropriate choice of the construction heuristic and the subsequent local search are the key to success. In the given description of the GRASP metaheuristic memory in terms of history was not mentioned. This is one of the reasons why GRASP is often outperformed by other metaheuristics. However, due to its simple concept GRASP is easy to implement for many applications. For example, applications exist for the set covering and maximum independent set problem by Feo and Resende [12] or the quadratic assignment problem by Li et al. [26]. Also the iterations for creating candidate solutions usually are fast and so GRASP is able to provide good quality solutions in a short amount of time. To improve the performance of GRASP several techniques are possible. As mentioned above the construction phase, especially the creation of the restricted candidate list, is critical. Some enhancements address this problem. With Reactive GRASP the RCL parameter α is not constant; in each iteration it is selected from a discrete sequence [37], yielding in a more robust algorithm. Other methods include a biased selection of new elements from the RCL, e.g. with a probability proportional to 1 /c ( e ). Parallelization can also be easily applied to GRASP [38].  Current research trends show that GRASP can gain a great performance boost if it is used in a hybrid manner. So it is possible to use greedy constructed solutions as starting population within evolutionary algorithms. The use of simulated annealing or tabu search within GRASP has been applied successfully, too.</p>
         <p>Not even the gods fight against necessity. Simonides</p>
      </sec>
      <sec>
         <title>Chapter 4 Requirements</title>
         <p>At the beginning of this master thesis the basic idea was to extend the existing EAlib [35] with some additional metaheuristics, since EAlib at that time only contained evolutionary algorithms. The EAlib is intended to be a problem-independent C++ library suiteable for the development of efficient metaheuristics for combinatorial optimization. It is developed at the Institute of Computergraphics and Algorithms, Vienna University of Technology, Austria since 1999. This chapter is structured into a description of the functional , design and usability requirements that were stated initially.</p>
      </sec>
      <sec>
         <title>4.1 Functionality</title>
         <p>Before we start off, a summary of the functionality that EAlib provided already is given. As mentioned, EAlib included initially classes for evolutionary algorithms (EA). In particular classes that provide a generic framework for a generational EA, a steady state EA and an EA using the island model were implemented. Some supporting classes designed for populations and subpopulations, chromosomes, i.e. solutions, parameter handling and logging were provided, too. For demonstration purposes an implementation of the simple ONEMAX problem is also included. As mentioned the primary goal is to enhance EAlib with classes that provide a framework for some commonly known metaheuristics. After some consideration we selected the following five:</p>
         <p>• Local Search • Simulated Annealing • Tabu Search • Guided Local Search</p>
         <p>• Greedy Randomized Adaptive Search Procedure</p>
         <p>Additionally an auxiliary more complex example problem should be implemented, for which we chose the already described quadratic assignment problem. Another important task is to enhance the existing parameter handling mechanism, because EAlib initially only featured a global parameter namespace. In the following we describe in detail the functional requirements on the individual components of the implementation.  An iterative improvement algorithm as described in Section 3.1 on page 19 should be developed. Therefore the standard step functions random neighbor, next improvement and best improvement are required too. The implementation of the simulated annealing algorithm should be straightforward. It should feature geometric as standard scheme, and the acceptance probability of down hill moves is to be calculated with the Metropolis criterion. The main features desired for tabu search are handling of an arbitrary number of tabu lists for different purposes. Due to the requirement for tabu attributes are to be used, of course support of aspiration criteria must be provided too. The requirements for the GLS implementation are straightforward. An appropriate mechanism for feature evaluation is needed. Additionally it is desired that the λ GLS parameter is automatically tuned, utilizing user provided α parameter and the size of the current instance, as described in Section 3.4 on page 25. GRASP has not many requirements, a simple construction heuristic must be provided and the underlying local search algorithm should be selectable. Besides the actual implementation of the generic algorithms an example problem has to be addressed. It serves two different purposes, at first it should of course show the possible potential of the used metaheuristics and secondly it should act as a template for developing other applications with EAlib. But of course demonstrating the benefits of a generic implementation of metaheuristic algorithms, like we did in this master thesis, is also one of the aims to be achieved. To fulfill this requirements certain aspects have to be considered:</p>
         <p>Local Search</p>
         <p>Simulated Annealing</p>
         <p>Tabu Search</p>
         <p>Guided Local Search</p>
         <p>Greedy Randomized Adaptive Search Procedure</p>
         <p>Example Problem</p>
         <p>• it must be a combinatorial optimization problem , since EAlib is designed for this type of problems, • computational and pratical hard to solve • practical relevance of the problem • existence of compareable results • existence of standard instancances for testing purposes • well known • easy understandable problem structure • adequate to fulfill demonstration purposes</p>
         <p>Initially we considered three proplems, maximum satisfiability, quadratic assignment and glass cutting. The latter one was droppen early because it is too complex for use as a demonstraton problem. As noted, finally the quadratic assignment problem was selected. In particular the QAP implementation must feature all algorithms with their specialities. I.e. appropriate step functions, tabu attributes, features for guided local search and a construction heuristic are needed.  The initial version of EAlib only featured one global parameter namespace in an application. Although this concept is simple and robust the major drawback of it is, that hierarchical parameter settings are not possible. This is not satisfactory when for example nested algorithms, like guided local search or GRASP which incorporate another inner local search algorithm , or other advanced methods are used. It is obvious that the inner local search should be parametrised without tampering the parameter settings of the outer algorithm. Apparently the extended parameter handling has to ensure compatibility with existing applications.</p>
         <p>Parameter Handling</p>
      </sec>
      <sec>
         <title>4.2 Design</title>
         <p>Building a problem independent library is a complex task and many decisions are not obvious at hand. Therefore designing such a library is a sophisticated task to acccomplish. Though this master thesis is based on an existing library and so many decisions are somewhat constrainted. Special attention has to paid for the design of the specialities of the individual algorithms, because they should not interfer each another, but as much as possible of the original ideas should be realised. To accomplish this special functionality is to be declared in a separate interface class which must be inherited if a class implements it. Examples for such interfaces are:</p>
         <p>• augmented objective values • construction heuristics • features • tabus • tabulists</p>
         <p>The use of common coding patterns is also encouraged, to make live easier for future changes and enhancements and to help developers understanding the sourcecode. For example functionality should be divided in reasonable methods within a particular class to ease customizations by users.</p>
      </sec>
      <sec>
         <title>4.3 Usability</title>
         <p>The EAlib is designed to help developing metaheuristics for combinatorial optimization problems. Therefore it is important that the user-visible part of the desired EAlib extensions meet some fundamental requirements which are summarised here:</p>
         <p>• easy to learn and clear programming interface • good documentation, at best with an C++ language integrated tool like doxy- gen • support for basic features included</p>
         <p>Work saves us from three great evils: boredom, vice and need. Voltaire</p>
      </sec>
      <sec>
         <title>Chapter 5 Implementation</title>
         <p>In this chapter a detailed description of our implementation is provided. At first a in-depth look on the internals is given in Section 5.2 on page 37 and afterwards an usercentric description of the interface and a guide for using the new classes is give in Section 5.3 on page 46.</p>
      </sec>
      <sec>
         <title>5.1 Overview</title>
         <p>In <xref id="XR266" ref-type="fig" rid="F5.1">Figure 5.1</xref> on the next page an instant overview of the most important EAlib classes is given in UML syntax. The figure shows the how the classes are related to each other by inheritance, realization or an other dependency. As depicted in the EAlib class overview the most important base classes are ea base and chromosome , where ea base is the top level base class for all algorithms, for both evolutionary and local search alike algorithms and chromosome is the top level base class for user provided problems, i.e. if a new application is to be created using EAlib it is required that the actual problem is implemented as a child class of chromosome and if necessary of some interfaces for to provide specialized methods for certain algorithms, e.g. guided local search.</p>
      </sec>
      <sec>
         <title>5.2 Class reference</title>
         <p>In this section we present the details of the implementation at hand. It is structured into a description of the individual classes and of the developers view of the new parameter handling mechanism.</p>
         <sec>
            <p>chromosome #objval: double #objval_valid: bool #length: int #alg: ea_base* #pgroup: string +&lt;&lt;constructor&gt;&gt; chromosome() +&lt;&lt;constructor&gt;&gt; chromosome(l:int,t:ea_base*=NULL,pg:pstring="") +&lt;&lt;constructor&gt;&gt; chromosome(l:int,pg:pstring="") +&lt;&lt;destructor&gt;&gt; ~chromosome() +createUninitialized(): chromosome* +clone(): chromosome* +operator=(orig:chromosome&amp;): chromosome&amp; +equals(orig:chromosome&amp;): bool +dist(c:chromosome&amp;): double +obj(): double +delta_obj(m:move&amp;): double +applyMove(m:move&amp;): void +initialize(count:int): void +mutation(prob:double): void +mutate(count:int): void +crossover(parA:chromosome&amp;,parB:chromosome&amp;): void +locallyImprove(): void +reproduce(par:chromosome&amp;): void +write(ostr:ostream&amp;,detailed:int=0): void +save(fname:char*): void +load(fname:char*): void +isBetter(p:chromosome&amp;): bool +isWorse(p:chromosome&amp;): bool +invalidate(): void +hashvalue(): unsigned long int +selectNeighbour(): void +selectRandomNeighbour(): void +selectImprovement(find_best:bool): void +setAlgorithm(alg:ea_base*): void #objective(): double</p>
            <fig id="F5.2">
               <caption>
                  <p>Figure 5.2: Class chromosome</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>This is the base class for all chromosomes. In EAlib the problem definition is given by deriving a new class from chromosome and implement all the problem relevant methods in a proper way, i.e. all pure virtual methods have to be implemented. additionally depending on the desired use, other virtual methods have to be reim-</p>
            <p>plemented, e.g. save and load or selectRandomNeighbour and selectImprovement to enable local search alike algorithms.</p>
         </sec>
         <sec>
            <p>ea_advbase +pop: pop_base* +nGeneration: int +nSelections: int +nCrossovers: int +nMutations: int +nDupEliminations: int +nCrossoverDups: int +nMutationDups: int +nLocalImprovements: int +nTabus: int +nAspirations: int +nDeteriorations: int #genBest: int #timGenBest: double #tmpChrom: chromosome* +&lt;&lt;constructor&gt;&gt; ea_advbase(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; ea_advbase(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~ea_advbase() +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +run(): void +performGeneration(): void +performCrossover(p1:chromosome*,p2:chromosome*,c:chromosome*): void +performMutation(c:chromosome*,prob:double): void +terminate(): bool +replaceIndex(): int +replace(c:chromosome*): chromosome* +printStatistics(ostr:ostream&amp;): void +writeLogEntry(inAnyCase:bool=false): void +writeLogHeader(): void +getBestChrom(): chromosome* +getGen(): int +getGenBest(): int +getTimGenBest(): double +tournamentSelection(): int #checkPopulation(): void #saveBest(): void #checkBest() #perfGenBeginCallback(): void #perfGenEndCallback(): void</p>
            <fig id="F5.3">
               <caption>
                  <p>Figure 5.3: Class ea advbase</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>The abstract base class for algorithms. Any new algorithm should use ea advbase as the base class, if no other derived class suits the requirements.</p>
            <p>lsbase +&lt;&lt;constructor&gt;&gt; lsbase(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; lsbase(pg:pstring&amp;="") +replace(p:chromosome*): chromosome*</p>
            <fig id="F5.4">
               <caption>
                  <p>Figure 5.4: Class lsbase</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>This is the base class for local search alike algorithms, i.e. algorithms that are not population base. To be as much compatible as possible with population based algorithms, no additional data members are introduced, instead a fixed subset of the population, namely the first element, is considered to be used.</p>
         </sec>
         <sec>
            <p>localSearch +&lt;&lt;constructor&gt;&gt; localSearch(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; localSearch(pg:pstring&amp;="") +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +performGeneration(): void</p>
            <fig id="F5.5">
               <caption>
                  <p>Figure 5.5: Class localSearch</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>The localSearch class implements the basic local search functionality as outlined in Section 3.1 on page 19. Additionally to its main base class lsbase it inherits the glsSubAlgorithm interface class, too, because we consider localSearch as embedded algorithm for guided local search.</p>
         </sec>
         <sec>
            <p>simulatedAnnealing #T: double +&lt;&lt;constructor&gt;&gt; simulatedAnnealing(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; simulatedAnnealing(pg:pstring&amp;="") +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +performGeneration(): void +accept(o:chromosome*,n:chromosome*): bool +cooling(): void</p>
            <fig id="F5.6">
               <caption>
                  <p>Figure 5.6: Class simulatedAnnealing</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>This class provides an application independent implementation of the simulated annealing metaheuristic (see Section 3.2 on page 20). The two main steps of the simulated annealing process are subdivided into the accept and the cooling methods. As suggested, the Metropolis-Criterion and geometric cooling are utilized. Compared with local search, simulated annealing adds a temperature to the state; therefor the attribute T is introduced. In the overwritten version of the performGeneration method accept is called when a newly selected neighbour has an inferior objective value than the current solution. The cooling method is called accordingly to the sacint parameter.</p>
         </sec>
         <sec>
            <p>tabuSearch +tl_ne: tabulist* +&lt;&lt;constructor&gt;&gt; tabuSearch(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; tabuSearch(pg:pstring&amp;="") +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +performGeneration(): void +isTabu(t:tabuAttribute*): bool +aspiration(c:chromosome*): bool</p>
            <fig id="F5.7">
               <caption>
                  <p>Figure 5.7: Class tabuSearch</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>The tabuSearch class offers a basic tabu search metaheuristic (see Section 3.3 on page 22). It utilizes a single tabulist and contained tabu attributes are considered as tabu. To overcome the problem of prohibiting a solution that is the best known sofar, a default aspiration criterion is implemented, too. Chromoses used in combination with tabu search have to inherit the tabuProvider interface class and should therefore take care of the embodied tabulists appropriately.</p>
         </sec>
         <sec>
            <p>This class provides a guided local search algorithm (see Section 3.4 on page 25), which can only be used in combination with a chromosome class that implements the featureProvider interface; the embedded algorithm has to be derived from glsSubAlgorithm class. This is necessary to ensure all involved classes are well prepared with respect to the requirements of guided local search. Statistics of the embedded algorithms are summarized, whereas the generation counter is threated individually to avoid sideeffects related to termination criteria and penalty resets.  To provide a population for the embedded algorithm an additional population is created by using the existing chromosomes as a template. This ensures that running the embedding algorithm has no hidden side-effects.</p>
            <p>guidedLS #f: feature* #lambda: double #spop: pop_base* +&lt;&lt;constructor&gt;&gt; guidedLS(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; guidedLS(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~guidedLS() +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +performGeneration(): void +aobj(c:chromosome*): double +delta_aobj(c:chromosome*,m:move*): double</p>
            <fig id="F5.8">
               <caption>
                  <p>Figure 5.8: Class guidedLS</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>GRASP #spop: pop_base* +&lt;&lt;constructor&gt;&gt; GRASP(p:pop_base&amp;,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; GRASP(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~GRASP() +clone(p:pop_base&amp;,pg:pstring&amp;=""): ea_advbase* +performGeneration(): void</p>
            <fig id="F5.9">
               <caption>
                  <p>Figure 5.9: Class GRASP</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>The GRASP class implements the greedy randomized adaptive search procedure metaheuristic (see Section 3.5 on page 27). It can only use chromosome derviates that additionally implement the gcProvider interface, because the used chromsomes are required to provide a greedy construction heuristic. Analogous to guided local search an additional population is created to be used by the embedded algorithm to circumvent possible side-effects.</p>
         </sec>
         <sec>
            <p>As mentioned, the guided local search metaheuristic requires the definition of features, i.e. a method to identify if a feature is present in a given solution. This class is an abstract base for those problem dependent feature classes, that handle the iden- tification of the features and their penalization. Although only one feature object is  used by the guidedLS class, it is possible to use several different types of features. However, due to this design the, it is very simple to access the features.</p>
            <p>feature #pgroup: string +&lt;&lt;constructor&gt;&gt; feature(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~feature() +penalty(c:chromsome*): double +delta_penalty(c:chromosome*,m:move*): double +updatePenalties(c:chromosome*): void +resetPenalties(): void +tuneLambda(c:chromosome*): double</p>
            <fig id="F5.10">
               <caption>
                  <p>Figure 5.10: Class feature</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>tabuAttribute #pgroup: string +&lt;&lt;constructor&gt;&gt; tabuAttribute(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~tabuAttribute() +equals(o:tabuAttribute&amp;): bool +hashvalue(): unsigned long int</p>
            <fig id="F5.11">
               <caption>
                  <p>Figure 5.11: Class tabuAttribute</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>This abstract class provides an interface for classes whose objects are used as elements of a tabulist. It is important that the methods equals and hashvalue are implemented in a proper way, because they are invoked by an internal hashing array object of the tabulist, which requires this to methods. So tabus which should be considered matching need to return equal hashvalues and true for the equals method. Vice versa tabus which should not be considered as matching should at best return different hashvalues and false for the equals method. If equal hashvalues are return but the tabus are not matching this must be assured by the equals method. The write method is mainly used for debugging purposes during development.</p>
         </sec>
         <sec>
            <p>This class provides the basic tabulist functionality as used by the tabu search metaheuristic. It is implemented using a hashing array to ensure efficient matching of existing tabu attributes. An additional queue is utilized in order to memorize the insertion sequence of the tabu attributes into the tabulist. The hashing array is utilized by the match method to decide if a given tabu attribute matches any existing  tabu attribute. The queue is used to remove older elements from the tabulist as new elements are added by the search process.</p>
            <p>tabulist #size: size_t #tlist: hash_map&lt;tabulist_entry, int&gt; #tqueue: queue&lt;tabuAttribute*&gt; #pgroup: string +&lt;&lt;constructor&gt;&gt; tabulist(N:int,pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; tabulist(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~tabulist() +clear(): void +add(t:tabuAttribute*): void +match(t:tabuAttribute*): bool</p>
            <fig id="F5.12">
               <caption>
                  <p>Figure 5.12: Class tabulist</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>&lt;&lt;interface&gt;&gt; move bitflipMove swapMove xchgMove&lt;T&gt; +r: int +r: int +r: int +s: int +o: T +n: T</p>
            <fig id="F5.13">
               <caption>
                  <p>Figure 5.13: Class move and childs</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>This classes represent moves in the neighbourhood of a chromosome. Their objects are can be used for example within incremental update of the objective value or as a base for tabu attributes.</p>
         </sec>
         <sec>
            <p>This is the main problem specific class which coordinates the interaction of all QAP related classes, i.e. qapInstance , qapFeature and qapTabuAttribute . It is derived directly from chromosome and its main member is a vector containing indicies of facilities, which represents a quaratic assignment. Mutation is performed by swapping two elements of the solution vector. A cycle crossover is implemented, too. Due to efficiency concerns the static data of the ac-  tual instance is stored in a global qapInstance object. To support each implemented algorithm (see Chapter 3 on page 18) the newly introduced interface classes featureProvider , tabuProvider and gcProvider are inherited and their virtual methods are implemented accordingly. In particular the classes qapFeature support guided local search and qapTabuAttribute tabu search. The construction heuristic proposed by Li, Resende and Pardalos [26] implemented, too.</p>
            <p>qapChrom #data: vector&lt;int&gt; +&lt;&lt;constructor&gt;&gt; qapChrom(c:chromosome&amp;) +&lt;&lt;constructor&gt;&gt; qapChrom(pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; qapChrom(t:ea_base*,pg:pstring&amp;="") +copy(orig:chromosome&amp;): void +equals(orig:chromosome&amp;): bool +dist(c:chromosome&amp;): double +initialize(count:int): void +mutate(count:int): void +crossover(parA:chromsome&amp;,parB:chromsome&amp;): void +write(ostr:ostream&amp;,detailed:int=0): void +save(fname:char*): void +load(fname:char*): void +hashvalue(): unsigned long int +delta_obj(m:move&amp;): double +applyMove(m:move&amp;): void +selectImprovement(find_best:bool): void +getFeature(): feature* +greedyConstruct(): void</p>
            <fig id="F5.14">
               <caption>
                  <p>Figure 5.14: Class qapChrom</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>An object of this class contains all necessary data for one particular quadratic assigment instance. It can load the instance data from a file whose filename is supplied as parametere to the constructor; if no filename is specified, the file to which the qapfile parameter is referring is loaded. Auxilliary to the basic storage functionality the presorting stage of the construction heuristic used by qapChrom is done in this class for performance reasons. because the indices only need to be sorted once for an instance.</p>
         </sec>
         <sec>
            <p>This is the specialized feature class for the quadratic assignment problem. The essential idea is that every possible facility-location pair is threated as a feature (see Section 3.4 on page 25). The penalties of these features can be efficiently stored in  a two-dimensional matrix. To maintain the benefits of an incremental update of the objective value the change in penalty can be computed for a given move.</p>
            <p>qapInstance #pgroup: string +int: n +a: vector&lt;int&gt; +b: vector&lt;int&gt; +indexa: vector&lt;pair&lt;int,int&gt;&gt; +indexb: vector&lt;pair&lt;int,int&gt;&gt; +cost: vector&lt;int&gt; +fdind: vector&lt;int&gt; +&lt;&lt;constructor&gt;&gt; qapInstance() +&lt;&lt;constructor&gt;&gt; qapInstance(pg:pstring&amp;="") +&lt;&lt;constructor&gt;&gt; qapInstance(fname:string&amp;,pg:pstring="") +initialize(fname:string&amp;): void +prepare(): void +A(i:int,j:int): int +B(i:int,j:int): int</p>
            <fig id="F5.15">
               <caption>
                  <p>Figure 5.15: Class qapInstance</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>qapFeature #pv: vector&lt;double&gt; +&lt;&lt;constructor&gt;&gt; qapFeature(pg:pstring&amp;="") +&lt;&lt;destructor&gt;&gt; ~qapFeature() +penalty(c:chromsome*): double +delta_penalty(c:chromosome*,m:move*): double +updatePenalties(c:chromosome*): void +resetPenalties(): void +tuneLambda(c:chromosome*): double</p>
            <fig id="F5.16">
               <caption>
                  <p>Figure 5.16: Class qapFeature</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>This tabu attribute is derived from the swapMove class. Two qapTabuAttributes are considered equal if the resulting chromosomes are equal when the moves they represent are applied to them.</p>
         </sec>
         <sec>
            <p>Originally the mechanism that EAlib used to deal with user provided parameter values had some disturbing deficiencies. At first it was not possible to handle multiple values for one parameter, which are for example distinguished by an additional parameter namespace or parameter key. This is especially a problem when one parameter is used in a different context at different places within EAlib, e.g. a guided local  search metaheuristic uses an embedded simulated annealing metaheuristic and the two algorithms should use differently parametrized termination criteria. Secondly only one parameter validator class was implemented, which checks if a numeric value is in a given range. As mentioned, a convenient solution for the first problem should maintain back- wards compatibility. At this point the way how values of parameters are access in EAlib helps very much, because the operator () is used when a parameter value is access. This operator method can be changed so that it fulfills our requirements appropriately. In particular a string parameter is added to the operator () method, which defaults to an empty string representing the already existing global parameter namespace. When a different parameter key is specified the actual value is determined in the following order:</p>
            <p>qapTabuAttribute +&lt;&lt;constructor&gt;&gt; qapTabuAttribute(pg:pstring&amp;="") +&lt;&lt;constrcutor&gt;&gt; qapTabuAttribute(t:qapTabuAttribute&amp;) +&lt;&lt;constructor&gt;&gt; qapTabuAttribute(m:swapMove&amp;) +equals(o:tabuAttribute&amp;): bool +hashvalue(): unsigned long int</p>
            <fig id="F5.17">
               <caption>
                  <p>Figure 5.17: Class qapTabuAttribute</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>1. value associated with parameter key 2. global parameter value 3. default parameter value defined in the sourcecode</p>
            <p>An efficent storage container for these parameter key and value pairs is provided by the hash map class, which is an SGI/GNU extension to the C++ standard template library. The second problem is solved by adding a validator which can perform an unary check such as greater or equal ( ≥ ), greater ( &gt; ), equal (=), less ( &lt; ), less or equal ( ≤ ) or not equal ( =). Also the already existing range checking validator is extendend in way that bounds can be included or excluded from the allowed range.</p>
         </sec>
      </sec>
      <sec>
         <title>5.3 Usage</title>
         <p>In this section we give a summary on how to use EAlib with new optimization problems. As mentioned the problem description has to be incorporated in a new  class which is derived from the chromosome class or one of its already existing derviates depending which ever fits best the actual requirements. However, inheriting class chromosome and implementing its pure virtual methods enables only the basic features. Therefore the new problem class is only useable by those algorithms that raise no specific requirements. In EAlib this is solved by the introduction of interface classes for specific sets of features. The usage this interface classes is not limited to problem classes and hence it is used for algorithm classes, too. This concept has several advantages compared with collecting all methods in the class chromosome .</p>
         <p>• The class chromosome is kept small and manageable, • developers do not need to take care about features they are not interested in, • new features can be integrated without affecting existing sourcecode.</p>
         <p>If a class is providing the functionality of an interface class it has to be derived from it. The dynamic cast operator can be used to determine if a certain class implements an interface. In the remainder of this section the already existings interface classes are described.</p>
         <sec>
            <p>&lt;&lt;interface&gt;&gt; aObjprovider +&lt;&lt;destructor&gt;&gt; ~aObjProvider() +aobj(c:chromosome*): double +delta_aobj(c:chromosome*,m:move*): double</p>
            <fig id="F5.18">
               <caption>
                  <p>Figure 5.18: Interface aObjProvider</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>This interface class is to be inherited by algorithm classes that provide an additional term to the objective value of the chromosomes. The interface class glsSub- Provider is derived from this class an should be used if the implemented algorithm should be used nested into guided local search.</p>
         </sec>
         <sec>
            <p>An algorithm class that provides tabulists has to inherit this interface class, to indicate the incorporated chromosome objects that they should use the tabulists.</p>
            <p>&lt;&lt;interface&gt;&gt; tabulistProvider +&lt;&lt;destructor&gt;&gt; ~tabulistProvider() +isTabu(t:tabuAttribute*): bool +aspiration(c:chromosome*): bool</p>
            <fig id="F5.19">
               <caption>
                  <p>Figure 5.19: Interface tabulistProvider</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>&lt;&lt;interface&gt;&gt; featureProvider +&lt;&lt;destructor&gt;&gt; ~featureProvider() +getFeature(): feature*</p>
            <fig id="F5.20">
               <caption>
                  <p>Figure 5.20: Interface featureProvider</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>Each chromosome class that is to be used in combination with guided local search has to inherit this interface class. An according class derived from class feature has to be realized, too.</p>
         </sec>
         <sec>
            <p>&lt;&lt;interface&gt;&gt; gcProvider +&lt;&lt;destructor&gt;&gt; ~gcProvider() +greedyConstruct()</p>
            <fig id="F5.21">
               <caption>
                  <p>Figure 5.21: Interface gcProvider</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
            <p>If a chromosome class inherits this interface class, this indicates that a greedy construction heuristic is implemented within. This is mandatory if the greedy randomized adaptive search procedure metaheuristic is applied.</p>
         </sec>
         <sec>
            <p>Chromosome classes which are able to deal with tabulists, that means they can fill them with tabu attributes and check if changes to them are currently tabu.</p>
            <p>&lt;&lt;interface&gt;&gt; tabuProvider +&lt;&lt;destructor&gt;&gt; ~tabuProvider()</p>
            <fig id="F5.22">
               <caption>
                  <p>Figure 5.22: Interface tabuProvider</p>
               </caption>
               <graphic xlink:href=""/>
            </fig>
         </sec>
         <sec>
            <p>As mentioned EAlib provides a powerful parameter handling feature. These parameters are used to configure the application. There exist parameters for a variety of domains, e.g.:</p>
            <p>• algorithm configuration, • problem specification, • termination criteria, • logging facility.</p>
            <p>During this master thesis many parameters where added, some where changed in their semantics while others are only used. To illustrate what a user can customize a detailed overview of the parameters affecting this master thesis is given:</p>
            <p>eamod The actual Algorithm to use. Currently the following choices are available: • 0: steady-state EA, • 1: generational EA, • 2: steady-state EA with island model, • 3: generational EA with island model, • 4: simple randomized local search, • 5: simulated annealing, • 6: tabu search, • 7: greedy randomized adaptive search procedure, • 8: guided local search. Default: 0 maxi Should be maximized? True if maximization, false for minimization. Default: 1</p>
            <p>mvnbop Neighbour selection function to use • 0: random neighbour, • 1: next improvement, • 2: best improvement. Default: 0 tgen The number of generations until termination. Default: 100000 tcgen The number of generations for termination according to convergence. Default: 0 tobj The objective value for termination when tcond==2. Default: 0 ttime Specifies the amount of time the algorithm is allowed to run in user-space. Default: 0 glsa Tuning parameter for the influence of penalties in guided local search. Default: 0.5 glsri Interval of generations after which a penalty reset should be performed. If this value is 0 penalty resets will be disabled. Default: 0 sacint Specifies the number of iterations between two successive cooling steps, in other words, the number of iterations for which the simulated annealing process stays at a certain temperature level. Default: 1 satemp Specifies the starting temperature of the simulated annealing process. Default: 1.0 tlsize Specifies the default size of newly created tabu lists. Default: 10 qapfile This parameter specifies from which file the qapInstance object should read the actual instance data. The format of the problem data is the same as used by the QAPLIB [7] instances:</p>
            <p>n A B where n is the size of the instance, and A and B are either flow or distance matrix. Default: “” saca Specifies the slope for the geometric cooling of the simulated annealing process. g ( T, t ) = T ∗ saca, 0 &lt; saca &lt; 1 Default: 0.95 graspa Alpha parameter for GRASP. It is used in the both stages of the QAP construction heuristic and controls the candidate restriction. Default: 0.5 graspb Beta parameter for GRASP. It is used in the first stage of the QAP construction heuristic and controls the candidate restriction. Default: 0.1</p>
            <p>By far the best proof is experience. Sir Francis Bacon</p>
         </sec>
      </sec>
      <sec>
         <title>Chapter 6 Experimental Results</title>
         <p>To be able to compare the results from this different methods many internet available QAP instances are used, the probably most important collection of instances is the QAPLIB [7]. Of special interest are problem instances with a known optimal solution, especially if they are of larger size. To address this requirement some algorithms for construction of such instances where proposed, for example the by Palubeckis [34].</p>
      </sec>
      <sec>
         <title>6.1 Test Cases</title>
         <p>Because the larger quadratic assignment problem instances are computationally intractable, suboptimal algorithms such as the previous mentioned heuristics and metaheuristics are very popular and enjoy wide use. However, when dealing with new methods the quality and other properties such as robustness are important to know before they can be applied in daily business or other critical environments. Usually new algorithms are tested on QAP instances from the QAPLIB (see Burkard, Karisch and Rendl [7]) which is a public internet available collection of well known instances which allows to compare algorithms with each other. However, the problem when using especially larger benchmark problems from QAPLIB is that the optimal solutions are not known in general and one has to rely on lower bounds (see Section 2.3 on page 11). To overcome this problem an other set of test cases can be used. Those are generated by special algorithms whose output are not only the matrices which the define the problem but also with a provable known optimal solution. It has been shown that instances generated by such algorithms are rather hard to solve for some metaheuristics, namely simulated annealing, tabu search and others [34].</p>
      </sec>
      <sec>
         <title>6.2 Test Setup and Procedure</title>
         <p>Our tests were performed on an ordinary desktop computer with GNU Linux in- stalled. The key data of this testing system is listed below:  We compiled EAlib and our test application for the quadratic assignment problem with all documented speed optimizations enabled, i.e. with switch -O4. The test instances are all included in the already mentioned QAPLIB problem library. Each algorithm had to solve each instance 25 times, whereat each run had a time limit of five minutes to complete. The parameter settings for the particular instances were made upon our knowledge which we obtained during preceeding experiments. In the following tables the parameter settings for each algorithm are given which were not at their default value.</p>
         <table-wrap id="Tx421">
            <caption>
               <p>Table 6.1: System setup</p>
            </caption>
            <table>
               <tbody>
                  <tr>
                     <td> CPU</td>
                     <td> Intel Pentium 4 2.8 GHz</td>
                  </tr>
                  <tr>
                     <td> OS</td>
                     <td> Linux 2.4.21</td>
                  </tr>
                  <tr>
                     <td/>
                     <td> GNU Libc 2.3.2</td>
                  </tr>
                  <tr>
                     <td> Compiler</td>
                     <td> GCC 3.3.1</td>
                  </tr>
                  <tr>
                     <td/>
                     <td> binutils 2.14.90.0.5 2.14.90.0.5</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <p>Parameter Value maxi 0 ttime 300 tgen 0 tcgen 5000</p>
         <table-wrap id="T6.2">
            <caption>
               <p>Table 6.2: Parameter settings for Local Search</p>
            </caption>
         </table-wrap>
         <p>During the testruns of local search the tcgen parameter was set so that the search process could terminate if now improvement was made for 5000 iterations, which occurred quite of often. However, this parameter setting did not affect the achieved results significantly.</p>
         <p>Parameter Value maxi 0 ttime 300 tgen 0</p>
         <table-wrap id="T6.3">
            <caption>
               <p>Table 6.3: Parameter settings for Simulated Annealing</p>
            </caption>
         </table-wrap>
         <p>For simulated annealing the parameters controlling the temperature schedule were set to estimated values dependig on the size of the problem instance to solve and the order of magnitude of the corresponding objective value.</p>
         <p>Parameter Value maxi 0 mvnbop 2 ttime 300 tgen 0</p>
         <table-wrap id="T6.4">
            <caption>
               <p>Table 6.4: Parameter settings for Tabu Search</p>
            </caption>
         </table-wrap>
         <p>The parameter tlsize which controls the length of the tabulist was set depending on the actual problem instance. The value chosen was in order of magnitude of the size of instance to solve.</p>
         <p>Parameter Value glsri 5000 maxi 0 ttime 300 tgen 0 sub.eamod 4 sub.ttime 0 sub.tcgen 500</p>
         <table-wrap id="T6.5">
            <caption>
               <p>Table 6.5: Parameter settings for Guided Local Search</p>
            </caption>
         </table-wrap>
         <p>Parameter Value maxi 0 ttime 300 tgen 0 sub.eamod 4 sub.ttime 0 sub.tcgen 500</p>
         <table-wrap id="T6.6">
            <caption>
               <p>Table 6.6: Parameter settings for GRASP</p>
            </caption>
         </table-wrap>
         <p>Additionally the tobj parameter has been set accordingly so that each testrun at which global optimum was found was terminated as soon as this global optimum was reached. This parameter setting did not change the results but sped up the experiments significant.</p>
      </sec>
      <sec>
         <title>6.3 Results</title>
         <p>For the comparison of the individual algorithms which were implemented and inter- pretation of the results four different charaterisic values are used, which in combination give a good insight into the data obtained during the experiments.</p>
         <p>• count of reached optima per problem instance. It is a measure for stability of the search process. For the overall statistics the sum of the particular instances is used, • best objective value per problem instance indicates primarily the potential quality of the search process, • mean objective value is a measure for both quality and stability of an algorithm. However, outliers can have great influence on its value, • deviation of objective value indicates the robustness of the search process.</p>
         <p>Obviously the latter three indicators can not be compared directly among different problem instances. Therefore they are presented in %-gap notation relative to the known global optimal solution. The %-gap of an objective value x relative to the global optimum opt is calculated as follows:</p>
         <p>At first <xref id="XR449" ref-type="table" rid="T6.7">table 6.7</xref> and accordingly figures 6.1 and 6.2 show the overall results of each algorithm for all test instances together, i.e. the sum of the number of reached optimas and the mean %-gap across all testruns. With the obtained results no definitive winning algorithm can be declared. Nevertheless the results show clearly which of the implemented methods are well suited to solve quadratic assignment problems. Looking at the detailed results presented in follwing tables (6.8, 6.9, 6.10, 6.11 and 6.12) show that guided local search and GRASP are indeed clearly outperform- ing the other algorithms. This is not very surprising because both guided local search and GRASP include received considerable more problem dependent knowledge in their implementation than the other metaheuristics implemented during this master thesis. It is also worthy to mention that guided local search and GRASP were the only algorithms which were able to find distinct optimal solutions if the problem instance has more than one. This capability is allegable with the major strengths of these algorithms. GLS gradually moves away from attractive solutions and therefor the embedded random local search is able to reach widespread areas of the search space. GRASP operates somewhat different, its strenghts lies the randomized greed heuristics which, in the optimal case, produces good starting solutions for the embedded local search procedure, which are distributed among the whole search space. An other important feature that <xref id="XR458" ref-type="table" rid="T6.11">table 6.11</xref> and 6.12 show is that the quality of the obtained solutions only decreases somewhat and so these solutions, altough not global optimal, can be adequate, too. Tabu search also achieved good results in terms of the mean %-gap. However, it has reached significant fewer global optima than guided local search or GRASP and the deviation values indicate that the stability of the search process is somewhat deteriorated. A reason for this behavior is the fixed tabulist length which could cause a either a lockout of interesting regions of the search space when the tabulist is too long or the search process gets stuck around a local optimum when the tabulist is too short. The results obtained with the simulated annealing metaheuristc were mixed. For some problem instances, especially instances from the bur collection and smaller instances from the other collections, simulated annealing clearly performs better than tabu search. On the other side some results, e.g. for instances from the chr collection, are not very satisfactory. This indicates that simulated annealing, in its traditional fashion, suffers from a worse stability of the obtained results when applied to the quadratic assignment problem, which could be an effect of the geometric cooling schedule. An improvement like reheating or an occasional perturbation phase might yield better solutions. As expected local search only yields a few global optimal solutions but although no global optimal solution was found for many problem instances especially from the bur set the obtained solutions were nearly optimal. This implies that the chosen neighborhood structure is well suited for solving the quadratic assignment problem which is certainly important for the other metaheuristics, too. However, it is somewhat surprising that only results for the instances from the chr set were very unsatisfactory. The following tables show the results of our tests per algorithm and test instance. In the following figures the results of the algorithms are grouped per test instance to show which instances were tackled best by what algorithms.</p>
         <table-wrap id="Tx451">
            <caption>
               <p>Table 6.7: Overall results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Algorithm #</td>
                     <td> Opt</td>
                     <td> Mean %-gap</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> Local Search</td>
                     <td> 2</td>
                     <td> 9.22</td>
                  </tr>
                  <tr>
                     <td> Simulated Annealing</td>
                     <td> 132</td>
                     <td> 3.47</td>
                  </tr>
                  <tr>
                     <td> Tabu Search</td>
                     <td> 186</td>
                     <td> 0.91</td>
                  </tr>
                  <tr>
                     <td> Guided Local Search</td>
                     <td> 494</td>
                     <td> 0.29</td>
                  </tr>
                  <tr>
                     <td> GRASP</td>
                     <td> 485</td>
                     <td> 0.12</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <fig id="F6.1">
            <caption>
               <p>Figure 6.1: Overall mean %-gap</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <fig id="F6.2">
            <caption>
               <p>Figure 6.2: Overall count of reached global optima</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <table-wrap id="Tx466">
            <caption>
               <p>Table 6.8: Local Search results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> absolute</td>
                     <td> count</td>
                     <td> Best</td>
                     <td> Mean</td>
                     <td> Deviation</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> bur26a</td>
                     <td> 5426670</td>
                     <td> 0</td>
                     <td> 0.13</td>
                     <td> 0.35</td>
                     <td> 0.17</td>
                  </tr>
                  <tr>
                     <td> bur26b</td>
                     <td> 3817852</td>
                     <td> 0</td>
                     <td> 0.20</td>
                     <td> 0.50</td>
                     <td> 0.22</td>
                  </tr>
                  <tr>
                     <td> bur26c</td>
                     <td> 5426795</td>
                     <td> 0</td>
                     <td> 0.00</td>
                     <td> 0.36</td>
                     <td> 0.36</td>
                  </tr>
                  <tr>
                     <td> bur26d</td>
                     <td> 3821225</td>
                     <td> 0</td>
                     <td> 0.02</td>
                     <td> 0.46</td>
                     <td> 0.49</td>
                  </tr>
                  <tr>
                     <td> bur26e</td>
                     <td> 5386879</td>
                     <td> 0</td>
                     <td> 0.01</td>
                     <td> 0.36</td>
                     <td> 0.31</td>
                  </tr>
                  <tr>
                     <td> bur26f</td>
                     <td> 3782044</td>
                     <td> 0</td>
                     <td> 0.02</td>
                     <td> 0.43</td>
                     <td> 0.37</td>
                  </tr>
                  <tr>
                     <td> bur26g</td>
                     <td> 10117172</td>
                     <td> 0</td>
                     <td> 0.02</td>
                     <td> 0.38</td>
                     <td> 0.32</td>
                  </tr>
                  <tr>
                     <td> bur26h</td>
                     <td> 7098658</td>
                     <td> 0</td>
                     <td> 0.02</td>
                     <td> 0.43</td>
                     <td> 0.34</td>
                  </tr>
                  <tr>
                     <td> chr12a</td>
                     <td> 9552</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 44.35</td>
                     <td> 31.81</td>
                  </tr>
                  <tr>
                     <td> chr15a</td>
                     <td> 9896</td>
                     <td> 0</td>
                     <td> 23.54</td>
                     <td> 49.39</td>
                     <td> 22.19</td>
                  </tr>
                  <tr>
                     <td> chr20a</td>
                     <td> 2192</td>
                     <td> 0</td>
                     <td> 18.80</td>
                     <td> 47.13</td>
                     <td> 15.03</td>
                  </tr>
                  <tr>
                     <td> nug12</td>
                     <td> 578</td>
                     <td> 0</td>
                     <td> 2.08</td>
                     <td> 5.81</td>
                     <td> 2.84</td>
                  </tr>
                  <tr>
                     <td> nug14</td>
                     <td> 1014</td>
                     <td> 0</td>
                     <td> 1.18</td>
                     <td> 4.52</td>
                     <td> 1.58</td>
                  </tr>
                  <tr>
                     <td> nug15</td>
                     <td> 1150</td>
                     <td> 0</td>
                     <td> 1.04</td>
                     <td> 4.75</td>
                     <td> 2.10</td>
                  </tr>
                  <tr>
                     <td> nug20</td>
                     <td> 2570</td>
                     <td> 0</td>
                     <td> 2.33</td>
                     <td> 4.65</td>
                     <td> 1.34</td>
                  </tr>
                  <tr>
                     <td> nug25</td>
                     <td> 3744</td>
                     <td> 0</td>
                     <td> 1.12</td>
                     <td> 3.92</td>
                     <td> 1.69</td>
                  </tr>
                  <tr>
                     <td> nug30</td>
                     <td> 6124</td>
                     <td> 0</td>
                     <td> 2.06</td>
                     <td> 4.46</td>
                     <td> 1.07</td>
                  </tr>
                  <tr>
                     <td> tai10a</td>
                     <td> 135028</td>
                     <td> 0</td>
                     <td> 0.59</td>
                     <td> 5.65</td>
                     <td> 3.12</td>
                  </tr>
                  <tr>
                     <td> tai12a</td>
                     <td> 224416</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 8.82</td>
                     <td> 3.37</td>
                  </tr>
                  <tr>
                     <td> tai15a</td>
                     <td> 388214</td>
                     <td> 0</td>
                     <td> 1.86</td>
                     <td> 4.61</td>
                     <td> 1.78</td>
                  </tr>
                  <tr>
                     <td> tai17a</td>
                     <td> 491812</td>
                     <td> 0</td>
                     <td> 2.74</td>
                     <td> 5.89</td>
                     <td> 1.67</td>
                  </tr>
                  <tr>
                     <td> tai20a</td>
                     <td> 703482</td>
                     <td> 0</td>
                     <td> 2.41</td>
                     <td> 5.67</td>
                     <td> 1.62</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <table-wrap id="Tx471">
            <caption>
               <p>Table 6.9: Simulated Annealing results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> absolute</td>
                     <td> count</td>
                     <td> Best</td>
                     <td> Mean</td>
                     <td> Deviation</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> bur26a</td>
                     <td> 5426670</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 0.14</td>
                     <td> 0.05</td>
                  </tr>
                  <tr>
                     <td> bur26b</td>
                     <td> 3817852</td>
                     <td> 7</td>
                     <td> 0.00</td>
                     <td> 0.14</td>
                     <td> 0.09</td>
                  </tr>
                  <tr>
                     <td> bur26c</td>
                     <td> 5426795</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 0.03</td>
                     <td> 0.05</td>
                  </tr>
                  <tr>
                     <td> bur26d</td>
                     <td> 3821225</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 0.03</td>
                     <td> 0.08</td>
                  </tr>
                  <tr>
                     <td> bur26e</td>
                     <td> 5386879</td>
                     <td> 6</td>
                     <td> 0.00</td>
                     <td> 0.01</td>
                     <td> 0.01</td>
                  </tr>
                  <tr>
                     <td> bur26f</td>
                     <td> 3782044</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 0.09</td>
                     <td> 0.14</td>
                  </tr>
                  <tr>
                     <td> bur26g</td>
                     <td> 10117172</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 0.02</td>
                     <td> 0.01</td>
                  </tr>
                  <tr>
                     <td> bur26h</td>
                     <td> 7098658</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 0.06</td>
                     <td> 0.17</td>
                  </tr>
                  <tr>
                     <td> chr12a</td>
                     <td> 9552</td>
                     <td> 18</td>
                     <td> 0.00</td>
                     <td> 4.18</td>
                     <td> 7.74</td>
                  </tr>
                  <tr>
                     <td> chr15a</td>
                     <td> 9896</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 16.68</td>
                     <td> 10.72</td>
                  </tr>
                  <tr>
                     <td> chr20a</td>
                     <td> 2192</td>
                     <td> 0</td>
                     <td> 7.21</td>
                     <td> 33.85</td>
                     <td> 50.55</td>
                  </tr>
                  <tr>
                     <td> nug12</td>
                     <td> 578</td>
                     <td> 23</td>
                     <td> 0.00</td>
                     <td> 0.61</td>
                     <td> 2.77</td>
                  </tr>
                  <tr>
                     <td> nug14</td>
                     <td> 1014</td>
                     <td> 5</td>
                     <td> 0.00</td>
                     <td> 3.66</td>
                     <td> 5.53</td>
                  </tr>
                  <tr>
                     <td> nug15</td>
                     <td> 1150</td>
                     <td> 8</td>
                     <td> 0.00</td>
                     <td> 1.84</td>
                     <td> 5.06</td>
                  </tr>
                  <tr>
                     <td> nug20</td>
                     <td> 2570</td>
                     <td> 3</td>
                     <td> 0.00</td>
                     <td> 2.44</td>
                     <td> 5.05</td>
                  </tr>
                  <tr>
                     <td> nug25</td>
                     <td> 3744</td>
                     <td> 4</td>
                     <td> 0.00</td>
                     <td> 2.93</td>
                     <td> 6.88</td>
                  </tr>
                  <tr>
                     <td> nug30</td>
                     <td> 6124</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 2.58</td>
                     <td> 5.77</td>
                  </tr>
                  <tr>
                     <td> tai10a</td>
                     <td> 135028</td>
                     <td> 23</td>
                     <td> 0.00</td>
                     <td> 0.16</td>
                     <td> 0.56</td>
                  </tr>
                  <tr>
                     <td> tai12a</td>
                     <td> 224416</td>
                     <td> 20</td>
                     <td> 0.00</td>
                     <td> 0.62</td>
                     <td> 1.34</td>
                  </tr>
                  <tr>
                     <td> tai15a</td>
                     <td> 388214</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 1.58</td>
                     <td> 0.93</td>
                  </tr>
                  <tr>
                     <td> tai17a</td>
                     <td> 491812</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 1.75</td>
                     <td> 1.00</td>
                  </tr>
                  <tr>
                     <td> tai20a</td>
                     <td> 703482</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 2.89</td>
                     <td> 2.83</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <table-wrap id="Tx476">
            <caption>
               <p>Table 6.10: Tabu Search results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> absolute</td>
                     <td> count</td>
                     <td> Best</td>
                     <td> Mean</td>
                     <td> Deviation</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> bur26a</td>
                     <td> 5426670</td>
                     <td> 3</td>
                     <td> 0.00</td>
                     <td> 0.19</td>
                     <td> 0.11</td>
                  </tr>
                  <tr>
                     <td> bur26b</td>
                     <td> 3817852</td>
                     <td> 0</td>
                     <td> 0.02</td>
                     <td> 0.37</td>
                     <td> 0.19</td>
                  </tr>
                  <tr>
                     <td> bur26c</td>
                     <td> 5426795</td>
                     <td> 4</td>
                     <td> 0.00</td>
                     <td> 0.22</td>
                     <td> 0.28</td>
                  </tr>
                  <tr>
                     <td> bur26d</td>
                     <td> 3821225</td>
                     <td> 0</td>
                     <td> 0.00</td>
                     <td> 0.31</td>
                     <td> 0.37</td>
                  </tr>
                  <tr>
                     <td> bur26e</td>
                     <td> 5386879</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 0.25</td>
                     <td> 0.27</td>
                  </tr>
                  <tr>
                     <td> bur26f</td>
                     <td> 3782044</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 0.40</td>
                     <td> 0.42</td>
                  </tr>
                  <tr>
                     <td> bur26g</td>
                     <td> 10117172</td>
                     <td> 2</td>
                     <td> 0.00</td>
                     <td> 0.13</td>
                     <td> 0.17</td>
                  </tr>
                  <tr>
                     <td> bur26h</td>
                     <td> 7098658</td>
                     <td> 0</td>
                     <td> 0.00</td>
                     <td> 0.45</td>
                     <td> 0.34</td>
                  </tr>
                  <tr>
                     <td> chr12a</td>
                     <td> 9552</td>
                     <td> 20</td>
                     <td> 0.00</td>
                     <td> 1.18</td>
                     <td> 2.41</td>
                  </tr>
                  <tr>
                     <td> chr15a</td>
                     <td> 9896</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr20a</td>
                     <td> 2192</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 8.85</td>
                     <td> 6.46</td>
                  </tr>
                  <tr>
                     <td> nug12</td>
                     <td> 578</td>
                     <td> 8</td>
                     <td> 0.00</td>
                     <td> 1.36</td>
                     <td> 1.40</td>
                  </tr>
                  <tr>
                     <td> nug14</td>
                     <td> 1014</td>
                     <td> 16</td>
                     <td> 0.00</td>
                     <td> 0.87</td>
                     <td> 1.55</td>
                  </tr>
                  <tr>
                     <td> nug15</td>
                     <td> 1150</td>
                     <td> 7</td>
                     <td> 0.00</td>
                     <td> 1.26</td>
                     <td> 1.56</td>
                  </tr>
                  <tr>
                     <td> nug20</td>
                     <td> 2570</td>
                     <td> 5</td>
                     <td> 0.00</td>
                     <td> 1.16</td>
                     <td> 1.10</td>
                  </tr>
                  <tr>
                     <td> nug25</td>
                     <td> 3744</td>
                     <td> 9</td>
                     <td> 0.00</td>
                     <td> 0.63</td>
                     <td> 1.14</td>
                  </tr>
                  <tr>
                     <td> nug30</td>
                     <td> 6124</td>
                     <td> 4</td>
                     <td> 0.00</td>
                     <td> 0.75</td>
                     <td> 1.09</td>
                  </tr>
                  <tr>
                     <td> tai10a</td>
                     <td> 135028</td>
                     <td> 23</td>
                     <td> 0.00</td>
                     <td> 0.04</td>
                     <td> 0.13</td>
                  </tr>
                  <tr>
                     <td> tai12a</td>
                     <td> 224416</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai15a</td>
                     <td> 388214</td>
                     <td> 19</td>
                     <td> 0.00</td>
                     <td> 0.19</td>
                     <td> 0.87</td>
                  </tr>
                  <tr>
                     <td> tai17a</td>
                     <td> 491812</td>
                     <td> 5</td>
                     <td> 0.00</td>
                     <td> 1.00</td>
                     <td> 0.95</td>
                  </tr>
                  <tr>
                     <td> tai20a</td>
                     <td> 703482</td>
                     <td> 7</td>
                     <td> 0.00</td>
                     <td> 0.52</td>
                     <td> 0.81</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <table-wrap id="Tx481">
            <caption>
               <p>Table 6.11: Guided Local Search results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> absolute</td>
                     <td> count</td>
                     <td> Best</td>
                     <td> Mean</td>
                     <td> Deviation</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> bur26a</td>
                     <td> 5426670</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26b</td>
                     <td> 3817852</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26c</td>
                     <td> 5426795</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26d</td>
                     <td> 3821225</td>
                     <td> 24</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26e</td>
                     <td> 5386879</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26f</td>
                     <td> 3782044</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26g</td>
                     <td> 10117172</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26h</td>
                     <td> 7098658</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr12a</td>
                     <td> 9552</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr15a</td>
                     <td> 9896</td>
                     <td> 10</td>
                     <td> 0.00</td>
                     <td> 0.93</td>
                     <td> 0.85</td>
                  </tr>
                  <tr>
                     <td> chr20a</td>
                     <td> 2192</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 5.22</td>
                     <td> 3.47</td>
                  </tr>
                  <tr>
                     <td> nug12</td>
                     <td> 578</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug14</td>
                     <td> 1014</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug15</td>
                     <td> 1150</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug20</td>
                     <td> 2570</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug25</td>
                     <td> 3744</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug30</td>
                     <td> 6124</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai10a</td>
                     <td> 135028</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai12a</td>
                     <td> 224416</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai15a</td>
                     <td> 388214</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai17a</td>
                     <td> 491812</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai20a</td>
                     <td> 703482</td>
                     <td> 9</td>
                     <td> 0.00</td>
                     <td> 0.26</td>
                     <td> 0.22</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <table-wrap id="Tx486">
            <caption>
               <p>Table 6.12: GRASP results</p>
            </caption>
            <table>
               <thead>
                  <tr>
                     <td> Instance</td>
                     <td> absolute</td>
                     <td> count</td>
                     <td> Best</td>
                     <td> Mean</td>
                     <td> Deviation</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td> bur26a</td>
                     <td> 5426670</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26b</td>
                     <td> 3817852</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26c</td>
                     <td> 5426795</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26d</td>
                     <td> 3821225</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26e</td>
                     <td> 5386879</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26f</td>
                     <td> 3782044</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26g</td>
                     <td> 10117172</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> bur26h</td>
                     <td> 7098658</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr12a</td>
                     <td> 9552</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr15a</td>
                     <td> 9896</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> chr20a</td>
                     <td> 2192</td>
                     <td> 0</td>
                     <td> 0.18</td>
                     <td> 2.02</td>
                     <td> 1.56</td>
                  </tr>
                  <tr>
                     <td> nug12</td>
                     <td> 578</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug14</td>
                     <td> 1014</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug15</td>
                     <td> 1150</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug20</td>
                     <td> 2570</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> nug25</td>
                     <td> 3744</td>
                     <td> 24</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.01</td>
                  </tr>
                  <tr>
                     <td> nug30</td>
                     <td> 6124</td>
                     <td> 1</td>
                     <td> 0.00</td>
                     <td> 0.30</td>
                     <td> 0.16</td>
                  </tr>
                  <tr>
                     <td> tai10a</td>
                     <td> 135028</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai12a</td>
                     <td> 224416</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai15a</td>
                     <td> 388214</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai17a</td>
                     <td> 491812</td>
                     <td> 25</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                     <td> 0.00</td>
                  </tr>
                  <tr>
                     <td> tai20a</td>
                     <td> 703482</td>
                     <td> 10</td>
                     <td> 0.00</td>
                     <td> 0.23</td>
                     <td> 0.21</td>
                  </tr>
               </tbody>
            </table>
         </table-wrap>
         <fig id="F6.3">
            <caption>
               <p>Figure 6.3: Mean %-gap for bur Instances</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <fig id="F6.4">
            <caption>
               <p>Figure 6.4: Mean %-gap for chr instances</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <fig id="F6.5">
            <caption>
               <p>Figure 6.5: Mean %-gap for nug instances</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <fig id="F6.6">
            <caption>
               <p>Figure 6.6: Mean %-gap for tai instances</p>
            </caption>
            <graphic xlink:href=""/>
         </fig>
         <p>A conclusion is the place where you got tired of thinking Steven Wright</p>
      </sec>
      <sec>
         <title>Chapter 7 Conclusions</title>
         <p>After an elaborated introduction of the quadratic assignment problem and the implemented metaheuristics this thesis presented a generic library for metaheuristics and its application to the QAP. The already existing foundations of the EAlib library have been improved to meet the requirements for the new generic metaheuristics. In particular interfaces classes have been introduced that allow fine grained modelling of new classes and support runtime queries for implemented features of specific components. Additionally the parameter handling mechanism has been extended with parameter groups that allow to denote different groups of parameter values for different components in EAlib. With this enhanced EAlib generic versions of the local search, simulated annealing, tabu search, guided local search and greedy randomized adapetive search procedure metaheuristics have been implemented. The latter two algorithms also introduced an efficient way of handling an embedded algorithm. All considered metaheuristics have then been applied to the quadratic assignment problem which showed that only some special parts need to be implemeted separately and most of the problem dependent sourcecode can be shared among all algorithms involved. Of course there are many interesting and useful ideas and task left open for future work.</p>
         <p>• implement more interesting algorithms like antcolony optimization, variable neighborhood search or iterated local search, • add more “standard” features to the implemented algorithms, e.g. dynamic tabulist length, reheating, disturbance methods • itegrate useful template chromosomes, e.g. a generic string chromosome, • provide additional language bindings e.g. for Java and C#.</p>
      </sec>
      <sec>
         <title>List of Algorithms</title>
         <p>1 Basic Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2 Simulated Annealing . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3 Basic Tabu Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4 Tabu Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5 Guided Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 6 Greedy Randomized Adaptive Search Procedure . . . . . . . . . . . . 28 7 GRASP construction phase . . . . . . . . . . . . . . . . . . . . . . . 29</p>
      </sec>
      <sec>
         <title>List of Figures</title>
         <p>2.1 A quadratic assignment example . . . . . . . . . . . . . . . . . . . . . 8 2.2 Original Backboard of the Steinberg Wiring Problem . . . . . . . . . 15 2.3 Conceptual design of a large space antenna (from [33]) . . . . . . . . 16 (a) Antenna configuration . . . . . . . . . . . . . . . . . . . . . . . 16 (b) Finite element model . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1 Escaping a local optimum with GLS . . . . . . . . . . . . . . . . . . 25 5.1 EAlib class overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 5.2 Class chromosome . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.3 Class ea advbase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 5.4 Class lsbase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.5 Class localSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.6 Class simulatedAnnealing . . . . . . . . . . . . . . . . . . . . . . . . 39 5.7 Class tabuSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.8 Class guidedLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.9 Class GRASP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.10 Class feature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.11 Class tabuAttribute . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.12 Class tabulist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.13 Class move and childs . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.14 Class qapChrom . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.15 Class qapInstance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.16 Class qapFeature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.17 Class qapTabuAttribute . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.18 Interface aObjProvider . . . . . . . . . . . . . . . . . . . . . . . . . . 47 5.19 Interface tabulistProvider . . . . . . . . . . . . . . . . . . . . . . . . 48 5.20 Interface featureProvider . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.21 Interface gcProvider . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.22 Interface tabuProvider . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.1 Overall mean %-gap . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 6.2 Overall count of reached global optima . . . . . . . . . . . . . . . . . 57 6.3 Mean %-gap for bur Instances . . . . . . . . . . . . . . . . . . . . . . 63 6.4 Mean %-gap for chr instances . . . . . . . . . . . . . . . . . . . . . . 64 6.5 Mean %-gap for nug instances . . . . . . . . . . . . . . . . . . . . . . 64 6.6 Mean %-gap for tai instances . . . . . . . . . . . . . . . . . . . . . . 65</p>
      </sec>
      <sec>
         <title>List of Tables</title>
         <p>6.1 System setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.2 Parameter settings for Local Search . . . . . . . . . . . . . . . . . . . 53 6.3 Parameter settings for Simulated Annealing . . . . . . . . . . . . . . 53 6.4 Parameter settings for Tabu Search . . . . . . . . . . . . . . . . . . . 54 6.5 Parameter settings for Guided Local Search . . . . . . . . . . . . . . 54 6.6 Parameter settings for GRASP . . . . . . . . . . . . . . . . . . . . . 54 6.7 Overall results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.8 Local Search results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 6.9 Simulated Annealing results . . . . . . . . . . . . . . . . . . . . . . . 59 6.10 Tabu Search results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 6.11 Guided Local Search results . . . . . . . . . . . . . . . . . . . . . . . 61 6.12 GRASP results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62</p>
      </sec>
      <sec>
         <title>Bibliography</title>
         <p>[1] Anstreicher, K. M., and Brixius, N. W. A New Bound for the Quadratic Assignment Problem Based on Convex Quadratic Programming. Mathematical Programming 89 , 3 (2001), 341–357. [2] Anstreicher, K. M., Brixius, N. W., Goux, J.-P., and Linderoth, J. Solving large quadratic assignment problems on computational grids. Mathematical Programming 91 , 3 (February 2002), 563–588. [3] Battiti, R., and Tecchiolli, G. The reactive tabu search. ORSA Journal on Computing 6 , 2 (1994), 126–140. [4] Blum, C., and Roli, A. Metaheuristics in combinatorial optimization: Overview and conceptual comparison. ACM Computing Surveys 35 , 3 (2003), 268–308. [5] Brixius, N. W., and Anstreicher, K. M. Solving quadratic assignment problems using convex quadratic programming relaxations. Optimization Methods and Software 16 (2001), 49–68. [6] Brixius, N. W., and Anstreicher, K. M. The Steinberg Wiring Problem. In The Sharpest Cut: The Impact of Manfred Padberg and His Work , M. Grötschel, Ed. SIAM, June 2004. [7] Burkard, R., Karisch, S., and Rendl, F. QAPLIB - A Quadratic Assignment Problem Library. Journal of Global Optimization 10 (1997), 391–403. <ext-link ext-link-type="uri" href="http://www.seas.upenn.edu/qaplib/.">http://www.seas.upenn.edu/qaplib/.</ext-link> [8] Cerny, V. Thermodynamical Approach to the Traveling Salesman Problem: An Efficient Simulation Algorithm. Journal of Optimization Theory and Applications 45 , 1 (1985), 41–51. [9] Commander, C. W., and Pardalos, P. M. A Survey of the Quadratic Assignment Problem, with Applications. Submitted to The Moreahead Electronic Journal of Applicable Mathematics, April 2003.</p>
         <p>[10] Edwards, C. The derivation of a greedy approximator for the Koopmans– Beckmann quadratic assigment problem. In Proceedings of the 77-th Combinatorial Programming Conference (CP77) (1977), pp. 55–86.  [11] Feo, T. A., and Resende, M. G. C. A probabilistic heuristic for a computationally difficult set covering problem. Operations Research Letters 8 (April 1989), 67–71.</p>
         <p>[12] Feo, T. A., and Resende, M. G. C. Greedy Randomized Adaptive Search Procedures. Journal of Global Optimization 6 , 2 (March 1995), 109–133. [13] Frazer, M. Exact Solution of the Quadratic Assignment Problem, April 1997.</p>
         <p>[14] Garey, M. R., and Johnson, D. S. Computers and Intractability; A Guide to the Theory of NP-Completeness . A series of books in the mathematical sciences. W.H. Freeman and Company, New York, NY, 1979.  [15] Gilmore, P. C. Optimal and Suboptimal Algorithms for the Quadratic Assignment Problem. SIAM Journal on Applied Mathematics 10 (1962), 305–313.</p>
         <p>[16] Glover, F. W. Future paths for integer programming and links to artificial intelligence. Computers and Operations Research 13 , 5 (May 1986), 533–549. [17] Glover, F. W. Tabu Search — Part I. ORSA Journal on Computing 1 , 3 (1989), 190–206. [18] Glover, F. W. Tabu Search — Part II. ORSA Journal on Computing 2 , 1 (1990), 4–32. [19] Glover, F. W., and Kochenberger, G. A. , Eds. Handbook of Metaheuristics , vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003.</p>
         <p>[20] Gutin, G., and Yeo, A. Polynomial approximation algorithms for the TSP and the QAP with a factorial domination number. Discrete Applied Mathematics 119 , 1–2 (June 2002), 107–116.  [21] Henderson, D., and Jacobson, S. H. The Theory and Practice of Simulated Annealing. In Handbook of Metaheuristics , F. W. Glover and G. A. Kochenberger, Eds., vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003, ch. 10, pp. 287–319.</p>
         <p>[22] Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. Optimization by Simulated Annealing. Science 220 , 4598 (May 1983), 671–680. [23] Koopmans, T. C., and Beckmann, M. Assignment Problems and the Location of Economic Activities. Economethica, Journal of the Econometric Society 25 , 1 (January 1957), 53–76. [24] Kuhn, H. W. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly 2 (1955), 83–97. [25] Lawler, E. L. The Quadratic Assignment Problem. Management Science 9 (1963), 586–599.</p>
         <p>[26] Li, Y., Pardalos, P. M., and Resende, M. A Greedy Randomized Adaptive Search Procedure for the Quadratic Assignment Problem. In Quadratic assignment and related problems , P. M. Pardalos and H. Wolkowicz, Eds., vol. 16 of DIMACS Series on Discrete Mathematics and Theoretical Computer Science . American Mathematical Society, 1994, pp. 237–261.</p>
         <p>[27] Loiola, E. M., de Abreu, N. M. M., Boaventura-Netto, P. O., Hahn, P., and Querido, T. An analytical Survey for the Quadratic Assignment Problem, 2004.</p>
         <p>[28] Louren co, H. R., Martin, O. C., and Stützle, T. A Beginner’s Introduction to Iterated Local Search. In Proceedings of MIC’2001—Meta–heuristics International Conference (July 2001), vol. 1, pp. 1–6. Porto, Portugal. [29] Louren co, H. R., Martin, O. C., and Stützle, T. Iterated Local Search. In Handbook of Metaheuristics , F. W. Glover and G. A. Kochenberger, Eds., vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003, ch. 11, pp. 321–353. [30] Mart ́ ı, R. Multi-Start Methods. In Handbook of Metaheuristics , F. W. Glover and G. A. Kochenberger, Eds., vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003, ch. 12, pp. 355–368.</p>
         <p>[31] Metropolis, N., Rosenbluth, A., Rosenbluth, M. N., Teller, A. H., and Teller, E. Equation of State Calculations by Fast Computing Machines. Journal of Chemical Physics 21 , 6 (June 1953), 1088–1092.</p>
         <p>[32] Mills, P., Tsang, E. P. K., and Ford, J. Applying an extended Guided Local Search to the Quadratic Assignment Problem. Annals of Operations Research 118 , 1 (Feburary 2003), 121–135. [33] Padula, S. L., and Kincaid, R. K. Aerospace Applications of Integer and Combinatorial Optimization. NASA Technical Memorandum 110210, NASA, Langley Research Center, Hampton, Virginia 23681-0001, October 1995. [34] Palubeckis, G. An Algorithm for Construction of Test Cases for the Quadratic Assigmnet Problem. Informatica 11 , 3 (2000), 281–296. [35] Raidl, G. EAlib 1.1 – A Generic Library for Metaheuristics. Institute of Computer Graphics and Algorithms, Vienna University of Technology, 2004. [36] Rendl, F., and Sotirov, R. Bounds for the Quadratic Assignment Problem Using the Bundle Method, August 2003. [37] Resende, M. G. C. Greedy Randomized Adaptive Search Procedures. In Handbook of Metaheuristics , F. W. Glover and G. A. Kochenberger, Eds., vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003, ch. 8, pp. 219–249. [38] Resende, M. G. C., and Ribeiro, C. C. Parallel Greedy Randomized Adaptive Search Procedures. Tech. Rep. TD-67EKXH, AT&amp;T Labs Research, December 2004.</p>
         <p>[39] Sahni, S., and Gonzales, T. P-Complete Approximation Problems. Journal of the ACM 23 , 3 (July 1976), 555–565. [40] Steinberg, L. The Backboard Wiring Problem: A Placement Algorithm. SIAM Review 3 , 1 (1961), 37–50.</p>
         <p>[41] Taillard, E. D. Robust taboo search for the quadratic assignment problem. Parallel Computing 17 , 4–5 (July 1991), 443–455.</p>
         <p>[42] Tsang, E. P. K., and Voudouris, C. Fast Local Search and Guided Local Search and Their Application to British Telecom’s Workforce Scheduling Problem. Tech. Rep. CSM-246, Department of Computer Science, University of Essex, Colchester CO4 3SQ, August 1995.  [43] Tsang, E. P. K., and Wang, C. J. A Generic Neural Network Approach for Constraint Satisfaction Problems. In Neural Network Applications , J. G. Taylor, Ed. Springer-Verlag, 1992, pp. 12–22. [44] Voudouris, C., and Tsang, E. P. K. Guided Local Search. Tech. Rep. CSM-247, Department of Computer Science, University of Essex, Colchester, C04 3SQ, UK, August 1995. [45] Voudouris, C., and Tsang, E. P. K. Guided Local Search. In Handbook of Metaheuristics , F. W. Glover and G. A. Kochenberger, Eds., vol. 57 of International series in operations research and management science . Kluwer Academic Publishers, Boston Hardbound, 2003, ch. 7, pp. 185–217.</p>
      </sec>
   </body>
   <back/>
</article>