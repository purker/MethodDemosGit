<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  SYSTEM "http://dtd.nlm.nih.gov/archiving/3.0/archivearticle3.dtd">
<article xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xlink="http://www.w3.org/1999/xlink">
   <front>
      <journal-meta>
         <journal-id/>
         <journal-title-group>
            <journal-title/>
         </journal-title-group>
         <issn/>
         <publisher>
            <publisher-name/>
         </publisher>
      </journal-meta>
      <article-meta>
         <title-group>
            <article-title>Topology in Distributed Computing</article-title>
         </title-group>
         <supplement>
            <p>
               <fig id="Fx1">
                  <caption>
                     <p/>
                  </caption>
                  <graphic xlink:href=""/>
               </fig>
            </p>
            <p>DIPLOMARBEIT zur Erlangung des akademischen Grades Diplom-Ingenieur im Rahmen des Studiums Technische Informatik ausgeführt von Thomas Nowak Matrikelnummer 0425201</p>
            <p>der Fakultät für Informatik der Technischen Universität Wien</p>
            <p>Betreuer: Univ.Prof. Dr. Ulrich Schmid</p>
            <p>18.03.2010 _______________________ ______________________ (Unterschrift Verfasser) (Unterschrift Betreuer) Technische Universität Wien</p>
            <p>Erklärung Thomas Nowak Rechte Wienzeile 73/23 1050 Wien</p>
            <p>Hiermit erkläre ich, dass ich diese Arbeit selbstständig verfasst habe, dass ich verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass die Stellen der Arbeit – einschließlich Tabellen, Karten und Abbildungen –, anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe.</p>
            <p>Wien, 18.03.2010</p>
            <p>______________________ (Unterschrift)</p>
            <p>Abstract Topology is the general mathematical theory of convergence. Distributed computing is the formal investigation of communicating concurrent processes. We explore applications of topology to distributed computing in two directions: (1) Point-set topology and (2) algebraic topology. We use the former to study the topological structure of infinite execution trees. This enables us to unify a number of impossibility proofs, in particular, the impossibility of distributed consensus — the task of all processes in a system agreeing on a single value — in various (close to) asynchronous systems with crash failures. The latter is used to look into the combinatorial structure of configurations , i.e., the collection of current process states in the system. Configurations are regarded as simplices in a simplicial complex, and topological incompatibility of such complexes is utilized to prove the impossibility of a generalization of distributed consensus in certain systems. The particular problem considered is k -set agreement , which is the task of letting all processes agree to values within a set of at most k elements.</p>
            <p>Kurzfassung Topologie ist die mathematisch adäquate Art, um über Konvergenz zu sprechen. Distributed Computing ist das formale Studium von verteilten Systemen. Die Arbeit beschäftigt sich mit zwei Anwendungen der Topologie im Bereich des Distributed Computing: (1) Mengentheoretische Topologie und (2) algebraische Topologie. Erstere wird verwendet, um die topologische Struktur von unendlichen Bäumen, die die Information über mögliche Ausführungen der Algorithmen sub- sumieren, zu untersuchen. Dieses Wissen wird verwendet, um einen einheitlichen Beweis der Unmöglichkeit von Distributed Consensus in mehreren Systemmo- dellen zu geben. Consensus ist das Einigen aller Prozesse des Systems auf einen einzigen Wert. Zweitere wird verwendet, um die kombinatorische Struktur von Konfiguratio- nen , also der Zusammenfassung aller lokaler Zustände der Prozesse, zu untersuchen. Hierbei wird eine Konfiguration als Simplex in einem Simplizialkomplex aufgefasst. Die topologische Unvereinbarkeit solcher Komplexe ermöglicht einen Beweis der Unmöglichkeit von k -Set Agreement in gewissen Systemen. Das ist eine Verallgemeinerung des Consensus-Problems: Es wird nicht mehr verlangt, dass sich die Prozesse auf nur einen Wert einigen, sondern es wird erlaubt, dass bis zu k unterschiedliche Werte auftreten.</p>
         </supplement>
      </article-meta>
   </front>
   <body>
      <sec>
         <title>Contents</title>
      </sec>
      <sec>
         <title>1. Introduction</title>
      </sec>
      <sec>
         <title>1</title>
         <p>1.1. Distributed Computing . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2. Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3. Structure of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.4. A Word on Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2</p>
      </sec>
      <sec>
         <title>2. Distributed Computing Models</title>
      </sec>
      <sec>
         <title>4</title>
         <p>2.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2. Asynchronous Message Passing à la FLP . . . . . . . . . . . . . . . . . 5 2.2.1. A Formal Description . . . . . . . . . . . . . . . . . . . . . . . 5 2.3. Omission Failure Model . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3.1. A Formal Description . . . . . . . . . . . . . . . . . . . . . . . 7 2.4. Asynchronous Shared Memory . . . . . . . . . . . . . . . . . . . . . . 8 2.4.1. A Formal Description . . . . . . . . . . . . . . . . . . . . . . . 8 2.4.2. Atomic Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.5. Safety and Liveness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9</p>
      </sec>
      <sec>
         <title>3. Problem Specifications</title>
      </sec>
      <sec>
         <title>11</title>
         <p>3.1. Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2. k -Set Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12</p>
      </sec>
      <sec>
         <title>4. Point-Set Topology</title>
      </sec>
      <sec>
         <title>13</title>
         <p>4.1. The Topology of Execution Spaces . . . . . . . . . . . . . . . . . . . . 13 4.1.1. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.1.2. Execution Trees . . . . . . . . . . . . . . . . . . . . . . . . . . 19 4.1.3. Path–Sequence Duality . . . . . . . . . . . . . . . . . . . . . . 21 4.2. Topological Impossibility . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.2.1. Additional Structure — Configuration Similarity . . . . . . . . 23 4.3. Impossibility Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3.1. Asynchronous Message Passing . . . . . . . . . . . . . . . . . . 25 4.3.2. Asynchronous Shared Memory . . . . . . . . . . . . . . . . . . 26 4.3.3. Transient Message Loss . . . . . . . . . . . . . . . . . . . . . . 27</p>
      </sec>
      <sec>
         <title>5. Algebraic Topology</title>
      </sec>
      <sec>
         <title>28</title>
         <p>5.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2. Homology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2.1. Chain Complexes . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2.2. The Homology Functor . . . . . . . . . . . . . . . . . . . . . . 29 5.3. Simplicial Complexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30</p>
         <p>5.3.1. Simplicial Homology . . . . . . . . . . . . . . . . . . . . . . . . 30 5.4. Algebraic vs. Combinatorial Topology . . . . . . . . . . . . . . . . . . 32 5.4.1. Singular Homology . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.4.2. Geometric Realization of Simplicial Complexes . . . . . . . . . 33 5.4.3. Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.5. Configuration Complexes . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.5.1. Input Complexes . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.5.2. Output Complexes . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.5.3. Protocol Complexes . . . . . . . . . . . . . . . . . . . . . . . . 36 5.6. Impossibility of k -Set Agreement . . . . . . . . . . . . . . . . . . . . . 36 5.6.1. Full Information Protocols . . . . . . . . . . . . . . . . . . . . . 37 5.6.2. Properties of Full Information Protocols . . . . . . . . . . . . . 37 5.6.3. This Implies Impossibility . . . . . . . . . . . . . . . . . . . . . 38</p>
      </sec>
      <sec>
         <title>6. Summary A. Topological Prerequisites</title>
      </sec>
      <sec>
         <title>39 40</title>
         <p>A.1. Motivation and Examples . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.1.1. Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.1.2. Compactness in R n . . . . . . . . . . . . . . . . . . . . . . . . . 46 A.2. Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 A.2.1. Open Sets and Neighborhoods . . . . . . . . . . . . . . . . . . 48 A.2.2. Closure, Interior, Boundary, Density . . . . . . . . . . . . . . . 52 A.2.3. Continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 A.2.4. Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 A.2.5. Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 60</p>
      </sec>
      <sec>
         <title>Bibliography</title>
      </sec>
      <sec>
         <title>61</title>
      </sec>
      <sec>
         <title>1. Introduction</title>
         <p>This thesis deals with applications of topology to distributed computing. These are twofold: Firstly, we use point-set topology to provide a unifying topological framework for consensus impossibility proofs. Secondly, we present the impossibility proof of k -set agreement by Herlihy and Shavit (1993) which uses algebraic topology.</p>
         <sec>
            <title>1.1. Distributed Computing</title>
            <p>Consider a system of N processes that communicate by means of passing messages. All processes take steps simultaneously at times t = 0 , 1 , 2 , . . . and in zero time. All message delays are equal to 1 / 2, i.e., processes at time t +1 have received all messages sent in computing steps at time t . Processes are modeled as state machines and run a local algorithm which governs state transitions and message sendings. Interesting questions to ask might include:</p>
            <p>(1) How many steps does it take until the last process has terminated? (2) How many messages are sent in the execution of the algorithm? (3) Is the algorithm correct, i.e., does it indeed fulfill its task specification?</p>
            <p>The investigation of such questions is the realm of distributed computing . We can spice things up a bit by varying model parameters. For example, we may allow more general message delays than fixing them all at exactly 1 / 2. Likewise, we might choose not to fix the times at which processes take steps to exactly 0 , 1 , 2 , . . . Of course, also the restriction that all processes take steps simultaneously might seem overly limiting. We may also introduce the possibility of lost messages: In the completely synchronous model with message delays equal to 1 / 2, suppose that in every time frame [ t, t + 1), up to N − 1 message may be lost. That is, these messages do not get delivered although all other messages are delivered timely. A surprising result (Santoro and Widmayer 1989) is that even in such a system with relatively few faults (there exist up to N 2 − N point-to-point links; at most N − 1 are lossy each round) it is impossible for any deterministic algorithm to solve consensus . Consensus is the task of all processes in the system agreeing on a single value.</p>
         </sec>
         <sec>
            <title>1.2. Topology</title>
            <p>Topology is the general mathematical theory of convergence. Its most popular special case is the study of metric spaces. It tackles questions like:</p>
            <p>(1) Does the image of a continuous function f : [0 , 1] → R have a maximum? (2) Does every Cauchy sequence converge? (3) How many holes does a given manifold have? (4) Can you cut two pizzas in half with a single cut, no matter how ugly they are shaped?</p>
            <p>The immediate investigation of topological spaces is called point-set topology , which questions (1) and (2) can be attributed to. Another common technique is to assign algebraic structures to topological spaces, reason about relations between these structures and map these insights back into the world of topological spaces. This method is called algebraic topology .</p>
         </sec>
         <sec>
            <title>1.3. Structure of the Thesis</title>
            <p>Chapter 2 introduces distributed computing as a discipline and presents formal system models. In Chapter 3, we talk about an important problem specification in distributed computing: k -set agreement and its important special case, consensus. Chapter 4 investigates execution spaces of distributed algorithms by means of point- set topology and provides a unified proof of the impossibility of consensus in some important system models. Chapter 5 deals with methods from algebraic topology. It explains the approach taken by Herlihy and Shavit (1993) to prove the impossibility of k -set agreement in the presence of up to k crash failures. A summary of the thesis is given in Chapter 6. Appendix A gives a self-contained introduction to point-set topology.</p>
         </sec>
         <sec>
            <title>1.4. A Word on Notation</title>
            <p>The purpose of this section is to introduce some conventions of the mathematical notation used in the thesis. We denote the set of real numbers by R and the set of non-negative real numbers by R + . Real intervals are written with round and square parentheses, e.g., [0 , 1) = { x ∈ R | 0 x &lt; 1 } . For a set X ⊂ R , inf X denotes the infimum of X and sup X denotes its supremum. The letter Z denotes the set of integers. We set N = { k ∈ Z | k 1 } and ω = { k ∈ Z | k 0 } .  For arbitrary sets A and B , we write B A = { f : A → B } to denote the set of all functions with domain A and range B . For a mapping f : A → B and subsets A ⊂ A and B ⊂ B , we define f [ A ] = { f ( x ) | x ∈ A } and f − 1 [ B ] = { x ∈ A | f ( x ) ∈ B } . The predicate A ⊂ B means ∀ x ( x ∈ A ⇒ x ∈ B ) and we set P ( A ) = { A ⊂ A } . If M is a set of sets, then M denotes the set { x | ∃ A ∈ M : x ∈ A } . If ( X, d ) is a metric space, x ∈ X and ε &gt; 0, then we write</p>
            <p>B ε ( x ) = { y ∈ X | d ( x, y ) &lt; ε } . (1.1) If X is a topological space and S ⊂ X , then S denotes the closure of S in X . Additional notation will be defined when necessary.</p>
         </sec>
      </sec>
      <sec>
         <title>2. Distributed Computing Models</title>
         <p>This chapter introduces the field of distributed computing to the extent needed to present the results in subsequent chapters. We start by examining some questions that are tackled and then introduce a number of mathematical models that are used in distributed computing.</p>
         <sec>
            <title>2.1. Introduction</title>
            <p>Distributed computing (Attiya and Welch 2004, Lynch 1996) is the investigation of concurrent processes that communicate by means of some communication medium. Commonly, processes are modeled as deterministic state machines taking steps (performing state transitions) in zero time. Examples of communication media include point-to-point RS232 links, a common data bus or a shared memory area allocated by the Linux kernel. These types of communicating differ in a number of properties: While changes to a shared memory are potentially visible immediately, messages which were sent at time t may arrive at time t + δ with δ &gt; 0, i.e., the message sent at time t is not immediately visible to the receiver. The transmission delay on a bus might be equal for all processes, while transmission delays on point-to-point links might be different for different links (processes). In message-passing systems, a fundamental distinction is whether message delays are bounded or not. Message delays are bounded if there is a constant ∆ such that every message sent at time t is guaranteed to have arrived at time t + ∆. Systems that lack this property are called message asynchronous . Other important properties of communicating distributed systems are process synchrony properties. The most process synchronous system imaginable might be a system in which all processes run at exactly the same speed, i.e., steps of processes are triggered by perfectly synchronous hardware clocks. A most process asynchronous system is one in which no information whatsoever is available on when processes take a step (perform a state transition). Of course, systems with more synchrony allow for harder problems to be solved than systems with weaker synchrony. It is, for example, impossible to do any kind of real-time clock synchronization in completely asynchronous systems. A major problem, however, is to determine whether one system is “more synchronous” than some other system (e.g., Dolev, Dwork, and Stockmeyer 1987) and for many pairs of systems, none is more synchronous than the other.  Things get even more complicated when components may fail, in particular, in asynchronous systems where no upper bound on message delays or inter-step times of processes exist. The seminal work of Fischer, Lynch, and Paterson (1985) shows that it is not possible in such systems for processes to even agree on a single value (i.e., consensus is not possible). The following sections introduce a number of popular models for distributed systems.</p>
         </sec>
         <sec>
            <title>2.2. Asynchronous Message Passing à la FLP</title>
            <p>Consider a system of N concurrent processes that communicate by means of point-to- point links, i.e., every process can send messages to any other process. Asynchronous message-passing deserves its name because</p>
            <p>(1) there is no upper bound on the transmission delay of messages and (2) there is no upper bound on the inter-step time of processes, i.e., there is no Φ such that a process that took a step at time t is guaranteed to have taken its next step by time t + Φ.</p>
            <p>The assumption coverage of this model, i.e., its ability to accurately describe real systems, is quite broad since it does not limit the timing behavior in any way. An algorithm in the asynchronous message-passing system model consists of a state machine for each of the N processes. State changes occur when a process takes a step and the transition function depends on the current internal state and received messages. Apart from the internal state transition, a process may also send messages to other processes. The structure of such a computing step is depicted in <xref id="XR76" ref-type="fig" rid="F2">Figure 2.1</xref>.</p>
            <p>receive msg state transition send msgs <xref id="XR78" ref-type="fig" rid="F2.1">Figure 2.1</xref>.: Structure of a single computing step</p>
            <p>Major problems arise when processes are allowed to crash, i.e., cease to take subsequent steps. The asynchronous nature of the system model prohibits distinguishing processes which have crashed from processes whose messages are very slow.</p>
            <sec>
               <title>2.2.1. A Formal Description</title>
               <p>In the asynchronous message-passing system model (Fischer, Lynch, and Paterson 1985), a system consists of N processes numbered 1 , 2 , . . . , N which possess an internal state , a transition function δ and a sending function β . The transition function  δ maps pairs ( s, m ), consisting of the local state and a received message, to some internal state. A message is a pair ( p, m ) where p is a process name and m is a message content drawn from a pool of possible message contents M or the void value ⊥ . The sending function β maps pairs ( s, m ) as above to a finite set of (sent) messages. Every process has a distinguished subset of its set of states — the set of initial states . A configuration is a tuple ( s 1 , s 2 , . . . , s N ) of internal states of the processes, together with the set of in-transit messages — the message buffer . An important point to understand is the relationship between the two notions of event and step , which we will define now. An event in the classical asynchronous message-passing model is a message. If in some configuration C , the message buffer holds the message (event) e = ( p, m ), then we say that event e is applicable to configuration C and we may apply e to C by the following means: We define the successor configuration C = e ( C ) by the following procedure.</p>
               <p>(1) Remove e = ( p, m ) from the message buffer. (2) Determine the internal successor state of process p by invoking the transition function δ using message content m and p ’s current state. (3) Determine, and add to the message buffer, the messages ( q, n ) sent by process p to processes q by invoking the sending function β .</p>
               <p>The pair ( C, C ) where C = e ( C ) for some event e is called a step . If we can apply event e to configuration C , we say that e is applicable to C . An event has the ability to trigger different steps, depending upon the configuration it is applied to. An infinite sequence of events that are in turn applicable to C is called a schedule starting from C . If ( e 1 , e 2 , . . . ) is a schedule starting from configuration C , we define the corresponding sequence of steps as follows: We set C 0 = C and C k +1 = e k ( C k ) for k 0; the corresponding sequence of steps is then defined to be ( C 0 , C 1 ) , ( C 1 , C 2 ) , ( C 2 , C 3 ) , . . . . Such a corresponding sequence of steps is called a run or an execution . A process is called non-faulty or correct in some run or schedule if it takes steps infinitely often. A process is called faulty if it is not non-faulty. A run or schedule is called admissible (with respect to the model parameter f 0) if every message sent to non-faulty processes is received and at most f processes are faulty.</p>
            </sec>
         </sec>
         <sec>
            <title>2.3. Omission Failure Model</title>
            <p>In the synchronous message-passing model (Lynch 1996, Part I), all processes p 1 , p 2 , . . . , p N take their steps at the same time, e.g., every process takes a step at times t = 0 , 1 , 2 , 3 , . . . Furthermore, there does exist an upper bound on message delays, namely every message sent at time t is delivered before time t + 1. That is, processes execute in lock-step rounds: Every process is guaranteed to have received  all messages that were sent to it before the current computing step. <xref id="XR94" ref-type="fig" rid="F2.2">Figure 2.2</xref> contains a space-time diagram of a synchronous execution; the diagonal arrows indicate messages.</p>
            <p>t =0 t =1 t =2 t =3 p 1 p 2 p 3 <xref id="XR96" ref-type="fig" rid="F2.2">Figure 2.2</xref>.: Synchronous message-passing</p>
            <p>In the synchronous omission failure model (Santoro and Widmayer 1989, Section 4.1), in every round, i.e., in every time interval [ t, t + 1), up to N − 1 messages may be lost. These omissions create difficulties and yield a number of impossibility results, because the adversary can completely silence a process by omitting all of its outgoing messages.</p>
            <sec>
               <title>2.3.1. A Formal Description</title>
               <p>In the synchronous omission failure system model, a system consists of N processes numbered 1 , 2 , . . . , N which possess an internal state , a transition function δ and a sending function β . The transition function δ maps pairs ( s, M ), consisting of the local state and a set of (received) messages, to some internal state. A message is a pair ( p, m ) where p is a process and m is the message content taken from a set M of possible message contents. The sending function β maps an internal state s to a set M of messages such that every other process occurs in the first component of an element in M . Every process has a distinguished subset of its set of states — the set of initial states . A configuration is a tuple ( s 1 , s 2 , . . . , s N ) of internal states of the processes. 1 An event in the model is a set O ⊂ { 1 , . . . , N } 2 \ { (1 , 1) , (2 , 2) , . . . , ( N, N ) } with | O | N − 1, the set of omissions . We define the successor configuration C = O ( C ) by the following procedure.</p>
               <p>(1) Determine the sent messages of all processes by invoking the sending functions β . 1 Contrary to the asynchronous message passing model, it is no longer necessary to remember the state of the medium in a configuration. This is because every message is received in the same step in which it was sent. See below for the exact step semantics.</p>
               <p>(2) Ignore all messages over links in O . (3) Determine the internal successor state of all processes by invoking the transition functions δ using the newly received (non-ignored) messages.</p>
               <p>The pair ( C, C ) where C = O ( C ) for some event O is called a step . An infinite sequence of events is called a schedule . If ( O 1 , O 2 , . . . ) is a schedule and C is a configuration, we define the corresponding sequence of steps as follows: We set C 0 = C and C k +1 = O k ( C k ) for k 0; the corresponding sequence of steps is then defined to be ( C 0 , C 1 ) , ( C 1 , C 2 ) , ( C 2 , C 3 ) , . . . . Such a corresponding sequence of steps is called a run or an execution . A process is called non-faulty or correct in some run or schedule if infinitely many message sent by it get delivered. A process is called faulty if it is not non-faulty.</p>
            </sec>
         </sec>
         <sec>
            <title>2.4. Asynchronous Shared Memory</title>
            <p>In this section, we consider a system of N processes communicating by means of M shared read-write registers . These registers can hold an unbounded amount of information and support two types of operations: read and write . Operation read ( R ) returns the value of register R and operation write ( R, v ) writes value v to register R . The fundamental limitation in this model is that processes can perform only one of the operations read and write in a single computing step. Hence a process performing a write does not know which value it overwrites. As in asynchronous message-passing, there is no upper bound on inter-step times of processes. 2 Also, the possibility of processes crashing introduces difficulties. A sometimes convenient simplification is to limit registers to be single-writer registers. That is, a shared register has a single process assigned to it which is the only process that may write to the register. It is known (Attiya and Welch 2004, Theorem 10.9) that this is not a serious restriction.</p>
            <sec>
               <title>2.4.1. A Formal Description</title>
               <p>In the asynchronous shared memory system model (Attiya and Welch 2004, Section 4.1), a system consists of (a) N processes numbered 1 , 2 , . . . , N which possess an internal state , a transition function δ and a shared memory operation function β and (b) M shared read-write registers which possess a value . The shared memory operation function β maps an internal state s to a shared memory operation, i.e., read ( R ) or write ( R, v ). The transition function δ maps pairs ( s, v ), consisting of the local state and the return value of the shared memory operation β ( s ), to some internal state. Every process has a distinguished subset of its set of states — the set 2 However, there is no delay between performing a write and the time the written value becomes visible to other processes. Thus, message delay δ = 0 in the language of message passing models.  of initial states . A configuration is a tuple ( s 1 , s 2 , . . . , s N ) of internal states of the processes, together with a tuple ( v 1 , v 2 , . . . , v M ) of shared memory register values. An event in the asynchronous shared memory model is a process number j ∈ { 1 , 2 , . . . , N } . We define the successor configuration C = j ( C ) by the following procedure.</p>
               <p>(1) Determine the next shared memory operation by process p j by invoking the shared memory operation function β . (2) Perform the shared memory operation by process p j , i.e., change the register value in case of a write operation. (3) Determine the internal successor state of process p j by invoking the transition function δ using the return value from (2).</p>
               <p>The pair ( C, C ) where C = j ( C ) for some event j is called a step . An infinite sequence of events that are in turn applicable to C is called a schedule starting from C . If ( j 1 , j 2 , . . . ) is a schedule starting from configuration C , we define the corresponding sequence of steps as follows: We set C 0 = C and C k +1 = j k ( C k ) for k 0; the corresponding sequence of steps is then defined to be ( C 0 , C 1 ) , ( C 1 , C 2 ) , ( C 2 , C 3 ) , . . . . Such a corresponding sequence of steps is called a run or an execution . A process is called non-faulty or correct in some infinite run or schedule if it takes steps infinitely often. A process is called faulty if it is not non-faulty. A run or schedule is called admissible (with respect to the model parameter f 0) if at most f processes are faulty. In case of single-writer registers, the set of allowed operations that may occur as the image of functions β is restricted such that there do not exist two processes p i and p j with operation functions β i and β j that can perform writes to a common register R .</p>
            </sec>
            <sec>
               <title>2.4.2. Atomic Snapshots</title>
               <p>A system with shared read-write registers supports atomic snapshots if there exists, besides read and write , a third operation, namely scan () which returns all register values at once, i.e., a tuple ( v 1 , v 2 , . . . , v M ) of register values.</p>
            </sec>
         </sec>
         <sec>
            <title>2.5. Safety and Liveness</title>
            <p>The notions of safety and liveness properties were introduced by Lamport (1977) and have been well adopted in the distributed computing community. Lamport used these notions to subdivide correctness proofs of programs into smaller and more homogeneous pieces.  Intuitively, a safety property is the statement that “something will not happen” (Lamport 1977). For instance, take the sentence “No message is ever sent.” The “thing” that should not happen according to this statement is that a message is sent. At any time in an execution, if already a message was sent, there is no way that the execution fulfills the above safety property, no matter how the execution continues. Hence if a “bad thing” happened in an execution prefix, any execution that extends this prefix does not fulfill the safety property. A liveness property is the statement that “something must happen” (Lamport 1977). An example would be the sentence “Every message that was sent is eventually received.” The important point is that at any time in an execution, even if not all sent messages were received yet, it is still possible that the execution fulfills the above liveness property (because the message can be received later). Hence for any finite execution prefix, there exists an execution extending this prefix that fulfills the liveness property. The immediate formalization of these two notions is contained in the following definition. Definition 2.1. Let S A be the set of admissible executions of some algorithm A . A property of executions is a subset P ⊂ S A . We call a property P a safety property if the following holds: For all E ∈ S A \ P exists some n ∈ N such that every E ∈ S A that coincides with E in the first n components holds E ∈ P . We call a property P a liveness property if the following holds: For all E ∈ S A and every n ∈ N there exists an E ∈ P that coincides with E in the first n components. It should be noted that the intuitive meaning of these notions is sometimes in conflict with Definition 2.1, in particular in the presence of failures. An investigation of this problem and alternative definitions were given by Charron-Bost, Toueg, and Basu (2000).</p>
         </sec>
      </sec>
      <sec>
         <title>3. Problem Specifications</title>
         <p>In this chapter, we discuss two prominent problems in distributed computing: the consensus problem and the k -set agreement problem which is a generalization of consensus. By the term “problem” we mean a specification on the behavior of an algorithm, which is said to “solve a problem” if all its executions satisfy the specification. The reason why we introduce exactly these two problems is firstly their fundamentality and secondly that we will prove impossibility of their solution in specific system models in later chapters.</p>
         <sec>
            <title>3.1. Consensus</title>
            <p>Informally, consensus is the task of getting all processes in a distributed system to agree on a single value. It is known (e.g., Fischer, Lynch, and Paterson 1985, Dolev, Dwork, and Stockmeyer 1987, Fich and Ruppert 2003) that consensus, as easy as the problem specification might seem, is in fact impossible to solve in a variety of system models in the presence of faults. Every process starts its execution with a prescribed input value and decides upon termination on an output value. We will consider consensus only in system models with model parameter f 1. Otherwise, consensus is trivially solvable. One simplification 1 that we make is that the set of possible input and output values is equal to { 0 , 1 } . This special case of consensus is called binary consensus. Formally, input and output values are modeled in the following way: First, we impose the restriction that every process has to have at least two distinct initial states. For every process p j , let S j denote its set of states and I j ⊂ S j its set of initial states. We demand | I j | 2. Input values are modeled by a mapping ι j : I j → { 0 , 1 } which we demand to be non-trivial. Output values are modeled by a mapping δ j : S j → { 0 , 1 , ⊥} . We say that process p j has decided on v ∈ { 0 , 1 } in state s ∈ S j if δ j ( s ) = v . We demand that decisions are irrevocable , i.e., if s is part of some configuration C , δ j ( s ) ∈ { 0 , 1 } , and configuration C follows C in some execution, then δ j ( s ) = δ j ( s ) where s is p j ’s state in C . Hence, we may extend δ j to execution of the algorithm. Of course, even with f crash failures, agreement on a value can be achieved trivially by programming every process to decide on 0. Hence, we limit our attention to non- trivial consensus. We say that an algorithm solves consensus if: 1 In reality, this does not make the problem any simpler, just the notation. And since we are doing impossibility results, it suffices to limit ourselves to this special case.</p>
            <p>(T) For every admissible execution holds: Every process that is correct 2 decides on some value. (Termination)</p>
            <p>(A) For every admissible execution holds: No two correct processes decide on differing values. (Agreement)  (V) For every admissible execution holds: If the execution starts from an initial configuration in which all input values are equal to v , then all correct processes decide on v . (Validity)</p>
         </sec>
         <sec>
            <title>3.2. k -Set Agreement</title>
            <p>Consensus is 1-set agreement. In k -set agreement with k ∈ N , we expand the set of possible input (and output) values to { 1 , 2 , . . . , M } with M N and replace condition (A) with</p>
            <p>k -A) For every admissible execution holds: No k + 1 correct processes decide to pairwise differing values. ( k -Agreement)</p>
            <p>Definition 3.1. Let S be the set of admissible executions of a k -set agreement (or consensus) algorithm and let C be a configuration in S . We say that C is α -valent if all successor configurations of C if which a decision was reached have the decision value α . In this case, we call C univalent , otherwise multivalent , or in the case M = 2 bivalent .</p>
            <p>2 See descriptions in 2.2.1, 2.3.1 and 2.4.1 for details when a process is considered correct .</p>
         </sec>
      </sec>
      <sec>
         <title>4. Point-Set Topology</title>
         <p>In this chapter, we treat techniques from elementary point-set topology (see Appendix A for an introduction to the subject) with respect to their applicability to distributed computing. We show how to equip execution spaces with a natural topology that can be used to derive impossibility results. In particular, we re-prove FLP impossibility (Fischer, Lynch, and Paterson 1985) in this novel topological framework.</p>
         <sec>
            <title>4.1. The Topology of Execution Spaces</title>
            <p>This section introduces the necessary tools for formulating the main result of this thesis in Section 4.2. We show how to equip a space of executions of some distributed algorithm with a certain topology that helps us express executional properties in a topological manner. But before we talk about execution spaces , we have to fix the term “execution” and explain what we mean by it. 1 Common to all models of distributed computing is the notion of a configuration , meaning a snapshot of the state of a system. That is, a configuration encompasses information about the internal state of every process and the state of the communication medium (e.g., messages in transit or contents of shared memory). An execution is a sequence of configurations such that each configuration in the sequence is a successor of the former ones. Notice that the meaning of these two notions is heavily model-dependent. From the topological viewpoint, we are not interested in the ontological question of what a configuration really is ; we only need to know which successor configurations are possible. Thus, we “shift focus from the structure of protocols for a distributed system to the structure of the set of possible schedules of a distributed system.” (Saks and Zaharoglou 2000) We denote by C A the set of all configurations of algorithm A . Let S A denote the set of admissible executions, which is a subset of the set C A ω of sequences of configurations. 2 We will equip the latter space with a natural topology that induces a topology on its subset S A . When there is no danger of ambiguity, we will write C and S for C A and S A , respectively.</p>
            <p>1 The formal definition of these terms were given in Chapter 2. 2 Executions, in models that we consider, are infinite per definitionem .</p>
            <p>We endow C with the discrete topology, i.e., every subset of C is defined to be open. This topology is induced by the metric d D : C × C → R + , d D ( C, C ) = 0 if C = C (4.1) 1 else. The natural topology to endow C ω = n ∈ ω C with is the product topology (see Section A.2.5). Lemma 4.1. The product topology on C ω is induced by the metric d ( C k ) , ( C k ) = 2 − inf { j | C j = C j } (4.2)</p>
            <p>Proof. We have to show that the sets that are open with respect to the product topology (Definition A.14) are exactly those sets that are open with respect to the metric (Example A.3). Let A ⊂ C ω be open with respect to the metric d . The definition of openness with respect to the metric asserts existence of ε ( γ ) &gt; 0 for every γ = ( C k ) ∈ A such that</p>
            <p>= ( γ )</p>
            <p>(4.3)</p>
            <p>A B ε ( γ ) . γ ∈A</p>
            <p>From this equation we derive that it suffices to show that B ε ( γ ) is open with respect to the product topology whenever ε &gt; 0. In this case, choose the integer K minimal with the property 2 − K ε . This choice implies  B ε ( γ ) = B 2 − K ( γ ) = { γ | γ and γ agree in the first K components } . (4.4) If π m : C ω → C denotes the projection onto the m th component, then the inverse image π m − 1 { C m } of the open set { C m } ⊂ C is exactly the set of elements in C ω whose m th component is equal to C m . Also, by definition, these inverse images are open with respect to the product topology. We thus conclude on the openness of B ε ( γ ) with respect to the product topology, because the latter set in (4.4) is equal to K π m − 1 { C m } . (4.5) m =0 To prove the converse direction, it suffices to show that all sets of the form π m − 1 [ O ] where O is a subset 3 of C are open with respect to the metric d . But we may write</p>
            <p>π m − 1 [ O ] = B 2 − m ( γ ) (4.6) γ ∈ π m − 1 [ O ] because both sides are equal to the set of elements in C ω whose m th element is in O . The openness of B 2 − m ( γ ) with respect to the metric d now concludes the proof. 3 Note that all sets O ⊂ C are open, because we equipped C with the discrete topology (see Example A.4(2)).</p>
            <p>Finally, we endow S ⊂ C ω with the subset topology (Example A.9), that is, the topology induced by the same metric. This topology on execution spaces was introduced by Alpern and Schneider (1985). They characterized safety and liveness properties of executions in a topological way. Namely, a property is a safety property if and only if the set of executions satisfying it is closed (Definition A.8) with respect to the previously defined topology. Similarly, a property is a liveness property if and only if the set of executions satisfying it is dense (Definition A.11) with respect to this topology. They used this characterization to prove that any property is an intersection of a safety and a liveness property. Also, in non-pathological cases, every property is an intersection of two liveness properties. We will now retrace these insights. In the following, let C be the set of configurations of some algorithm. Lemma 4.2. A property P ⊂ C ω is a safety property if and only if P is closed in C ω .  Proof. It is equivalent to prove that P is a safety property if and only if its complement P c is open. Let P be a safety property. Then, by definition, for every E ∈ P c , there exists some index k such that for every E ∈ C ω that agrees with E in the first k components, we have E ∈ P c . By setting ε = 2 − k , we arrive at the insight that B ε ( E ) ⊂ P c and we are done. Conversely, let P c be an open set. Let E ∈ P c . We have to show that there exists some index k such that for any E ∈ C ω that coincides with E in the first k components, we have E ∈ P c . The set P c being open, there exists some ε &gt; 0 such that B ε ( E ) ⊂ P c . Let k ∈ N such that 2 − k &lt; ε . Then the set of all E that coincide with E in the first k components is a subset of B ε ( E ), which concludes the proof. Lemma 4.3. A property P ⊂ C ω is a liveness property if and only if P is dense in C ω . Proof. Let P be a liveness property and let E ∈ C ω and ε &gt; 0. We will show that there exists some E ∈ P such that d ( E, E ) &lt; ε . Let k ∈ N such that 2 − k &lt; ε . By definition of liveness, there exists an extension E of the execution fragment formed by taking the first k components of E such that E ∈ P . But then d ( E, E ) &lt; 2 − k &lt; ε and we are done. Conversely, let P be a dense set. Let ( C 0 , C 1 , . . . , C k ) be an execution fragment. We have to show that there exists some extension E ∈ P of the fragment. Let E be any extension of the fragment. Because P is dense, there exists E ∈ P such that d ( E, E ) &lt; 2 − k − 1 , which shows that E is also an extension of ( C 0 , C 1 , . . . , C k ).</p>
            <p>The following result was proved by Alpern and Schneider (1985). Theorem 4.1. Let P ⊂ C ω be any property. Then there exists a safety property S and a liveness property L such that P = S ∩ L .</p>
            <p>Proof. Define S = P to be the topological closure of P and L = S c ∪ P . Then of course S ∩ L = P . It is also clear that S is closed, hence a safety property by Lemma 4.2. It remains to show that L is dense (Lemma 4.3). By Lemma A.4(4), we have L = P c ∪ P ⊃ P c ∪ P = C ω (4.7)</p>
            <p>which concludes the proof.</p>
            <p>Notice that the above notions of closedness and density, but also of safety and liveness, are dependent on the surrounding space C ω . We used these notions in a “global” sense, meaning that they were understood with respect to the whole space C ω . When considering a specific algorithm running in a specific model, however, we may mean something different by “safety” and “liveness”. For example, consider a consensus algorithm A with set S ⊂ C ω of admissible executions. Then C 0 , the set of initial configurations of A , is not equal to C , because there exist configurations in which processes have decided, but no process has decided in an initial configuration. It follows that S is not dense in C ω , since every E ∈ S starts with an initial configuration. Hence S , as a property in C ω , is not a liveness property. But S itself is of course a liveness (and even a safety) property in S . It follows that the notions of safety and liveness are relative notions. Of course, statements analog to that of Lemma 4.2 and Lemma 4.3 hold relative to some set S of executions.</p>
            <sec>
               <title>4.1.1. Motivation</title>
               <p>In this subsection, we will demonstrate topological proof techniques for impossibility results with the help of a simple example. Consider a consensus algorithm A for N 3 processes communicating by means of single-reader multiple-writer shared read-write registers. 4 Of these processes, f = N − 1 might fail by crashing. According to Section 2.4, a configuration C in this model consists of a tuple ( s 1 , s 2 , . . . , s N ) and a tuple ( v 1 , v 2 , . . . , v M ) where s i is the internal state of process p i and v i is the content of a shared memory register. An event in this system model is a process number j ∈ { 1 , 2 , . . . , N } , which expresses that process p j takes a step. An admissible schedule is a sequence of process numbers in which at least one process occurs infinitely often (recall that f = N − 1). Hence every sequence in { 1 , 2 , . . . , N } N is an admissible schedule. We work with two sequence spaces: the space of schedules and the space of executions. In this example, the set of schedules is equal to Σ = { 1 , . . . , N } N and the set S of admissible executions of some algorithm is a subset of C ω where C denotes the set of all configurations. We equip both sets with the topology discussed at the beginning of Section 4.1. 4 This is very similar to the model of Dolev, Dwork, and Stockmeyer 1987, Theorem I1.1 and a special case of the system model in Section 2.4.</p>
               <p>Lemma 4.4. Let S be the set of admissible executions of some consensus algorithm A . Define the map ∆ : S → { 0 , 1 } such that ∆( E ) is the decision value of algorithm A in execution E . Then ∆ is continuous.</p>
               <p>Proof. It suffices to show that ∆ is locally constant, i.e., for all E ∈ S , there exists some neighborhood N of E (Definition A.6) such that ∆ is constant on N , that is, ∆( E ) = ∆( E ) for all E ∈ N . Let E ∈ S be some admissible execution of A . By the termination property of consensus, there exists some configuration C in E such that some process has already decided. Let k be an index such that the k th configuration in E is equal to C . We claim that</p>
               <p>N = E ∈ S | E coincides with E up to the k th configuration (4.8)</p>
               <p>is the desired neighborhood. It is clear that ∆ is constant on N , because by the agreement condition of consensus, no other consensus decision value is possible after a process has decided. It remains to show that N is indeed a neighborhood of E . Define ε = 2 − k . With the metric defined in (4.2), we conclude that N is the set of admissible executions that have distance to E less than ε . Thus N is an ε -ball, hence open.  Let S denote the set of admissible executions. When we fix some initial configuration I , by the semantics of the model, every admissible schedule determines exactly one admissible execution. This induces a mapping f I : Σ → S , details of which are depicted in Figure 4.1.</p>
               <p>initial config. C 0 ; schedule ( j 1 , j 2 , j 3 , j 4 , . . . ) f j 1 j 2 j 3 j 4 j 5 C 0 C 1 C 2 C 3 C 4 ··· Figure 4.1.: Mapping from schedules to executions If C 0 denotes the set of initial configurations, then I → f I is a map with domain C 0 . Hence f may be viewed 5 as a function C 0 × Σ → S . Lemma 4.5. If we equip C 0 with the discrete topology, then f : C 0 × Σ → S as defined above is continuous. Proof. Let ( I, σ ) be an element of C 0 × Σ and let ( I k , σ k ) k be a sequence converging to ( I, σ ). Then I k → I and σ k → σ . We will show f ( I k , σ k ) → f ( I, σ ). 5 by uncurrying</p>
               <p>Since C 0 carries the discrete topology, convergence of ( I k ) means that it is eventually constant (and equal to I ). Hence it is no loss of generality to assume I k = I for all k . It remains to show that f I ( σ k ) → f I ( σ ). Let ε &gt; 0. Choose n ∈ N such that 2 − n &lt; ε . Because the n th component of ( σ k ) must be eventually constant for every n , there exists some K ∈ N such that the first n components of σ k agree for all k K . But then, by construction of f I ( σ ), also the first n components of f I ( σ k ) agree for all k K . Hence d f I ( σ ) , f I ( σ k ) &lt; 2 − n &lt; ε for all k K (4.9) and we are done.  By definition of the system model, f : C 0 × Σ → S is surjective. But by Tychonoff’s theorem (Theorem A.3), Σ and also C 0 × Σ are compact. Hence S is a continuous image of a compact space, which implies that S itself is compact (Lemma A.10). Since C ω is metrizable, hence Hausdorff, we conclude that S is closed in C ω (Lemma A.9). Thus, S is a safety property in C ω (Lemma 4.2). The established continuity (both ∆ and f I are continuous) of ∆ ◦ f I : Σ → { 0 , 1 } for I ∈ C 0 has the following consequence: The sets Σ α , α ∈ { 0 , 1 } , of schedules σ for which ∆( f I ( σ )) = α , i.e., where the algorithm decides on α in execution f I ( σ ), are closed in Σ, because they are inverse images of the closed sets { α } ⊂ { 0 , 1 } under a continuous mapping. Being closed in a compact space, the sets Σ α are compact (Lemma A.9). But since each of these two sets is the complement of the other in Σ, they are also open. The following lemma now establishes a uniform bound K such that every execution starting from I is univalent after the K th step. By passing to the maximum over all I ∈ C 0 , we get a uniform bound K not restricted to a particular initial configuration. In other words, C K is univalent for every ( C 0 , C 1 , . . . , C K , . . . ) ∈ S . Lemma 4.6 (Lebesgue) . Let ( X, d ) be a compact metric space and let ( U λ ) λ ∈ Λ be an open covering of X , i.e., every U λ is open and X = U λ . Then there exists some ε &gt; 0 such that for any x ∈ X , the ball B ε ( x ) is contained in one of the U λ .</p>
               <p>Proof. For every x ∈ X let δ x &gt; 0 such that B δ x ( x ) ⊂ U λ for some λ . The family of balls B δ x / 2 ( x ) indexed by x ∈ X forms an open covering of X . By compactness (Definition A.13), there exist x 1 , x 2 , . . . , x m ∈ X such that m X = B δ j / 2 ( x j ) . (4.10) j =1 where δ j = δ x j . We set δ = min { δ 1 , δ 2 , . . . , δ m } and ε = δ/ 2. Let x ∈ X , then x ∈ B δ j / 2 ( x j ) for some j . Hence d ( x, x j ) &lt; δ j / 2. Let now y ∈ B ε ( x ), then d ( x, y ) &lt; ε δ j / 2. The triangle inequality implies d ( y, x j ) &lt; δ j , hence B ε ( x ) ⊂ B δ j / 2 ( x ) ⊂ B δ j ( x j ) ⊂ U λ (4.11)</p>
               <p>for some λ and we are done. Corollary 4.1. There exists a K ∈ N such that for every execution ( C 0 , C 1 , . . . ) ∈ S , the K th configuration, C K , is univalent.</p>
               <p>On to the non-topological part of the impossibility proof: Together with the existence of a bivalent initial configuration, 6 we have established the existence of a fork, i.e., there exists a bivalent configuration C and direct successor configurations D 0 and D 1 of C such that D α is α -valent. We show that such a fork is impossible. 7 Then, we have proved consensus impossibility. Let p be the process taking the step C → D 0 and let q be the process taking the step C → D 1 . The processes p and q are distinct. Case 1: Both p and q perform read operations. Since f = N − 1 2, we can choose a third process r and apply the schedule ( r, r, . . . ) to both D 0 and D 1 . The resulting decision value (the value that r decides on) is in both cases the same, because the local state of r and all register values that r can read are the same in D 0 and D 1 . But since D 0 is 0-valent and D 1 is 1-valent, this is a contradiction. Case 2: p performs a read and q performs a write operation. The same trick as in Case 1 works. Choose a process other than p and other than the reader of the register that q writes to. Case 3: Both p and q perform write operations. Silence the readers of both registers that get written to by p and q .</p>
            </sec>
            <sec>
               <title>4.1.2. Execution Trees</title>
               <p>Executions are sequences of configurations. At any point (configuration) in such a sequence, it is often possible to choose from more than one successor configuration, as governed by the system model and the algorithm. One can, in a natural way, assign a decision tree to any set of executions that captures the decision of choosing a successor. We will characterize in Section 4.1.3 certain sets of executions whose decision trees capture all the information about the original set. This follows an idea by Lubitch and Moran (1995). Let C be the set of configurations of an algorithm A and let S ⊂ C ω . We will construct a tree T ( S ) that reflects the local decisions of choosing a successor configuration. We construct it inductively, of course. First of all, we insert a root ⊥ . We then connect to it nodes labeled with every configuration C 0 that occurs as an initial configuration in S , see Figure 4.2. These are exactly the vertices at depth 1. Suppose now that we already constructed the tree up to depth n . We describe how to construct the vertices at depth n + 1. Let C n − 1 be a vertex at depth n . There exists a unique path ( ⊥ , C 0 , C 1 , . . . , C n − 1 ) from ⊥ to C n − 1 . We connect to C n − 1 6 Existence of a bivalent initial configuration is established by a bit-flipping argument. This standard proof technique can be examined in Attiya and Welch 2004, Lemma 5.16. 7 cf. Dolev, Dwork, and Stockmeyer 1987, Lemma I1.1.1</p>
               <p>nodes labeled with every configuration C n such that ( C 0 , C 1 , . . . , C n − 1 , C n ) occurs as a prefix of some execution in S . ) 4 ,. .. ) 1 ,. .. 4 , C 2 1 ,C 2 ( ( C 4 0 ,C 1 ( C 0 5 ,C 5 ( C 1 ,C 0 1 C 0 , C 2 2 1 ,C 5 1 , 2 ,. . . ) C 2 ,.. 2 . ) ( C 0 3 , C 1 3 , C 2 3 , . . . ) S T ( S ) ⊥ C 0 1 C 0 2 C 0 3 C 0 4 C 0 5 Figure 4.2.: Constructed tree up to depth 1</p>
               <p>Note that this procedure indeed does result in a tree since no single vertex is connected twice to a predecessor node. In particular, vertices are not configurations, but only labeled by configurations. Hence it is indeed possible that there exist two different nodes in T ( S ) that are labeled with the same configuration. The labels of nodes at depth n are exactly the configurations that occur in the ( n − 1)th component of executions in S . Also, nodes in this tree may have infinite degree. But, as we will see later, for any specific distributed system model and algorithm running on it, the tree of the set of admissible executions will be locally finite, i.e., nodes will have finite degree. By construction, every execution in S corresponds to an infinite path in the tree T ( S ). The converse, however, is not true in general. There exist sets S and infinite paths in T ( S ) that do not correspond to an execution in S . For example, consider a shared memory algorithm for N 3 processes and up to f = N − 2 crash faults. The corresponding set S contains all sequences of process numbers with the property that at least two processes occur infinitely often. In the tree T ( S ) every node has exactly N children — one for every process. Hence the infinite path (1 , 1 , . . . ) exists in T ( S ), but this sequence is not an element of S . In the following section, we will explore for which sets S every path in the assigned tree describes an execution is S .</p>
            </sec>
            <sec>
               <title>4.1.3. Path–Sequence Duality</title>
               <p>For a tree T ( S ) as constructed in 4.1.2, let P T ( S ) denote the set of executions in C ω that correspond to infinite paths in T ( S ). The relation P T ( S ) ⊃ S always holds. We are interested in those S for which P T ( S ) = S . These are the sets of executions for which it suffices to make local decisions when constructing an admissible execution. For all sets that do not have this property, we have to filter out some executions constructed by virtue of paths in order to arrive at the set of admissible executions. The following theorem solves the above question. Theorem 4.2. Let S ⊂ C ω . The following are equivalent:</p>
               <p>(1) P T ( S ) = S (2) S is closed in C ω In this case, S is compact if and only if T ( S ) is locally finite.</p>
               <p>Proof. (1) ⇒ (2): Let E = ( C 0 , C 1 , . . . ) ∈ S = P T ( S ) . Then either C 0 is not an initial configuration in S or there exists some n ∈ N such that C n is not a child of C n − 1 in T ( S ). In the first case, no execution that starts with C 0 is an element of S , i.e., B 1 ( E ) ⊂ C ω \ S . In the second case, by definition of T ( S ), we have B 2 − n ( E ) ⊂ C ω \ S . We conclude that C ω \ S is open, hence S is closed. (2) ⇒ (1): Let E = ( C 0 , C 1 , . . . ) ∈ P T ( S ) . We show E ∈ S . By definition of T ( S ), for any n ∈ N there exists some execution E n ∈ S that starts with ( C 0 , C 1 , . . . , C n ). Since E n → E as n → ∞ and since S is closed, the claim follows from Lemma A.3. Now, let T ( S ) be locally finite, i.e., every node has finite degree. Then, for every n ∈ ω , there are only finitely many configurations possible to appear as the n th component of an execution in S . Denote the set of possible configurations in the n th component by C n . Then, S ⊂ n ∈ ω C n . The latter set being compact (Theorem A.3 and Example A.10), the compactness of S follows from Lemma A.9(1). Finally, let S be compact. Let v be any vertex in T ( S ). We will show that v has only finitely many children. Suppose that v = ⊥ is the root and ⊥ has infinitely many children, i.e., C 0 is infinite. Consider the following open covering of S :</p>
               <p>B 1 ( C 0 , C 1 , . . . ) = E ∈ S | E starts with C 0 where C 0 ∈ C 0 (4.12)</p>
               <p>By compactness of S , there exists a finite subcovering, but this is impossible. The case v = ⊥ is completely analogous.  The advantage of closed sets of executions is now obvious: We can restrict ourselves to local decisions when constructing an execution in the set. So, if we are considering some closed subset of the set of admissible executions, we are guaranteed admissibility of the execution constructed in a local fashion.</p>
            </sec>
         </sec>
         <sec>
            <title>4.2. Topological Impossibility</title>
            <p>This section contains the core of our topological impossibility proofs. We begin with the topological main theorem: Theorem 4.3. Let ( X, d ) be a metric space, A ⊂ X closed in X and C ⊂ X compact. If A ∩ C = ∅ , then</p>
            <p>d ( A, C ) = inf { d ( a, c ) | a ∈ A, c ∈ C } &gt; 0 . (4.13)</p>
            <p>Proof. Suppose not, i.e., d ( A, C ) = 0. Define the map f A : C → [0 , ∞ ) by f A ( x ) = inf { d ( x, a ) | a ∈ A } . If we are able to show that f A is continuous, then we are done. Because then the continuous function f A attains its minimum in the compact set C (Lemma A.11), i.e., there exists some c ∈ C with f A ( c ) = 0. But since c is an element of the open set X \ A , there exists some ε &gt; 0 such that B ε ( c ) ⊂ X \ A . This implies f A ( c ) = inf { d ( c, a ) | a ∈ A } ε &gt; 0, a contradiction. It remains to show that f is continuous. Let ε &gt; 0 and c ∈ C . We choose δ = ε . For c ∈ C with d ( c, c ) &lt; ε , we have</p>
            <p>f A ( c ) − f A ( c ) d ( c, c ) + f A ( c ) − f A ( c ) &lt; ε (4.14)</p>
            <p>where we assumed without loss of generality that f A ( c ) f A ( c ) and used that f A ( c ) d ( c, c ) + f A ( c ) by the triangle inequality and that taking the infimum preserves weak inequalities.  If we are able to find a compact (hence closed, Lemma A.9(2)) set K ⊂ S for which d ( K 0 , K 1 ) = 0 where K α denotes the set of α -deciding executions, we are done. We then know that the K α are compact, because the decision function ∆ is continuous. But this is a direct contradiction to Theorem 4.3. We will, however, use a slightly different argument to derive a contradiction. Most often, we will not directly reason with sequences of configurations. What we will rather do is follow an idea which was introduced by Lubitch and Moran (1995) and generalized by Moses and Rajsbaum (2002): We use schedulers to construct executions. Definition 4.1. Let S be the set of admissible executions of some algorithm. A scheduler for S is a metric space X together with a continuous map f : X → S . A scheduler is called closed if X is compact. We will use schedulers to describe the construction of executions in a (closed) subset of S (namely the image of f ). An example of a scheduler is the above mapping f of Section 4.1.1. We prominently used the fact that f and in particular that ∆ ◦ f was a continuous mapping. The general result about transportation of properties in Theorem 4.3 is the following lemma. Lemma 4.7. Let f : X → S be a closed scheduler. The following assertions are true:</p>
            <p>(1) If A ⊂ X is closed then f [ A ] ⊂ S is closed and compact. (2) d ( A, B ) = 0 implies d ( f [ A ] , f [ B ]) = 0 for all A, B ⊂ X .</p>
            <p>Proof. (1): Every closed set A ⊂ X is compact since X is compact (Lemma A.9(1)). Since f is continuous, we may deduce that f [ A ] is compact (Lemma A.10). But S is Hausdorff and hence every compact set is closed by Lemma A.9(2); in particular f [ A ]. (2): Let d ( A, B ) = 0. If we set ε k = 2 − k we get by the uniform continuity of f (similar to Theorem A.2) the existence of δ k &gt; 0 such that d ( x, y ) &lt; δ k ⇒ d ( f ( x ) , f ( y )) &lt; 2 − k (4.15) By hypothesis there exist sequences ( a k ) in A and ( b k ) in B such that d ( a k , b k ) &lt; δ k for all k ∈ N . The implied relation d ( f ( a k ) , f ( b k )) → 0 as k → ∞ now concludes the proof.  Any closed scheduler we will construct in the subsequent will, like S , itself be a sequence space, i.e., X ⊂ L N for some set L . Hence a closed scheduler X can be viewed as the set of paths in the locally finite tree T ( X ); see Theorem 4.2. Often, the set L will be a set of layers (Moses and Rajsbaum 2002), i.e., each ∈ L will correspond to a finite sequence of events. For example, a layer in the shared memory model will be every process taking one step in some fixed order. The fact that each layer is a fixed finite sequence of events will immediately establish continuity of f . Hence the continuity of ∆ ◦ f where ∆ is the decision function will establish the compactness of both X 0 and X 1 where X α is the set of schedules in which the algorithm decides on α . It remains to show d ( X 0 , X 1 ) = 0 to complete the impossibility proof by virtue of Theorem 4.3 — see Section 4.2.1. In a way, a scheduler f : X → S defines a sub-model of S . This is particular apparent if we consider schedulers that consist of sequences of layers: In S , it is defined which configurations may follow which. The layering limits these possibilities and takes “shortcuts” from one configuration to another. Thus, the tree T ( X ) can be seen as a sub-tree of T ( S ) in some sense with the additional convenient property that P T ( X ) = X (if we are considering a closed scheduler).</p>
            <sec>
               <title>4.2.1. Additional Structure — Configuration Similarity</title>
               <p>Up to now, we solely considered a single structural entity regarding the set of executions: In which order configurations may occur in an execution. But there is more information to configurations than their order. To be precise, we now introduce two similarity relations on the set of configurations; one model-dependent (process similarity) and one model-independent (valence similarity). Definition 4.2. Let C and C be configurations. We write C ∼ v C if it is not the case that one is 0-valent and the other is 1-valent. Then C and C are called valence similar .  Definition 4.3. Let C and C be configurations of a message-passing algorithm, i.e., an algorithm in either of the models of Sections 2.2 or 2.3. We write C ∼ p C and call C and C process similar if C and C differ in the state of at most one process. We denote the transitive closure of this relation by the same name and symbol. Definition 4.4. Let C and C be configurations of a single-writer shared-memory algorithm (Section 2.4). We write C ∼ p C and call C and C process similar if C and C differ in the state and registers 8 of at most one process. We denote the transitive closure of this relation by the same name and symbol. The following lemmata provide a relation between these two similarity notions and an argument why d ( X 0 , X 1 ) &gt; 0, i.e., a 0-1-fork, is often not possible in closed sets. Lemma 4.8. Consider either a message-passing or a single-writer shared memory model with at most f 1 crash faults. Let S be a closed set of admissible executions that</p>
               <p>(1) has the possibility to silence a process, i.e., from every configuration C and for every process p exists an infinite path in T ( S ) starting from C in which p does not take steps. (2) is locally uniform , i.e., if a sequence of events that does not involve process q is applicable to a configuration C in S , then it is applicable to all configurations in S that differ from C at most in the state (and registers) of q . Let C and C be configurations in S . If there exist successors D and D of C and C respectively such that D ∼ p D , then C ∼ v C</p>
               <p>Proof. Suppose not, i.e., without loss of generality C is 0-valent and C is 1-valent. Since C is 0-valent, so is its successor D . By the same token, D is 1-valent. The relation D ∼ p D implies the existence of processes q 1 , q 2 , . . . , q t and configurations D 0 , D 1 , . . . , D t such that D 0 = D , D t = D , and configurations D j − 1 and D j differ exactly in the state of process q j . By transitivity of valence similarity, it suffices to show that if D and D differ only in the state of a single process q , then D ∼ v D . Because we assumed that we can silence process q , there exists a sequence of events starting from both D and D that does not include steps taken by q . By the event semantics of the particular models, the processes other than q decide on the same values in both executions, hence the valency of D and D is the same. This is a contradiction.</p>
               <p>Lemma 4.9. Let C be a bivalent configuration. Then not all successor configurations of C are valency similar. 8 We say that register R belongs to process p if p is the sole writer of R .</p>
            </sec>
         </sec>
         <sec>
            <title>4.3. Impossibility Results</title>
            <p>We will list a number of impossibility proofs in this section that utilize our topological framework. In particular, we give a closed scheduler for each of the models. We will use arguments from Moses and Rajsbaum 2002, Sections 3, 7 and 8.</p>
            <sec>
               <title>4.3.1. Asynchronous Message Passing</title>
               <p>We now present a topological proof of the consensus impossibility result (Fischer, Lynch, and Paterson 1985) in the model introduced in Section 2.2. We fix some enumeration p 1 , p 2 , . . . , p N of the processes. The scheduler we will use is the following. We choose  L π = { aok π , except π } ∪ { delayed π ( i ) | 1 i N } (4.16) for any permutation 9 π ∈ S N , L = π ∈ S N L π as the set of layers, and X = L N . The mapping f : X → S is defined to be the application of a sequence of layers to these initial configurations. More precisely, every layer in L is defined to be a finite sequence of events (see below for the exact definition) and the application of a sequence ( 1 , 2 , 3 , . . . ) of layers to an initial configuration C 0 is defined by concatenating all layers to a single schedule σ and taking the corresponding sequence of steps. It remains to describe the layers aok π , except π and delayed π ( i ). In layer aok π , every process takes steps in the order governed by the permutation π , i.e., in the order p π (1) , p π (2) , . . . , p π ( N ) , each process receives all messages sent to it thus far. Layer except π is the same as aok π except that process p π ( N ) does not take steps. In layer delayed π ( i ), processes take steps in the order of π and all messages are received except for messages sent from p π ( i ) to p π ( i )+1 mod N in this very layer. (They are likely to be received in the next layer — namely if and only if the recipient takes a step in the next layer.) We equip the finite set L with the discrete topology and X = L N with the product topology. Finite sets are always compact and by Tychonoff’s theorem (Theorem A.3), also X is compact. The mapping f is continuous, because the n th component of f ( x ) only depends on the first n components of x ∈ X . 10 Hence we are dealing with a closed scheduler. Let C be a configuration. For any layer ∈ L , let C · denote the configuration that arises when applying to C . We want to show that the precondition of Lemma 4.8 is fulfilled. The set f [ X ] of executions is closed and has the ability to silence and is locally uniform. Let D = C · and D = C · be children of C in the tree T ( X ). We want to show that D ∼ v D with Lemma 4.8. In a first step, we restrict ourselves to the case , ∈ L π for some π ∈ S N . This sub-claim will follow if we 9 We denote the set of all permutations of { 1 , 2 , . . . , N } by S N . 10 cf. Lemma 4.5 show C · ∼ v C · aok π for all ∈ L π . The non-trivial cases are = except π and = delayed π ( i ). In the first case, we note that C · except π · aok ( π ◦ σ ) = C · aok π · except π where σ is the permutation with σ ( j ) ≡ j − 1 mod N . This is because in both cases the order in which processes take steps is equal to</p>
               <p>π (1) , . . . , π ( N − 1) , π ( N ) , π (1) , . . . , π ( N − 1) (4.17)</p>
               <p>with the same sequence of events (receive all messages). The second case is = delayed π ( i ). There, we have C · delayed π ( i ) ∼ p C · aok π because these two configurations only differ in the state of a single process (the process to which not all messages were delivered — namely π ( i ) + 1 mod N ). This concludes the proof of the precondition of Lemma 4.8 if we restrict our choices to a single L π . Let now π, σ ∈ S N . We want to show C · aok π ∼ v C · aok σ . For this, write π − 1 ◦ σ = τ 1 ◦ τ 2 ◦ · · · ◦ τ k where every τ j is a transposition , i.e., every τ j flips the positions of two neighboring elements of { 1 , 2 , . . . , N } . 11 An elementary result of group theory is the possibility to write a permutation as a product of transpositions. We see that it suffices to show C · aok π ∼ v C · aok ( π ◦ τ ) for transpositions τ ∈ S N . But this follows from</p>
               <p>C · aok π ∼ v C · delayed π ( j ) = C · delayed ( π ◦ τ ) ( j ) ∼ v C · aok ( π ◦ τ ) (4.18)</p>
               <p>if τ flips j and j + 1 mod N . The rest of the proof is as follows: Prove the existence of a bivalent initial configuration by classical means (e.g., a bit-flipping argument; note that we can completely silence a process from an execution using our layers), use Theorem 4.3 to arrive at d ( X 0 , X 1 ) &gt; 0, deduce the existence of a 0-1-fork and finally use Lemmata 4.8 and 4.9 to derive a contradiction. Notice that we proved the following result:  Theorem 4.4. There is no consensus algorithm in the asynchronous message-passing model with at most one crash failure.</p>
            </sec>
            <sec>
               <title>4.3.2. Asynchronous Shared Memory</title>
               <p>In this section, we will prove impossibility of consensus in asynchronous system models of Section 2.4 with shared single-writer read-write registers where one process may fail by crashing (e.g., Fich and Ruppert 2003, Section 5.2). We again fix some enumeration p 1 , p 2 , . . . , p N of the processes. Again, we choose for every permutation π ∈ S N a set L π = { aok π , except π } of layers and set L = π ∈ S N L π . The so-defined scheduler f : L N → S is closed (see previous section for details). In layer aok π , all processes take a step in the order π (1) , π (2) , . . . , π ( N ). In layer except π , all processes except for p π ( N ) take a step in order π (1) , π (2) , . . . , π ( N − 11 To be more precise, a permutation τ ∈ S N is called a transposition if there exist 1 i, j N such that i + 1 ≡ j mod N , π | ( { 1 , . . . , N } \ { i, j } ) is the identity and π |{ i, j } is not the identity.  1). Let C be any configuration. It is C · aok π · except π = C · except π · aok ( π ◦ σ ) where σ ( j ) ≡ j − 1 mod N which shows that the hypothesis of Lemma 4.8 holds if we restrict our choices to a single L π . As above, we are done if we show C · aok π ∼ v C · aok ( π ◦ τ ) for every transposition τ . But this follows from the fact that we consider single-writer registers by the following case distinction. Let τ flip indices i and i + 1. Case 1: Processes p π ( i ) and p π ( i +1) both perform a read operation or both perform a write operation. Then the resulting configurations are equal. Case 2: Process p π ( i ) performs a read and p π ( i +1) performs a write . Then the resulting configurations differ in at most the state of p π ( i ) . Case 3: Process p π ( i ) performs a write and p π ( i +1) performs a read . Then the resulting configurations differ in at most the state of p π ( i +1) . The rest of the impossibility proof follows as above: Existence of a bivalent initial configuration by bit-flipping (since we can completely silence a process), existence of a fork by Theorem 4.3 and contradiction by Lemma 4.8. Theorem 4.5. There is no consensus algorithm in the asynchronous single-writer shared memory model with at most one crash failure.</p>
            </sec>
            <sec>
               <title>4.3.3. Transient Message Loss</title>
               <p>We will prove the impossibility result of Santoro and Widmayer 1989, Section 4.1 in the model of Section 2.3. Let p 1 , p 2 , . . . , p N be an enumeration of the set of processes. We define the set of layers to be L = { loss( i, j ) | 1 i N, 0 j N } (4.19) and again describe the scheduler f : L N → S by describing each of the layers. As above, this scheduler is closed. The layer loss( i, j ) is equal to the single event (set of omissions) O = { ( i, k ) | k j and k = i } . (4.20) For every configuration, we have C · loss( i, j ) ∼ p C · loss( i, j − 1) and loss( i, 0) = loss( i , 0) which implies that the precondition of Lemma 4.8 holds. Note that we can silence any process p i from any time on by repeatedly issuing loss( i, N ). By the usual method, this concludes the impossibility proof.  Theorem 4.6. There is no consensus algorithm in the synchronous message-passing model with at most N − 1 per-round message omissions.</p>
            </sec>
         </sec>
      </sec>
      <sec>
         <title>5. Algebraic Topology</title>
         <p>This chapter deals with a different view on topology than that we took in Chapter 4. We will consider algebraic topology (Hatcher 2002). In this discipline, we assign to topological spaces certain algebraic objects, reason about relations between these algebraic objects and then translate back these insights to statements about topological spaces. These techniques will enable us to prove the impossibility of k -set agreement.</p>
         <sec>
            <title>5.1. Introduction</title>
            <p>Algebraic topology splits up into two major threads: homotopy and homology. An example of a construction used in homotopy is the fundamental group of a (path- connected) topological space X . It is defined as the quotient of the group of all loops, i.e., continuous maps [0 , 1] → X starting from and ending at the same point x 0 ∈ X where the group operation is defined as the juxtaposition of two loops, with respect to the equivalence relation of homotopy, i.e., continuous deformability of one loop to another. A very natural question to ask is which topological spaces have trivial fundamental groups, i.e., in which spaces are all loops continuously deformable into each other. The class of these spaces is called the class of simply connected spaces. The other major branch of algebraic topology is homology. Similar to homotopy, it deals with spaces of continuous mappings [0 , 1] q → X , but unlike homotopy, it does not directly define a group operation on this set, but rather factors the free Abelian group generated by these mappings with respect to a certain equivalence relation. We will need a few techniques from homology in the course of this chapter and we provide a brief introduction to this topic in Section 5.2. Turning to the world of distributed computing again, what we will do in this chapter is proving impossibility of k -set agreement in asynchronous systems communicating by read-write registers in the presence of up to k crash failures. The proof that we present here was developed by Herlihy and Shavit (1993). Its strategy is to introduce a structure on the set of local processor states of an algorithm, namely that of a simplicial complex and reason that the subcomplex of final configurations (configurations in which enough processes have decided) is incompatible with the so- called output complex, i.e., a simplical complex describing decisions that are allowed by the problem statement.</p>
         </sec>
         <sec>
            <title>5.2. Homology</title>
            <p>This section introduces basic notions of homology theory.</p>
            <sec>
               <title>5.2.1. Chain Complexes</title>
               <p>In this section, we will discuss the basic algebraic objects we will encounter along the way. These are chain complexes and more generally graded Abelian groups . Let A ∗ = ( A k ) k ∈ Z be a sequence of Abelian groups. Then we call A ∗ a graded Abelian group . A morphism φ of degree m ∈ Z from A ∗ to B ∗ is a sequence φ k : A k → B k + m of Abelian group morphisms. We denote a morphism of degree m = 0 plainly by the name morphism . That is, a graded Abelian group is just an enumerable collection of Abelian groups and a morphism is just an enumerable collection of Abelian group morphisms. A chain complex ( A ∗ , ∂ ) is a graded Abelian group A ∗ together with a morphism ∂ of degree − 1 from A ∗ to itself, i.e., for every k ∈ Z we have that ∂ k : A k → A k − 1 is an Abelian group morphism, with the additional property that ∂ k ◦ ∂ k +1 = 0 for all k ∈ Z . This restriction is the same as saying that the image im ∂ k +1 is a subset of the kernel 1 ker ∂ k . We call ∂ the boundary operator of the chain complex. A morphism φ : C → D of graded Abelian groups between chain complexes is a morphism of chain complexes if and only if for all k ∈ Z it holds that φ k − 1 ◦ ∂ k C = ∂ k D ◦ φ k . In other words, the diagram in Figure 5.1 commutes.</p>
               <p>··· ← ∂ − k C − − − 2 − C k − 2 ← ∂ − k C − − − 1 − C k − 1 ← − ∂ − k C − − C k ← ∂ − k C − +1 − − C k +1 ← ∂ − k C − +2 − − C k +2 ← ∂ − k C − +3 − − ···      φ k − 2  φ k − 1  φ k  φ k +1  φ k +2  ··· ← ∂ − k D − − − 2 − D k − 2 ← ∂ − k D − − − 1 − D k − 1 ← − ∂ − k D − − D k ← ∂ − k D − +1 − − D k +1 ← ∂ − k D − +2 − − D k +2 ← ∂ − k D − +3 − − ··· Figure 5.1.: Commutative diagram for chain complex morphisms</p>
            </sec>
            <sec>
               <title>5.2.2. The Homology Functor</title>
               <p>To every chain complex C , we may assign a special graded Abelian group H ∗ ( C ) called the homology of C . It has very interesting properties and is especially interesting when putting topological spaces into the mix as is done in Section 5.4.1. Let C be a chain complex. We have already noted that im ∂ q +1 is contained in ker ∂ q for every q ∈ Z . These two sets being Abelian groups, we may form the  quotient H q = ker ∂ q / im ∂ q +1 and H q is again an Abelian group. Hence, H ( C ) = ( H q ) q ∈ Z is a graded Abelian group which we call the homology of C . Note that if H q = 0 for all q ∈ Z , then ker ∂ q = im ∂ q +1 . Hence the homology of a chain complex measures how far the following diagram is from being exact 2 at C q :</p>
               <p>1 The kernel of an Abelian group morphism f : A → B is defined to be the set of all a ∈ A with f ( a ) = 0 and is denoted by ker f . It is ker f an Abelian subgroup of A .</p>
               <p>∂ q − 1 ∂ q ∂ q +1 ∂ q +2 ··· ← − − − C q − 1 ← − C q ← − − − C q +1 ← − − − ··· (5.1)</p>
               <p>Let φ : C → D be a chain complex morphism. Because of the defining relation for chain complex morphisms, we may deduce that φ q [im ∂ q C +1 ] ⊂ im ∂ q D +1 and φ q [ker ∂ q C ] ⊂ ker ∂ q D . But this implies that φ q factors to an Abelian group morphism ker ∂ q C / im ∂ q C +1 → ker ∂ q D / im ∂ q D +1 . We write φ ∗ for the resulting morphism H ( C ) → H ( D ). This construction has the property that ( ψ ◦ φ ) ∗ = ψ ∗ ◦ φ ∗ and (id C ) ∗ = id H ( C ) which indeed qualifies it for the name functor . Let ψ : C → D be another chain complex morphism. A chain homotopy from φ to ψ is a graded Abelian group morphism h : C → D of degree 1 such that</p>
               <p>ψ − φ = ∂ D ◦ h + h ◦ ∂ C . (5.2) In this case, we write φ ψ and it is φ ∗ = ψ ∗ : H ( C ) → H ( D ).</p>
            </sec>
         </sec>
         <sec>
            <title>5.3. Simplicial Complexes</title>
            <p>A simplicial complex C is a set of sets with the following property: (LC) If A ∈ C and B ⊂ A , then B ∈ C .</p>
            <p>In other words, a simplicial complex is left-closed with respect to the set inclusion relation. The elements of C are called simplices and the elements of the set C are called vertices . For a simplex S ∈ C , we define its dimension dim S = | S | − 1. We set dim C = sup S ∈C dim S . A vertex map between two simplicial complexes C and D is a map f : C → D . It is called simplicial if for every S ∈ C , f [ S ] ∈ D , i.e., every simplex in C gets mapped to a simplex in D . Simplicial complexes together with simplicial vertex maps form a category.</p>
            <sec>
               <title>5.3.1. Simplicial Homology</title>
               <p>In this section, we will assign a chain complex C ( C ) and also its homology H ( C ) to any simplicial complex C . 2 A sequence A − → f B → g C where A, B, C are Abelian groups and f : A → B , g : B → C are Abelian group morphisms is called exact if im f = ker g .</p>
               <p>Definition 5.1. Let S be a set. We define the free Abelian group generated by S to be the group Z S .</p>
               <p>Let C be a simplicial complex and fix any total order on its set of vertices. For any q 0 let C q denote the set of q -dimensional simplices in C and let C q be the free Abelian group generated by C q . The family ( C q ) q ∈ Z is a graded Abelian group. We will now define a boundary operator ∂ q : C q → C q − 1 . Let S = { v 0 , v 1 , . . . , v q } ∈ C q with v i &lt; v j for i &lt; j . It suffices to define ∂ q for such elements by the universal property of free Abelian groups. 3 Set q ∂ q ( S ) = ( − 1) k { v 0 , v 1 , . . . , v k − 1 , v k +1 , . . . , v q } . (5.3) k =0 Lemma 5.1. With the above definition, ∂ q ◦ ∂ q +1 = 0.</p>
               <p>Proof. Let S = { v 0 , v 1 , . . . , v q +1 } ∈ C q +1 with v i &lt; v j for i &lt; j . Then q +1 q +1 ∂ q ( ∂ q +1 ( S )) = ∂ q ( − 1) k ( S \ { v k } ) = ( − 1) k ∂ q ( S \ { v k } ) k =0 k =0 q +1 k − 1 q = ( − 1) k ( − 1) ( S \ { v , v k } ) + ( − 1) ( S \ { v k , v +1 } ) k =0 =0 = k q +1 k − 1 q +1 (5.4) = ( − 1) k ( − 1) ( S \ { v , v k } ) + ( − 1) − 1 ( S \ { v k , v } ) k =0 =0 = k +1 = ( − 1) k + ( S \ { v , v k } ) − ( − 1) k + ( S \ { v k , v } ) 0 &lt;k q +1 0 k&lt; q +1 =0 as claimed.</p>
               <p>It is clear that a different choice of the total order on C yields an isomorphic chain complex. This concludes the definition of the chain complex C ( C ) = ( C ∗ , ∂ ). We set H ( C ) = H ( C ( C )). Let f : C → D be a simplicial vertex map. Define for every q 0 the map f # : C q ( C ) → C q ( D ) for S ∈ C q , by</p>
               <p>f [ S ] if dim f [ S ] = q f # ( S ) = (5.5) 0 else. The so-defined map f # : C ( C ) → C ( D ) is a chain map and we have ( g ◦ f ) # = g # ◦ f # and id S C # = id C ( C ) . 3 Let G be the free Abelian group generated by some set S , let H be an Abelian group and let f : S → H be a map. Then there exists a unique morphism φ : G → H such that φ | S = f .</p>
               <p>Definition 5.2. Let C be a simplicial complex and let k 0. We say that C is k -acyclic if H 0 ( C ) = ∼ Z and H q ( C ) = 0 for 1 q k . Let C and D be simplicial complexes and let Σ : C → P ( D ) be a mapping with the following properties: (1) Σ( S ) is a simplicial complex for every S ∈ S . (2) Σ( S ) ⊂ Σ( S ) if S ⊂ S . (3) Σ( S ) is ( q − 1)-acyclic for every S ∈ S with dim S = q .</p>
               <p>Then we call Σ an acyclic carrier . Let φ : C ( C ) → C ( D ) be a chain map. We say that φ is carried by Σ if T ∈ Σ( S ) for all S ∈ C and T ∈ D with c T = 0 where φ ( S ) = T ∈D c T T . The following is Herlihy and Rajsbaum 2000, Theorem 3.3: Theorem 5.1. Let Σ : C → P ( D ) be an acyclic carrier.</p>
               <p>(1) There exists a chain map C ( C ) → C ( D ) that is carried by Σ. (2) If φ, ψ : C ( C ) → C ( D ) are both carried by Σ and dim S = dim Σ( S ) for all S ∈ C , then φ = ψ .</p>
            </sec>
         </sec>
         <sec>
            <title>5.4. Algebraic vs. Combinatorial Topology</title>
            <p>This section explains the commonalities of pure algebraic topology, i.e., the investigation of topological spaces with help of assigned algebraic structures, and combinatorial topology, i.e., the investigation of combinatorial structures with help of assigned algebraic structures. To be more precise, we introduce the homology of a topological space and the geometric realization of a simplicial complex and show that these two constructions are compatible. Results from this section are not needed later on and are presented to deepen the reader’s understanding of these interconnections.</p>
            <sec>
               <title>5.4.1. Singular Homology</title>
               <p>We will now show how to relate the algebraic construction of homology to topological spaces. More precisely, we will assign to every topological space X a chain complex C ( X ) and a graded Abelian group H ( X ) called its singular homology .</p>
               <p>Definition 5.3. Let q 0. The topological space ∆ q = x ∈ R q +1 | x j = 1 and x j 0 for all j (5.6) is called the q -dimensional standard simplex .</p>
               <p>The q th component of the graded Abelian group of C ( X ) is defined to be the free Abelian group generated by the set of continuous mappings ∆ q → X . It remains to define the boundary operator ∂ q : C q ( X ) → C q − 1 ( X ). It suffices to define this map on the generators of C q ( X ). So let σ : ∆ q → X be continuous. Consider the following continuous functions δ q i : ∆ q − 1 → ∆ q for 1 i q + 1: δ q i ( x 1 , . . . , x q ) = ( x 1 , . . . , x i − 1 , 0 , x i , x i +1 , . . . , x q ) (5.7) It is easy to see that this function really has values in ∆ q . We now define ∂ q ( σ ) by the equation q +1 ∂ q ( σ ) = ( − 1) i − 1 σ ◦ δ q i . (5.8) i =1 Lemma 5.2. For any topological space X and any q ∈ Z , with the above definition of C ( X ), it holds that ∂ q ◦ ∂ q +1 = 0.  Hence C ( X ) is really a chain complex. We may thus form its homology H ( C ( X )) or just H ( X ) in short. Let f : X → Y be a continuous mapping. We define the mapping f # : C ( X ) → C ( Y ) by setting f # ( σ ) = f ◦ σ for continuous σ : ∆ q → X . It holds that ( g ◦ f ) # = g # ◦ f # and (id X ) # = id C ( X ) . We denote by f ∗ the mapping H ( X ) → H ( Y ) induced by the chain complex morphism f # . Definition 5.4. Let X be a topological space and let k 0. We say that X is k -acyclic if H 0 ( X ) ∼ = Z and H q ( X ) = 0 for 1 q k .</p>
            </sec>
            <sec>
               <title>5.4.2. Geometric Realization of Simplicial Complexes</title>
               <p>In this section, we assign a topological space |C| to every simplicial complex C , called its geometric realization . Let C be a simplicial complex. As a set, we define |C| by     |C| = α : C → [0 , 1] α ( v ) = 1 and { v | α ( v ) = 0 } ∈ C (5.9)  v ∈ S C  and we define the topology on |C| by the metric d ( α, β ) = ( α ( v ) − β ( v )) 2 . (5.10) v ∈ S C For a simplicial vertex map f : C→ D , we define | f | : |C| → |D| by | f | ( α )( w ) = α ( v ) . (5.11) f ( v )= w Consequences of these definitions are (Spanier 1966, Sec. 3.1):</p>
               <p>(1) | f | is continuous. (2) | g ◦ f | = | g | ◦ | f | and id S C = id |C| (3) If C is finite then |C| is a compact Hausdorff space.</p>
            </sec>
            <sec>
               <title>5.4.3. Equivalence</title>
               <p>The following theorem relates the homologies of C and |C| (Spanier 1966, Sec. 4.6, Theorem 8). Theorem 5.2. Let C be a simplicial complex. Then H ( C ) = ∼ H ( |C| ).</p>
            </sec>
         </sec>
         <sec>
            <title>5.5. Configuration Complexes</title>
            <p>This section introduces simplicial complexes that we will assign to algorithms in order to reason about the topological structure of these algorithms. In these complexes, vertices will represent a state of a single process and a simplex consisting of vertices s 1 , s 2 , . . . , s N will represent a reachable configuration in which the i th process has local state s i .</p>
            <sec>
               <title>5.5.1. Input Complexes</title>
               <p>k -set agreement is a decision task, i.e., every process has an input value and computes an output value. Simplicial complexes provide a convenient way to describe the structure of possible combinations of input and output values. For example, consider the complex of initial configurations of the t -resilient binary consensus problem. Here, every process p j starts with a private input value x j ∈ { 0 , 1 } . For simplicity, we assume that every process has only two distinct initial states: one with input value x j = 0 and one with input value x j = 1. Denote by ( j, α ) the initial state of process p j with input value x j = α . Then the set of vertices is equal to { (1 , 0) , (1 , 1) , (2 , 0) , (2 , 1) , . . . , ( N, 0) , ( N, 1) } . (5.12) Basic simplices are sets of the form</p>
               <p>S = { (1 , α 1 ) , (2 , α 2 ) , . . . , ( N, α N ) } (5.13)</p>
               <p>where α j ∈ { 0 , 1 } . The input complex for the binary consensus problem is defined by the set of these basic simplices and its subsets. A geometric realization of the input complex for N = 2 and N = 3 is depicted in Figure 5.2. The input complex of k -set agreement is similar to that of consensus except that the set in which the input values x j may vary is changed from { 0 , 1 } to { 1 , 2 , . . . , M } where M N .</p>
               <p>( p, 1) ( q, 1) ( q, 0) ( p, 0) (a) N = 2 (b) N = 3 Figure 5.2.: Input Complexes</p>
            </sec>
            <sec>
               <title>5.5.2. Output Complexes</title>
               <p>We may also look at the output complex of the t -resilient binary consensus problem, i.e., the complex that describes the possible output values y j . Vertices here are also of the form ( j, α ) which reflects the fact that process p j has decided to output value y j = α . Basic simplices are either of the form</p>
               <p>S = { ( j 1 , 0) , ( j 2 , 0) , ( j 3 , 0) , . . . , ( j r , 0) } (5.14)</p>
               <p>or of the form S = { ( j 1 , 1) , ( j 2 , 1) , ( j 3 , 1) , . . . , ( j r , 1) } (5.15) where all j are distinct and r N − t , that is, at least N − t processes have decided and they all decided to the same value. The output complex for the t -resilient binary consensus problem is defined by the set of these basic simplices and its subsets. Examples for N = 2 and N = 3 are depicted in Figure 5.3.</p>
               <p>( p, 1) ( q, 1) ( r, 0) ( q, 1) ( r, 1) ( q, 0) ( p, 0) ( p, 0) ( q, 0) ( p, 1) (a) N = 2 (b) N = 3 Figure 5.3.: Output Complexes The output complex of the t -resilient k -set agreement problem is defined by the set of simplices of the following form S = { ( j 1 , y 1 ) , ( j 2 , y 2 ) , . . . , ( j r , y r ) } (5.16)</p>
               <p>where |{ j 1 , . . . , j r }| N − t and |{ y 1 , . . . , y r }| k and all of its subsets. The problem specification of a decision task is a mapping ∆ that maps a basic input simplex S n − 1 to the set of basic output simplices that are allowed as decision values given the specified initial configuration (input simplex).</p>
            </sec>
            <sec>
               <title>5.5.3. Protocol Complexes</title>
               <p>When considering an initial configuration ( an input simplex) of a decision task, one may ask which final configurations , i.e., configurations in which all processes have halted (decided or crashed), are reachable from this initial configuration. The answer to this question defines a mapping from input simplices to sets of final configurations. For an input simplex S ∈ I with dim S n − t − 1, i.e., | S | n − t , we define the complex P ( S ) to be the reachable subcomplex of P where only processes take steps that appear in S with initial states as specified in S . The set of configurations (viewed as simplices) that occur in the image of this mapping define a complex, the protocol complex . It encompasses the information which final configurations may occur when running the protocol (algorithm). To be more precise, a simplex S = { C j 1 , C j 2 , C j 3 , . . . , C j r } (5.17) is in the protocol complex if and only if</p>
               <p>(1) C j k is an internal state of process with number j k in which this process has decided (2) all j k are distinct (3) there exists an execution of the protocol in which exactly the processes with numbers j 1 , j 2 , . . . , j r decide and all other processes crash (and do not reach a decision value) (4) in the above execution, process p j k halts with internal state equal to C j k .</p>
               <p>Consider the protocol complex P of some protocol that solves a decision task with input complex I , output complex O and decision map ∆. Then, we may assign to every final state of a process (vertex in P ) its decision value (vertex in O ). More precisely, a final state C j gets mapped to α ∈ { 1 , 2 , . . . , M } if and only if process p j has decided to α in state C j . This mapping can be extended in a natural manner to a map from simplices in P to simplices in O . Denote this simplicial vertex map by δ : P → O .</p>
            </sec>
         </sec>
         <sec>
            <title>5.6. Impossibility of k -Set Agreement</title>
            <p>In this section, we prove the impossibility of wait-free k -set agreement in asynchronous shared-memory environments, as presented in Herlihy and Shavit 1993. Of course, we will heavily rely on methods from algebraic topology. In particular, we will define a class of protocols (“full information protocols”) that has stronger system assumption than the asynchronous shared-memory model and analyze its protocol complex. This will help us derive a contradiction. A fortiori, this will establish the impossibility result for usual asynchronous shared-memory systems.</p>
            <sec>
               <title>5.6.1. Full Information Protocols</title>
               <p>Full information protocols communicate using a set of single-writer shared-memory variables which allows for taking atomic snapshots , i.e., the contents of all variables are read in a single step. Denote by RW n k the asynchronous shared-memory model with read-write registers where at most k processes may fail by crashing and denote by FI n k the model of full information protocols where at most k processes may fail by crashing. The set of algorithms in FI n k is quite restricted: The set S 0 = ∼ I of initial states of processes are arbitrary, but the rest of the algorithm is defined by a single decision vertex map δ : F → O ∪ {⊥} where O is an output complex and the set F is the set of full information states . The set F consists of tuples of the form ( v 1 , v 2 , . . . , v n ) where every v j is either contained in I ∪ {⊥} or again such a tuple. In a full information protocol each process</p>
               <p>(1) has a single unbounded shared register it can write. (2) writes its input value to its shared register in its first step. (3) after the first step, repeatedly reads the values v j from all registers R j , puts them together into a tuple v = ( v 1 , . . . , v n ) and writes this tuple v into its own register. (4) if δ ( v ) = ⊥ after such an iteration, the process halts with output value equal to δ ( v ). Theorem 5.3. If there exists a k -set agreement protocol in RW n k , then there exists a k -set agreement protocol in FI n k .</p>
               <p>Proof. The non-trivial part of this proof is to simulate multiple-writer registers with single-writer registers. This is part of Attiya and Welch 2004, Theorem 10.9. —  Attiya and Welch 2004, Theorem 10.15 shows that the converse of Theorem 5.3 also holds.</p>
            </sec>
            <sec>
               <title>5.6.2. Properties of Full Information Protocols</title>
               <p>The most important property of full information protocols to us is the following (Herlihy and Shavit 1993, Corollary 4.9): Theorem 5.4. Let P be a protocol in FI n n − 1 and let S ∈ I with dim S = q . Then P ( S ) is ( q − 1)-acyclic.</p>
            </sec>
            <sec>
               <title>5.6.3. This Implies Impossibility</title>
               <p>This section contains the proof of Herlihy and Rajsbaum 2000, Corollary 5.3. Lemma 5.3. Let P be a protocol in FI n n − 1 . Then S → P ( S ) is an acyclic carrier.</p>
               <p>Proof. Property (1) of an acyclic carrier is fulfilled by definition of P ( S ). Property (3) is Theorem 5.4. We will show property (2). So let S ⊂ S in I . Let T ∈ P ( S ), we have to show T ∈ P ( S ). By definition, T is a final configuration of the protocol where only processes in S take steps with initial states as in S . Since S ⊂ S , the initial states of processes in S are equal in S and S . Also, since it is admissible to crash processes initially, every execution contributing to P ( S ) also contributes to P ( S ). Hence T ∈ P ( S ) as claimed. Theorem 5.5. Let P be a protocol in FI n n − 1 that solves k -set agreement. Then k n . Proof. Suppose by contradiction that k &lt; n . Let S n − 1 = { (1 , 1) , (2 , 2) , (3 , 3) , . . . ( n, n ) } ∈ I and set A = P ( S n − 1 ) ⊂ I which is a simplicial subcomplex of I . Let B ⊂ O be defined to contain sets of the form</p>
               <p>{ ( j 1 , y 1 ) , ( j 2 , y 2 ) , . . . , ( j r , y r ) } ∈ O (5.18)</p>
               <p>where 1 y n for all 1 r . Define π : B → A by π ( j, y ) = ( y, y ). Let σ : C ( I ) → C ( P ) be a chain map that is carried by S → P ( S ) (Theorem 5.1(1)). Now set φ = π # ◦ δ # ◦ σ : C ( A ) → C ( A ) and Σ : A → P ( A ), Σ( S ) = P ( S ). Σ is an acyclic carrier. It is obvious that Σ carries φ . Theorem 5.1(2) now implies that φ = id C ( A ) . But since the simplex S n − 1 = { (1 , 1) , (2 , 2) , . . . , ( n, n ) } does not occur in the image of φ because k &lt; n , we derive a contradiction.</p>
            </sec>
         </sec>
      </sec>
      <sec>
         <title>6. Summary</title>
         <p>We investigated two applications of topology to problems in distributed computing. These were impossibility proofs of (a) consensus in a number of 1-resilient systems and (b) k -set agreement in asynchronous k -resilient systems. For this, we used methods from (a) point-set topology and (b) algebraic topology. Point-set topology helped us in providing a way of reasoning about execution trees in a unified way to prove impossibility of consensus. We regarded an execution as a sequence of configurations and equipped the sequence space of all possible executions with a metric which had the property that those executions are close together which share a long common prefix. We then used schedulers to pass to a closed (hence compact) subspace of the space of all executions. This subspace satisfied the precondition of Lebesgue’s lemma which provided a uniform step bound after which every configuration is univalent. A model-dependent analysis of configuration similarity then concluded the impossibility proofs. Algebraic topology, in particular homology, was introduced to examine simplicial complexes. To utilize this, we considered input and output complexes of decision tasks and configuration complexes of protocols (algorithms). The function that maps input simplices to the complex of possible final configurations starting from it turned out to be an acyclic carrier. This was then used to derive the impossibility of wait-free k -set consensus, following Herlihy and Shavit (1993).</p>
      </sec>
      <sec>
         <title>A. Topological Prerequisites</title>
         <sec>
            <title>A.1. Motivation and Examples</title>
            <p>Topology (Bourbaki 1989) is the mathematical discipline that explores the concept of “closeness” and emerging notions. Fundamental is the notion of “neighborhood”. Informally speaking, a topological space is a set together with a structure on this set that specifies which points (elements of the set) are close to each other.</p>
            <sec>
               <title>A.1.1. Distances</title>
               <p>Topological spaces (though in disguise) are actually encountered in every beginning calculus class. More specifically, the real line R is a topological space and many of its famous properties are in fact of topological nature. The following example shall exemplify how specific topological concepts might look like.  Example A.1 (The real line) . The dominant and natural notion of closeness depends on the definition of distance between two real numbers. For real numbers x and y , their distance is defined as</p>
               <p>(see Figure A.1), where | z | denotes the absolute value of z . Starting from this definition, we may now state what it means for real numbers to be close to each other. We may call x and y to be ε -close if their distance satisfies</p>
               <p>where ε is some positive number. So, for every ε , we get a different notion of closeness. Of course, these notions are not independent of each other. The most important dependencies are:</p>
               <p>(1) The only point that is ε -close to x for all ε , is x itself. (2) If x is ε -close to y , then y is also ε -close to x . (3) If x and z are ε -close and z and y are ε -close, then x and y are 2 ε -close.</p>
               <p>We will later see how these properties generalize to the formal definition of a topology. We have already discussed an easy but important (topological) property of the real line; above property (1): For any two distinct real numbers x and y , there exists</p>
               <p>| x − y | x y R Figure A.1.: Distances on the real line</p>
               <p>some positive ε such that x and y are not ε -close, for we may choose ε = d ( x, y ). This property characterizes the real line as a topological T 1 space. (The properties T ι for ι ∈ { 0 , 1 , 2 , 3 , 3 1 2 , 4 } are the so-called separation axioms for topological spaces.) But R satisfies even more: it is a T 2 or Hausdorff space. In subsequent sections, we will define for any topological space what it means to be of this important class of spaces, i.e., to be Hausdorff and show some of their convenient properties.  We observe that we may define the notion of ε -closeness on any set that, as in the previous example, has a distance function d defined on it. This generalization leads to the definition of metric spaces which lie in the class of topological spaces. But before we formally define this, we look at a slight generalization of Example A.1, namely the Euclidean spaces R n , and discuss in more detail the topological structure and properties that these spaces carry.</p>
               <p>Example A.2 (Euclidean spaces) . As an analogue of the real absolute value, we have the norm of a vector x ∈ R n :  x 1 </p>
               <p>Thus, the distance of two vectors x and y in R n is defined as</p>
               <p>The three properties of Example A.1 still hold. Property (3), also known as the triangle inequality, is depicted in Figure A.2. Its name comes from the fact that in a triangle, the length of any edge is less than the sum of lengths of the other two. The ε -neighborhood of a point x is the set of all points that are ε -close to x . It is also called a ball with center x and radius ε and is denoted by B ε ( x ). Now, an open set is a set X ⊂ R n such that, for every x ∈ X , there exists an ε -neighborhood of x that is contained in X . Intuitively, an open set is a set that has “a little room” around every of its points, i.e., it does not have a “sharp boundary”. The situation is sketched below in Figure A.3. Examples of open sets include:</p>
               <p>z ε ε x 2 ε y Figure A.2.: Triangle inequality in R 2 ε x X Figure A.3.: Point x has an ε -neighborhood that is contained in X (1) In R 1 , the so-called “open intervals”</p>
               <p>are in fact open. For if x ∈ ( a, b ), then, by definition, x − a &gt; 0 and b − x &gt; 0. Hence with ε = min { x − a, b − x } , we get B ε ( x ) ⊂ ( a, b ): We may assume without loss of generality that ε = x − a , i.e., x − a b − x . But then,</p>
               <p>We have just proved that every set of the form ( a, b ) for real numbers a and b is open. Note that this result still holds if the interval ( a, b ) is the empty interval, i.e., if b a . The empty set is always trivially open since there are no elements in it to be checked by the defining condition for open sets. The result even holds if a = −∞ or b = + ∞ . More generally, every open interval in a totally ordered set is indeed open in the induced order topology . We note that every ε -ball in R 1 is of the form ( x − ε, x + ε ), hence an open interval, hence open. This is part of a more general principle. (2) In R n , we may also define “open intervals” by setting</p>
               <p>Similar to the above case, we may choose</p>
               <p>and arrive at the insight that these open intervals are also open sets in the topological sense. Again, we may allow for the a ι and b ι to be infinity (positive or negative). Contrary to the above, however, it is not the case that every ε -ball in R n is an open interval, i.e., of the form ( a, b ) for some a, b ∈ R n . But nonetheless, ε -balls are always open as we will see next. (3) ε -neighborhoods in R n are open. This fact is due to the triangle inequality which we already discussed above. Let x ∈ R n be any point and r &gt; 0 any radius. We will show that the ball</p>
               <p>is open in R n : Let y ∈ B r ( x ). Choose ε = r − y − x . It remains to show that B ε ( y ) is a subset of B r ( x ). So, let z ∈ B ε ( y ), i.e.,</p>
               <p>Then, by the triangle inequality and (A.10),</p>
               <p>and we are done, because this implies z ∈ B r ( x ). The proof is pictured in</p>
               <fig id="FA.4">
                  <caption>
                     <p>Figure A.4.</p>
                  </caption>
                  <graphic xlink:href=""/>
               </fig>
               <p>r − y − x y − x y x r Figure A.4.: ε -balls are open</p>
               <p>We have defined the notion of an open set in Euclidean spaces and identified some important classes of sets to be open. In the following, we will generalize the ideas of this example to spaces that are equipped with some way of measuring distances. These spaces are known as metric spaces .  Metric spaces are an immediate generalization of Euclidean spaces. As with any generalization, the idea is to purposely ignore certain aspects and properties of the object in question and focus on just a very limited number of properties that these objects have in common. In our case, the important notion that generalizes Euclidean spaces to metric spaces is that of distance . The idea is to forget everything we know about the Euclidean norm · except that we may use it to define the distance of two points x and y by taking the norm of their difference. Thus, we take the entity “norm” and build a new machine out of it: A machine that takes two points as input and outputs a number — their distance d ( x, y ). After identifying a notion that lends itself to generalization, it is crucial to work out which basic properties have to be attributed to it such that one can define the notion by means of these properties. We already have listed these properties for our case: Properties (1), (2), (3) from Example A.1 which we will use in the following definition.</p>
               <p>Definition A.1. Let X be a non-empty set and d : X × X → [0 , ∞ ) a function with the following properties. (M1) d ( x, y ) = 0 holds if and only if x = y (M2) d ( x, y ) = d ( y, x ) for all x, y ∈ X (M3) d ( x, z ) d ( x, y ) + d ( y, z ) for all x, y, z ∈ X Then we call d a metric on X and X a metric space .</p>
               <p>It will be the purpose of the next example to explore some properties of such spaces. Note that metric spaces are an important special case of topological spaces. In particular, execution spaces which will deliver our main results are in fact metric spaces.</p>
               <p>Example A.3 (Metric spaces) . Let X denote a metric space throughout this example. An ε -ball around x ∈ X is again defined as</p>
               <p>We also repeat the definition of an open set: A set A ⊂ X is called open if for every x ∈ A , there exists some ε &gt; 0 such that B ε ( x ) ⊂ A . We could now repeat the proof of the fact that every ε -ball is open from the previous example basically word-by-word. But instead, we will explore properties that are a bit more advanced. Let us begin by proving that every union of open sets is again open: Let A ι be open sets for every ι in some non-empty index set I and denote their set-theoretic union by A . We will show that A is open. For every x ∈ A , by the definition of union, there exists some ι 0 such that x ∈ A ι 0 . Now, because A ι 0 is open, there exists some ε &gt; 0 such that B ε ( x ) ⊂ A ι 0 . But A ι 0 ⊂ A means that we are done. In particular, every union of balls is open. What is interesting now, is that the converse also holds true: Every open set is a union of balls. To prove this, let A be an</p>
               <p>open set. For any x ∈ A , denote by ε x some positive number such that B ε x ( x ) ⊂ A . By definition of openness of A , these numbers do exist. We claim that</p>
               <p>It is clear that A is contained in the right-hand side of (A.13), because x is contained in any ball around itself that has positive radius. For the opposing direction, note that any ball that appears in the union is a subset of A by construction. Hence the union itself is a subset of A , which concludes the proof. We have just glanced at a very important notion: that of a basis of a topology. With this notion, we can express the last result as: The balls form a basis of the topology that is induced by the metric. We may now ask, of course, if intersections of open sets are again open. Unfortu- nately, this is not true in general as the following example shows: Let X = R and d ( x, y ) = | y − x | . The sets A k = ( −∞ , 1 /k ) are all open. However, their intersection</p>
               <p>is not. The fact that ( −∞ , 0] is not open can be seen in the following way: Of course, 0 ∈ ( −∞ , 0]. But for every ε &gt; 0, we have ε/ 2 ∈ B ε (0), while ε/ 2 ∈ ( −∞ , 0], hence the first is not a subset of the latter. A picture clarifying the situation is drawn in</p>
               <p>−∞ 0 2 ε ε Figure A.5.: The set ( −∞ , 0] is not open</p>
               <fig id="FA.5">
                  <caption>
                     <p>Figure A.5. A finite intersection of open sets, however, is indeed again open as the following reasoning shows: Let A 1 , A 2 , . . . , A k be open sets and let A denote their set-theoretic</p>
                  </caption>
                  <graphic xlink:href=""/>
               </fig>
               <p>intersection. We will show that A is open. So, as always, let x ∈ A . Since the A j are all open, there exist ε 1 , ε 2 , . . . , ε k such that B ε j ( x ) ⊂ A j for all 1 j k . We set ε = min { ε 1 , ε 2 , . . . , ε k } . But then B ε ( x ) ⊂ B ε j ( x ) for every j and hence B ε ( x ) ⊂ A by definition of A . By inspection of the preceding proof, we find the reason why it does not work in the case of infinitely many sets: The infimum of infinitely many positive numbers need not be positive. And this is exactly what happened in our counterexample: If we choose x = 0, then the maximal possible ε k such that B ε k (0) is contained in ( −∞ , 1 /k ) is equal to 1 /k . If we now try to set ε as in the proof above, we get</p>
               <p>which is not an admissible radius in the definition of openness.</p>
            </sec>
            <sec>
               <title>A.1.2. Compactness in R n</title>
               <p>This subsection introduces the concept of compactness in the special case of the Euclidean spaces R n and tries to communicate a bit of its importance in topology. Compactness is a property of a subset of a topological space that can “make local things global”. An example for this would be the well-known theorem “A continuous real function defined on the interval [ a, b ] is uniformly continuous”. Here, the set [ a, b ] is compact, continuity is a local property and uniform continuity is a global property. We start with the Definition A.2. A set C ⊂ R n is compact if it is bounded (i.e., there some ball with radius R &gt; 0 that contains C ) and its complement is an open set.  The most important property of compact sets is the following Theorem whose proof’s insight-length ratio is too low to demonstrate it here. Theorem A.1 (Heine-Borel) . Let C ⊂ R n be compact. Further, let A ι be a family of open sets, indexed by some set I , that covers C , i.e.,</p>
               <p>Then there exists some finite subfamily A ι 1 , A ι 2 , . . . , A ι k that covers C , i.e.,</p>
               <p>This theorem can also hold as a definition of compactness: A subset of R n is compact if and only if it satisfies the condition of Theorem A.1. The opposing direction is not too hard to prove and is demonstrated in order to get some feeling with the condition of Theorem A.1. Lemma A.1. Let C ⊂ R n satisfy the condition of Theorem A.1, i.e., for every family of open sets that covers C , there exists a finite subfamily that covers C . Then C is compact.  Proof. We have to show that (1) C is bounded (i.e., there exists some real R &gt; 0 such that C ⊂ B R (0)) and (2) its complement R n \ C is open. To prove (1), we choose the following family of open sets: The family of all balls B r (0) where r &gt; 0 is a real number. We already know that these are open. Of course, C is covered by this family of sets, because every x ∈ C is contained in the ball B 2 x (0) for obvious reasons. By hypothesis now, there exists a finite subfamily</p>
               <p>B r 1 (0) , B r 2 (0) , . . . , B r k (0) that covers C . But these balls are subsets of the ball B R (0) where R = max { r 1 , r 2 , . . . , r k } . Hence</p>
               <p>j =1 For (2), we have to show that R n \ C is an open set. So let x ∈ R n \ C . We define for every ε &gt; 0 the following set</p>
               <p>In order to use the condition of Theorem A.1, we have to prove that (a) all D ε are open and (b) C is covered by the D ε . Part (a) follows from the triangle inequality: Let y ∈ D ε , i.e., d ( x, y ) &gt; ε . We have to find some δ &gt; 0 such that d ( x, z ) &gt; ε for all z with d ( y, z ) &lt; δ . We claim that this is satisfied by δ = d ( x, y ) − ε . Let d ( y, z ) &lt; d ( x, y ) − ε , then</p>
               <p>hence D ε is open. Part (b) is obvious. By hypothesis, there exist ε 1 , ε 2 , . . . , ε k such that C ⊂ k j =1 D ε j . By setting ε = min { ε 1 , ε 2 , . . . , ε k } &gt; 0, we have C ⊂ D ε and thus</p>
               <p>which shows that R n \ C is open.</p>
               <p>It is hoped that this proof has created some insight on the nature of the condition of Theorem A.1. Most important and a major source of misunderstandings is the following triviality: The condition does not claim that there has to exist some finite family of open sets that covers C . This would be trivially fulfilled by any set since R n as a subset of itself is an open set and covers any other. The condition reads that any open covering of C , no matter how ugly it might look like, has to have a finite subcovering. This observation is of utmost importance. We will now proceed by showing the result announced in the introduction to this subsection whose proof exemplifies the routinely used reasoning known as “compact- ness argument”. For this, we recall some basic definitions from calculus. Definition A.3 (Continuity) . Let U be a subset of R and f : U → R a function. The function f is called continuous at a point x ∈ U if the following condition is satisfied: For every ε &gt; 0 there exists some δ &gt; 0 such that x ∈ U and | x − x | &lt; δ implies | f ( x ) − f ( x ) | &lt; ε . The function f is called continuous if f is continuous at every point x ∈ U . Definition A.4 (Uniform continuity) . Let U be a subset of R and f : U → R a function. The function f is called uniformly continuous if the following condition is satisfied: For every ε &gt; 0 exists some δ &gt; 0 such that x, x ∈ U and | x − x | &lt; δ implies | f ( x ) − f ( x ) | &lt; ε .  Of course, uniform continuity implies continuity, but the converse is not true in general as the example U = R and f ( x ) = x 2 shows. Our goal for now, however, will be to show that the converse does hold in a special case, namely that of compact intervals. The set of compact real intervals is quite easy to determine: It is exactly the set of bounded closed intervals [ a, b ]. The following proof is a most prototypical compactness argument.</p>
               <p>Theorem A.2 (Heine-Cantor) . Let f : [ a, b ] → R be a function. If f is continuous, then f is uniformly continuous.</p>
               <p>Proof. We will use Theorem A.1. Let ε &gt; 0. Since f is continuous, there exists a δ x &gt; 0 for every x ∈ [ a, b ] such that | x − x | &lt; δ x implies | f ( x ) − f ( x ) | &lt; ε/ 2. The condition | x − x | &lt; δ x can be reformulated as x ∈ B δ x ( x ). Since every δ x is greater than zero, we have</p>
               <p>which means that the family B δ x ( x ) is an open covering of the interval [ a, b ]. By Theorem A.1, there exist x 1 , x 2 , . . . , x k such that</p>
               <p>Let δ j = δ x j . If we now set δ = min { δ 1 , δ 2 , . . . , δ k } / 2, we are done: Let x, x ∈ [ a, b ] and | x − x | &lt; δ . There exists some j such that x ∈ B δ j / 2 ( x j ). By the triangle inequality, x, x ∈ B δ ( x ) ⊂ B δ j / 2 ( x ) ⊂ B δ j ( x j ). We can hence use the definition of δ j and conclude</p>
            </sec>
         </sec>
         <sec>
            <title>A.2. Topologies</title>
            <p>This section formally defines the notion “topology” resp. “topological space” and introduces basic properties.</p>
            <sec>
               <title>A.2.1. Open Sets and Neighborhoods</title>
               <p>Without further ado, finally, the fundamental Definition A.5 (Topological space) . Let X be a non-empty set. A set T ⊂ P ( X ) is called a topology on X if it satisfies the following properties. (O1) Every union of sets in T is an element of T .</p>
               <p>(O2) Every finite intersection of sets in T is an element of T . (O3) ∅ ∈ T and X ∈ T In this case, X is called a topological space . The sets O ∈ T are called open sets .</p>
               <p>To be precise, property (O3) could be omitted from the definition since ∅ = ∅ and ∅ = X by convention. This definition is indeed a generalization of the notion of openness as defined in Section A.1. We have defined the notion of openness two times now: One time in the language of metric spaces and one time in the language of topological spaces. We also mentioned that every metric space is also a topological space. This can be done, given a metric d on the space X , by the following definition:</p>
               <p>We proved in the language of metric spaces (O1) that arbitrary unions of open sets are again open and (O2) that finite intersections of open sets are open.</p>
               <p>Example A.4. We will now give some examples of topologies in simple settings. (1) The trivial topology exists on any non-empty set X . It is defined as T = {∅ , X } . (2) The discrete topology also exists on any non-empty set X . It is defined as T = P ( X ), i.e., the power set of X . (3) On a two-element set X = { a, b } , there are four different topologies, namely</p>
               <p>(4) There are 29 different topologies on a three-element set X = { a, b, c } , see Figure A.6. From there on, it gets complicated: 355 topologies on four-element sets, 6942 on five-element sets, 209527 on six-element sets, 9535241 on seven- element sets, 642779354 on eight-element sets and so on. (5) Any topology in which all singleton sets { x } are open, is the discrete topology by (O1).</p>
               <p>In Section A.1, we gave a number of examples of open sets, in particular we showed that specific sets that we called “neighborhoods” or “balls” are open. We will now generalize the notion of neighborhood and show some fundamental properties.</p>
               <p>Definition A.6 (Neighborhood) . Let X be a topological space and x ∈ X . A set N ⊂ X is called a neighborhood of x if there exists an open set O ⊂ X such that x ∈ O and O is contained in N , i.e., O ⊂ N . We will denote the set of all neighborhoods of x by N ( x ).</p>
               <p>{∅ , X } , {∅ , { a } , X } , {∅ , { b } , X } , {∅ , { c } , X } , {∅ , { a, b } , X } , {∅ , { a, c } , X } , {∅ , { b, c } , X } , {∅ , { a } , { b, c } , X } , {∅ , { b } , { a, c } , X } , {∅ , { c } , { a, b } , X } , {∅ , { a } , { a, b } , X } , {∅ , { b } , { a, b } , X } , {∅ , { a } , { a, c } , X } , {∅ , { c } , { a, c } , X } , {∅ , { b } , { b, c } , X } , {∅ , { c } , { b, c } , X } , {∅ , { a } , { b } , { a, b } , X } , {∅ , { b } , { c } , { b, c } , X } , {∅ , { a } , { c } , { a, c } , X } , {∅ , { a } , { a, b } , { a, c } , X } , {∅ , { b } , { a, b } , { b, c } , X } , {∅ , { c } , { a, c } , { b, c } , X } , {∅ , { a } , { b } , { a, b } , { b, c } , X } , {∅ , { b } , { c } , { a, b } , { b, c } , X } , {∅ , { a } , { c } , { a, c } , { b, c } , X } , {∅ , { b } , { c } , { a, c } , { b, c } , X } , {∅ , { a } , { b } , { a, b } , { a, c } , X } , {∅ , { a } , { c } , { a, b } , { a, c } , X } , {∅ , { a } , { b } , { c } , { a, b } , { b, c } , { a, c } , X } Figure A.6.: The 29 topologies on the set X = { a, b, c }</p>
               <p>We note that, in particular, every open set is neighborhood of any of its points. Simple consequences of the definition are:</p>
               <p>(1) If N ∈ N ( x ) and M ⊃ N , then M ∈ N ( x ). In particular, every union of neighborhoods of x is a neighborhood of x . (2) Every finite intersection of neighborhoods of x is a neighborhood of x . (3) x ∈ N for all N ∈ N ( x ). Properties (1) and (2) justify the name neighborhood filter for the set N ( x ). A less trivial result is the following. (4) For every N ∈ N ( x ) there exists some M ∈ N ( x ) such that N ∈ N ( y ) for all y ∈ M , i.e., N is is a neighborhood of all points in M .</p>
               <p>Of course, any such M has to be a subset of N . It is sufficient to take M to be the open set containing x as demanded in the definition of a neighborhood. In fact, it is possible to take the notion of neighborhood as the primary notion in the definition of a topology, as opposed to taking the notion of openness as we did in Definition A.5. More precisely, open sets are characterized as being those sets that are neighborhoods of all their points. This allows to define the notion of an open set in terms of neighborhoods. Now, given a family N ( x ) indexed by x ∈ X with the above properties (1) to (4), we may define the family</p>
               <p>which, because the N ( x ) satisfy properties (1) to (4), is a topology on X such that N ( x ) is exactly the set of neighborhoods of x with respect to this topology T .</p>
               <p>Example A.5 (Neighborhoods in metric spaces) . Let X be a metric space and x ∈ X . We will characterize the set N ( x ) ⊂ P ( X ), more precisely we show</p>
               <p>Let N be a neighborhood of x . We want to show that N is an element of the right-hand side of (A.28). By definition, there exists some open set O ⊂ N that contains the point x . Recalling what it means for a set to open in a metric space, we conclude the existence of some ε &gt; 0 such that B ε ( x ) ⊂ O ⊂ N . Conversely, if N is an element of the right-hand side of (A.28), then we may set O = B ε ( x ) in the definition of neighborhood since B ε ( x ) is open.  We will now define what it means for a sequence in X to converge to a point. Definition A.7 (Convergence) . Let X be a topological space and ( x k ) k ∈ N ∈ X N a sequence in X . We say that ( x k ) converges to the point x ∈ X if for every neighborhood N of x , there exists some integer K ∈ N such that for every k K , x k ∈ N . In this case, x is called limit point of ( x n ) and ( x n ) is called convergent . Compare this definition to the definition of convergence in metric spaces: The sequence ( x k ) converges to x if “for all ε &gt; 0 there exists some integer K ∈ N such that for every k K , x k ∈ B ε ( x )”. The generalization of Definition A.7 is that the prototypical neighborhoods B ε ( x ) were replaced by arbitrary neighborhoods of x . One fact we recall from Example A.3 is that in the case of a metric space X , every open set is the union of ε -balls. In such a case, we call the set of ε -balls a basis of the topology. More specifically, a basis B of a topology T on X is a set of subsets of X such that every open set O ∈ T is a union of elements of B . Trivially, the topology itself is always a basis. A special case occurs when there exists a countable basis B of a topology. These spaces are called second countable or AA2 spaces. Euclidean spaces R n have a basis consisting of the ε -balls. But they even are AA2 spaces since the set of balls with rational radius and rational center also form a basis of the Euclidean topology, i.e.,</p>
               <p>This set B is countable by Cantor diagonalization. AA2 spaces are spaces in which it suffices to use the term “sequence”, i.e., a mapping N → X , when talking about convergence. In other spaces it might be too restrictive for its notion of convergence that the domain N of the sequence is just countable. We would have to generalize sequences to either nets or filters , the former being mappings Λ → X where Λ is not necessarily countable, but has to carry an additional structure; that of a directed set 1 . If this is not done, popular theorems such as “A map F : X → Y is continuous if and only if for each sequence x k in 1 A directed set is a set Λ together with a preorder (reflexive and transitive relation) on Λ such that for every λ and μ in Λ, there exists a ν such that λ ν and μ ν .  X converging to x , the sequence f ( x k ) converges to f ( x )” would be plainly false. It is true in any topological space, however, if the word “sequence” is replaced by “net”. But the introduction of this generalization has a few complications attached to it. For example, it need not be the case that a net has only one limit, but it may have multiple. There even exist nets that converge to every point in the space. Nevertheless, we will not take on the endeavor to explore the theory in this direction since the spaces we will look at do not have this inconvenience. We call a topological space Hausdorff if for every two distinct points x and y , there exist neighborhoods N x and N y of x and y , respectively, such that N x ∩ N y = ∅ . Metric spaces are Hausdorff.</p>
            </sec>
            <sec>
               <title>A.2.2. Closure, Interior, Boundary, Density</title>
               <p>This subsection introduces accompanying notions for talking about topological spaces. Definition A.8 (Closure) . Let X be a topological space. A set A ⊂ X is called closed if it is the complement of an open set, i.e., if X \ A is open. The closure of a set B ⊂ X , denoted by B , is the least (with respect to set inclusion) closed set that contains B as a subset.  The first question that arises here is, of course, whether the notion of closure is indeed well-defined. To be more precise, the question is whether there does exist a least closed set that contains B for every B ⊂ X . Before we answer this question in the affirmative, we collect some simple facts: Lemma A.2. Let X be a topological space. The following assertions are true.</p>
               <p>(1) Every finite union of closed sets is closed. (2) Every intersection of closed sets is closed. (3) ∅ and X are both closed. Proof. We show (1): Let A 1 , A 2 , . . . , A n be closed sets, i.e., the X \ A j are open. With use of De Morgans law,</p>
               <p>which is open by defining property (O2). Part (2) is proved just as easy. Let A i be an arbitrary family of closed set indexed by i ∈ I . Again, De Morgans law yields</p>
               <p>and property (O1) tells that we are done. The sets ∅ and X are closed since they are open and each other’s complement.</p>
               <p>Note the duality of these assertions and the condition on open sets in the definition of a topology. It is possible to define the notion of a topology by defining what sets should be closed, as opposed to defining what sets should be open as in Definition A.5. The properties that a family of sets has to fulfill such that it appears as the family of closed sets of some topology are exactly the assertions of Lemma A.2. The question whether the notion of closure is actually well-defined for any set B ⊂ X follows from the fact every intersection of closed sets is again closed. More precisely,</p>
               <p>i.e., the closure of B is equal to the intersection of all closed sets A that contain B . The set on right-hand side of (A.32) is closed by (2) of Lemma A.2 and is of course contained in every other closed set by definition of intersection. Hence we constructed B for every B ⊂ X . For spaces in which sequences suffice to build a proper notion of convergence, in particular in AA2 spaces (see Section A.2.1), the following important characterization of closure holds:</p>
               <p>Lemma A.3. Let X be an AA2 space and B ⊂ X . Then the closure of B is equal to the set of limit points that sequences in B have, i.e.,</p>
               <p>Proof. We first prove that B is contained in the right-hand side R . It suffices to show that the right-hand side is closed and contains B as a subset. The latter claim is clear since every x ∈ B is limit of the constant sequence ( x ) k ∈ N . We show closedness by contradiction. Suppose that X \ R is not open. Then, by definition, there exists some x ∈ X \ R such that for every neighborhood N of x , we have N ∩ R = ∅ . We will construct a sequence of points in B that converges to x , deriving the desired contradiction. In a first step, we will show that there exists a sequence of points in R that converges to x and then show how this implies the claim. By hypothesis, there exists some countable basis B of the topology. Let ( N k ) k ∈ N denote the family of basis sets that contain x . It is easy to show that for every neighborhood N of x , there exists a k ∈ N such that N k ⊂ N . Now, for every k ∈ N , let x k be an arbitrary point in R ∩ i k =1 N i (which is non-empty, see above). This is a sequence of points in R that converges to x . For let N ∈ N ( x ), then there exists some K ∈ N such that N K ⊂ N . Hence for any k K , we have x k ∈ k i =1 N i ⊂ N K ⊂ N which shows x k → x . To show that the existence of a sequence in R converging to x implies the existence of a sequence in B converging to x , we take for every point x k ∈ R from the above construction a sequence ( ξ k,j ) j ∈ N ∈ B N converging to x k . These sequences exist by the definition of R . We claim that the following sequence y k converges to x . The set</p>
               <p>k i =1 N i containing x k and being open, there exists some index j k ∈ N such that for all j j k , ξ k,j ∈ i k =1 N i . We define</p>
               <p>which obviously converges to x . This is a contradiction and we have shown B ⊂ R . The converse direction R ⊂ B is much easier. Suppose that there exists some x ∈ R \ B . Because x lies in the open set X \ B , there exists some neighborhood N of x such that</p>
               <p>But this relation denies the existence of a sequence in B converging to x , which contradicts the assumption.  It is also possible to define a topological space in terms of its closure operator , i.e., the map P ( X ) → P ( X ) that takes a set A ⊂ X to its closure. We can retain the topology from the closure operator, because a set A ⊂ X is closed if and only if A = A . The axioms needed to define the family of closed sets of a topology by the above procedure are the following:</p>
               <p>Lemma A.4. Let X be a topological space and let C : P ( X ) → P ( X ) be its closure operator. The following assertions are true for any A, B ⊂ X : (1) C ( ∅ ) = ∅ (2) A ⊂ C ( A ) (3) C ( C ( A )) = C ( A ) (4) C ( A ∪ B ) = C ( A ) ∪ C ( B )</p>
               <p>Proof. (1) is clear since ∅ is a closed set. (2) and (3) are immediate consequences of the definition of closure. We will now prove (4). The inclusion C ( A ∪ B ) ⊂ C ( A ) ∪ C ( B ) is a consequence of the fact that C ( A ) ∪ C ( B ) is closed by Lemma A.2(1) and of course contains both A and B as subsets. For the other inclusion, we note that it is sufficient to show C ( A ) ⊂ C ( A ∪ B ) which is clear since A ⊂ A ∪ B .</p>
               <p>Dual to the notion of closure is that of interior, as defined next. Definition A.9 (Interior) . Let X be a topological space and A ⊂ X . We call the (with respect to set inclusion) greatest open set that is contained in A the interior of A , denoted by A ◦ .</p>
               <p>As with closure, we have to check that this is a well-defined notion, i.e., that A ◦ exists for all A ⊂ X . This follows from the formula</p>
               <p>A set A ⊂ X is open if and only if A ◦ = A . The important properties of the interior operator P ( X ) → P ( X ), A → A ◦ are: Lemma A.5. Let X be a topological space and let I : P ( X ) → P ( X ) be its interior operator. The following assertions are true for any A, B ⊂ X : (1) I ( X ) = X (2) I ( A ) ⊂ A (3) I ( I ( A )) = I ( A ) (4) I ( A ∩ B ) = I ( A ) ∩ I ( B )</p>
               <p>Proof. (1) holds because the set X is open by definition, (2) and (3) are obvious. To prove (4), we show both set inclusions. The inclusion from I ( A ∩ B ) ⊂ I ( A ) ∩ I ( B ) is true, because A ∩ B ⊂ A and A ∩ B ⊂ B . We now show I ( A ) ∩ I ( B ) ⊂ I ( A ∩ B ). The set I ( A ) ∩ I ( B ) is open and contained in both A and B , hence contained in A ∩ B . The claim now follows from the definition of interior.</p>
               <p>The notions of closure and interior are connected by the following formula</p>
               <p>where B c = X \ B denotes the complement of B ⊂ X .</p>
               <p>Example A.6 (Intervals) . We will demonstrate the use of the notions interior and closure with real intervals. The interior and the closure of an interval do not depend on whether the boundary points belong to the interval or not. More precisely, the interiors of ( a, b ), ( a, b ], [ a, b ) and [ a, b ] are all equal to ( a, b ) and their closures are all equal to [ a, b ]. It follows from the fact that ( a, b ) is open, [ a, b ] is closed and neither ( a, b ] nor [ a, b ) are open or closed.  We may generalize this situation and identify for any set A ⊂ X a set of points for which it does not matter whether they are added or removed when considering interior and closure.</p>
               <p>Definition A.10 (Boundary) . Let X be a topological space and A ⊂ X . We call ∂A = A \ A ◦ the boundary of A . We state the following observations. Lemma A.6. Let X be a topological space and let A ⊂ X . The following statements are true.</p>
               <p>(1) ∂A = { x ∈ X | for all N ∈ N ( x ) it is N ∩ A = ∅ and N ∩ A c = ∅} (2) ∂A = ∂ ( A c ) (3) ( A \ ∂A ) ◦ = A ◦ (4) A ∪ ∂A = A (5) ∂A is closed (6) A = A ∪ ∂A . In particular, A is closed if and only if ∂A ⊂ A . (7) A ◦ = A \ ∂A . In particular, A is open if and only if ∂A ∩ A = ∅ . Proof. (1): From the definition and equation (A.37), we deduce ∂A = A c ◦ c \ A ◦ = A c ◦ c ∩ A ◦ c . It hence suffices to show</p>
               <p>But this is trivially equivalent (by taking complements) to</p>
               <p>which is true, because the relation N ∩ A c = ∅ is the same as the relation N ⊂ A . (2) is a trivial consequence of (1). (3): After a simple calculation involving De Morgan’s law and R \ S = R ∩ S c , we arrive at the equation</p>
               <p>Since A ◦ ⊂ A c ∪ A ◦ and A ◦◦ = A ◦ , we get</p>
               <p>and we are done, the other inclusion being trivial. (4) follows from (2), (3) and (A.37) as the following calculation shows:</p>
               <p>(5) is clear since ∂A = A ∩ A ◦ c is an intersection of two closed sets. (6) and (7) are simple calculations.</p>
               <p>We now turn to a different notion that is derived from the notion of closure, namely density . Informally, we will call a set dense if every point in the space is arbitrarily close to a point of the dense set. The formal definition follows now.</p>
               <p>Definition A.11 (Density) . Let X be a topological space and A ⊂ X . We call A dense in X if the closure of A in X is equal to X , i.e., A = X . Equivalent statements are summarized in the next Lemma A.7. Let X be a topological space and A ⊂ X . The following statements are equivalent: (1) A is dense in X . (2) For every non-empty open set O ⊂ X it follows that A ∩ O = ∅ . (3) For every neighborhood N of a point x ∈ X it follows that A ∩ N = ∅ .</p>
               <p>Proof. The equivalence (2) ⇔ (3) is trivial. Let A be dense and suppose that (2) does not hold. Then there exists a non-empty open set O with A ⊂ O c where O c denotes X \ O . But since O c = X and O c is closed, we deduce A = X which is a contradiction. Conversely, let (2) hold and suppose that A = C = X . But then the complement of C is non-empty, open, and has trivial intersection with A . Contradiction.  Example A.7. The set Q ⊂ R is dense: Let N ⊂ R be a neighborhood of some x ∈ R . Then, by definition, there exists some ε &gt; 0 such that B ε ( x ) ⊂ N . The decimal expansion of x yields a sequence ( q k ) that converges to x . This implies that there exists some K ∈ N such that q K ∈ B ε ( x ) ⊂ N . The claim follows because, by construction, q K ∈ Q .</p>
            </sec>
            <sec>
               <title>A.2.3. Continuity</title>
               <p>In the previous sections, we studied the objects “topological spaces”. It is the purpose of this section to deal with “morphisms” of such objects, i.e., functions between topological spaces that preserve the topological structure. We will then have laid the ground to study the category of topological spaces (Herrlich and Strecker 1973).  Definition A.12 (Continuity) . Let X and Y be topological spaces. Furthermore, let f : X → Y be a function. We call f continuous if for every open set O ⊂ Y , it follows that its inverse image f − 1 [ O ] is open in X .</p>
               <p>By taking complements and recalling f − 1 [ Y \ A ] = X \ f − 1 [ A ] for all A ⊂ Y , we arrive at the insight that f is continuous if and only if f − 1 [ C ] is closed in X for every set C that is closed in Y .</p>
               <p>Example A.8. Let X be a set equipped with the discrete topology. Then every map f : X → Y is continuous, because every subset of X is open. Conversely, if Y is equipped with the trivial topology, i.e., only ∅ and Y are open, then again every map f : X → Y is continuous, because f − 1 [ ∅ ] = ∅ and f − 1 [ Y ] = X which are in any case open in X .  If Y a topological space and X is any non-empty set, we may ask ourselves with which topology we must equip X such that a given mapping f : X → Y becomes continuous. Of course, we want to do this in the most general fashion, i.e., we do not want to add too many open sets to the to be defined topology on X , just enough to make f continuous. We are obliged to have sets of the form f − 1 [ O ] where O ⊂ Y is open as open sets in X . But by recalling all those useful properties of the inverse image, it also turns out that these sets are enough: The sets of the above form f − 1 [ O ] form a topology on X with respect to which f is continuous. We call this topology on X the initial topology with respect to f : X → Y . Example A.9 (Subspace topology) . If X ⊂ Y and Y is equipped with a topology, we may consider the inclusion map ι : X → Y , i.e., ι ( x ) = x for all x ∈ X , and equip X with the initial topology with respect to ι . We call this topology on X the subspace topology inherited from Y . The open sets of X are exactly the sets X ∩ O where O ⊂ Y is open. We may well go the opposite direction and ask, given a mapping f : X → Y where X is a topological space and Y is an arbitrary non-empty set, which topology on Y makes f continuous and has the least number of open sets. This time, it turns out that indeed the sets A ⊂ Y such that f − 1 [ A ] ⊂ X is open form a topology on Y . This topology on Y is called the final topology with respect to f : X → Y . We have already seen a notion called “continuity” in Definition A.3 where we defined it for maps U → R where U ⊂ R . It would be embarrassing if those notions would not agree for maps U → R . Luckily, the following lemma holds.</p>
               <p>Lemma A.8. Let U ⊂ R and f : U → R . The following are equivalent: (1) f is continuous with respect to Definition A.3. (2) f is continuous with respect to Definition A.12 (where U is equipped with the subspace topology inherited from R ).</p>
               <p>Proof. (1) ⇒ (2): Let O ⊂ R be non-empty and open. Let x ∈ f − 1 [ O ]. Then there exists some ε &gt; 0 such that B ε ( f ( x )) ⊂ O since O is open. But now, by (1), there exists some δ &gt; 0 such that y ∈ B δ ( x ) implies f ( y ) ∈ B ε ( f ( x )), i.e.,</p>
               <p>(2) ⇒ (1): Let x ∈ U and ε &gt; 0. The set B ε ( f ( x )) is open in R . Hence also f − 1 [ B ε ( f ( x ))] is open. Hence there exists some δ &gt; 0 such that</p>
               <p>But this implies the condition of (1).</p>
            </sec>
            <sec>
               <title>A.2.4. Compactness</title>
               <p>We briefly discussed compactness in Section A.1.2 for the case of subsets of R n . Since we do not have the notion of distance and hence boundedness in general topological spaces, the idea is to take the conclusion of Theorem A.1 as the definition of compactness. Below, we collect the most important facts about compact sets. Definition A.13 (Compactness) . Let X be a topological space. We call X compact if for any collection ( A ι ) ι ∈ I of open sets for which X = A ι , there exists some n ∈ N n and ι 1 , . . . , ι n such that X = k =1 A ι k . We call a subset A of a topological space compact if A is compact with respect to the subspace topology inherited from X . Example A.10. A space equipped with the discrete topology is compact if and only if it is finite. This follows easily because all singleton sets { x } are open in discrete spaces. Lemma A.9. Let X be a topological space and A ⊂ X . The following assertions are true:</p>
               <p>(1) If X is compact and A is closed, then A is compact. (2) If X is Hausdorff and A is compact, then A is closed.</p>
               <p>Proof. (1): For any open cover ( A ι ) of A , the family ( A ι ) together with the open set X \ A is an open cover of X . Since X is compact, there exist finitely many indices ι 1 , . . . , ι n such that X = n k =1 A ι k ∪ ( X \ A ). By intersecting both sides with A , we n arrive at A = k =1 A ι k which shows that A is compact. (2): We show that the complement of A is open. Let x ∈ X \ A . By the Hausdorff property, for every y ∈ A , there exist disjoint open sets U ( y ) and V ( y ) such that y ∈ U ( y ) and x ∈ V ( y ). The family U ( y ) where y ∈ A is an open covering of n A . Because A is compact, there exist y 1 , . . . , y n such that A = i =1 U ( y i ). Setting n V = i =1 V ( y i ) reveals that V is an open neighborhood of x ((O2) in Definition A.5) which is disjoint to A , hence V ⊂ X \ A . This proves that X \ A is open, i.e., A is closed.</p>
               <p>Lemma A.10. Let f : X → Y be continuous and let X be compact. Then f [ X ] is compact. Proof. Follows immediately from the definitions. Lemma A.11. Let X be a compact space and f : X → R continuous. Then f attains its minimum, i.e., there exists some x ∈ X such that</p>
               <p>Proof. By Lemma A.10, the image f [ X ] is a compact set in R . By Lemma A.9, this set is closed. We may deduce the result from order completeness of R . 2 2 For every set B ⊂ R which is bounded from below, the infimum of B exists in R .</p>
            </sec>
            <sec>
               <title>A.2.5. Product Spaces</title>
               <p>In this section, we will answer the question, which topology is “natural” to equip a product space X = X ι with when all X ι are topological spaces. We do this by considering the projection mappings π ι : X X ι and equipping X with a slight generalization of the initial topology. Namely, we will have to make all projection mapping continuous, not only one. This is done with the following</p>
               <p>Definition A.14. Let ( X ι ) be a family of topological spaces and let X = X ι be the set-theoretic product. We call the topology induced by sets of the form π ι − 1 [ O ] where O ⊂ X ι is open the product topology on X .</p>
               <p>A most important result that we do not prove here for space limitations is the following (Bourbaki 1989, Chapter I, § 9, no. 5, Theorem 3).</p>
               <p>Theorem A.3 (Tychonoff) . Let X = X ι be equipped with the product topology. The following are equivalent: (1) X is compact. (2) All X ι are compact.</p>
            </sec>
         </sec>
      </sec>
      <sec>
         <title>Bibliography</title>
         <p>Alpern, Bowen and Fred B. Schneider. Defining liveness. Technical Report TR85-650, Cornell University, 1985.</p>
         <p>Attiya, Hagit and Jennifer Welch. Distributed Computing: Fundamentals, Simula- tions, and Advanced Topics . John Wiley &amp; Sons, second edition, 2004. Bourbaki, Nicolas (pseudonym). General Topology, Chapters 1–4 . Elements of Math- ematics. Springer, 1989. Charron-Bost, Bernadette, Sam Toueg, and Anindya Basu. Revisiting safety and liveness in the context of failures. In Proceedings of CONCUR 2000 —Concurrency Theory , pages 552–565. Springer, 2000.</p>
         <p>Dolev, Danny, Cynthia Dwork, and Larry Stockmeyer. On the minimal synchronism needed for distributed consensus. Journal of the ACM 34 (1):77–97, 1987.</p>
         <p>Fich, Faith and Eric Ruppert. Hundreds of impossibility results for distributed computing. Distributed Computing 16 (2):121–163, 2003. Fischer, Michael J., Nancy A. Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM 32 (2):374– 382, 1985. Hatcher, Allan. Algebraic Topology . Cambridge University Press, 2002. Herlihy, Maurice and Sergio Rajsbaum. Algebraic spans. Mathematical Structures in Computer Science 10 (4):549–573, 2000. Herlihy, Maurice and Nir Shavit. The asynchronous computability theorem for t resilient tasks. In Proceedings of the 25th Annual ACM Symposium on Theory of Computing , pages 111–120. 1993. Herrlich, Horst and George E. Strecker. Category Theory: An Introduction . Allyn and Bacon, 1973. Lamport, Leslie. Proving the correctness of multiprocess programs. IEEE Transac- tions on Software Engineering SE-3 (2):125–143, 1977.</p>
         <p>Lubitch, Ronit and Shlomo Moran. Closed schedulers: A novel technique for ana- lyzing asynchronous protocols. Distributed Computing 8 (4):203–210, 1995.</p>
         <p>Lynch, Nancy A. Distributed Algorithms . Morgan Kaufmann, 1996.</p>
         <p>Moses, Yoram and Sergio Rajsbaum. A layered analysis of consensus. SIAM Journal on Computing 31 (4):989–1021, 2002. Saks, Michael and Fotios Zaharoglou. Wait-free k -set agreement is impossible: The topology of public knowledge. SIAM Journal on Computing 29 (5):1449–1483, 2000. Santoro, Nicola and Peter Widmayer. Time is not a healer. In Proceedings of the 6th Annual Symposium on Theoretical Aspects of Computer Science , pages 304–313. Springer, 1989. Spanier, Edwin H. Algebraic Topology . McGraw-Hill, 1966.</p>
      </sec>
   </body>
   <back/>
</article>