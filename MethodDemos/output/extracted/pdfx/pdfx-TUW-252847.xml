<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>2bb329ea8111408fb9421b743bdc974337f5aac7a7c1f7575632d7df7860cae0</job>
    <base_name>l4t</base_name>
    <doi>http://dx.doi.org/10.1007/978-3-319-64689-3_23</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="1">Retrieving Diverse Social Images at MediaEval 2016: Challenge, Dataset and Evaluation</article-title>
      </title-group>
      <region class="unknown" id="6">Bogdan Ionescu Alexandru Lucian Gînsc a  ̆ LAPI, University Politehnica of CEA, LIST, France Bucharest, Romania <email id="2">alexandru.ginsca@cea.fr</email> <email id="3">bionescu@alpha.imag.pub.ro</email> Bogdan Boteanu Mihai Lupu LAPI, University Politehnica of Vienna University of Bucharest, Romania Technology, Austria <email id="4">bboteanu@alpha.imag.pub.ro</email> <email id="5">lupu@ifs.tuwien.ac.at</email></region>
      <abstract class="DoCO:Abstract" id="7">This paper provides an overview of the Retrieving Diverse Social Images task that is organized as part of the MediaEval 2016 Benchmarking Initiative for Multimedia Evaluation. The task addresses the problem of result diversification in the context of social photo retrieval where images, metadata, text information, user tagging profiles and content and text models are available for processing. We present the task challenges, the proposed data set and ground truth, the required participant runs and the evaluation metrics.</abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="8" page="1" column="1">1. INTRODUCTION</h1>
      </section>
      <region class="DoCO:TextChunk" id="24" page="1" column="1">An efficient image retrieval system should be able to present results that are both relevant and that are covering different aspects, i.e., diversity , of the query. By diversifying the pool of possible results, one can increase the likelihood of providing the user with the information needed. Relevance was more thoroughly studied in existing literature than diversification [<xref ref-type="bibr" rid="R1" id="9" class="deo:Reference">1</xref>, <xref ref-type="bibr" rid="R2" id="10" class="deo:Reference">2</xref>, <xref ref-type="bibr" rid="R3" id="11" class="deo:Reference">3</xref>], especially within the text community. Even though a considerable amount of diversification literature exists [<xref ref-type="bibr" rid="R8" id="12" class="deo:Reference">8</xref>, <xref ref-type="bibr" rid="R9" id="13" class="deo:Reference">9</xref>, <xref ref-type="bibr" rid="R10" id="14" class="deo:Reference">10</xref>], the topic remains important, especially in the emerging fields of social multimedia [<xref ref-type="bibr" rid="R4" id="15" class="deo:Reference">4</xref>, <xref ref-type="bibr" rid="R5" id="16" class="deo:Reference">5</xref>, <xref ref-type="bibr" rid="R6" id="17" class="deo:Reference">6</xref>, <xref ref-type="bibr" rid="R7" id="18" class="deo:Reference">7</xref>, <xref ref-type="bibr" rid="R11" id="19" class="deo:Reference">11</xref>]. The 2016 Retrieving Diverse Social Images task is a fo- llowup of the 2015 edition [<xref ref-type="bibr" rid="R14" id="20" class="deo:Reference">14</xref>, <xref ref-type="bibr" rid="R13" id="21" class="deo:Reference">13</xref>, <xref ref-type="bibr" rid="R12" id="22" class="deo:Reference">12</xref>, <xref ref-type="bibr" rid="R15" id="23" class="deo:Reference">15</xref>] and aims to foster new technology to improve both relevance and diversification of search results with explicit emphasis on the actual social media context . The task was designed to support evaluation of techniques emerging from a wide range of research fields, such as image retrieval (text, vision, multimedia communities), machine learning, relevance feedback and natural language processing, but not limited to these.</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="25" page="1" column="1">2. TASK DESCRIPTION</h1>
      </section>
      <region class="DoCO:TextChunk" id="26" page="1" column="1">The task is built around the use case of a general ad-hoc image retrieval system, which provides the user with diverse representations of the queries (see for instance Google Image ∗ This task is partly supported by the Vienna Science and Technology Fund (WWTF) through project ICT12-010.</region>
      <region class="unknown" id="27" page="1" column="1">Copyright is held by the author/owner(s). MediaEval 2016 Workshop, October 20-21, 2016, Hilversum, Netherlands.</region>
      <region class="DoCO:TextChunk" id="30" confidence="possible" page="1" column="2">Maia Zaharieva ∗ University of Vienna &amp; Vienna University of Technology, Austria <email id="28">maia.zaharieva@univie.ac.at</email> Henning Müller HES-SO, University of Applied Sciences Western Switzerland <email id="29">henning.mueller@hevs.ch</email></region>
      <region class="DoCO:TextChunk" id="31" page="1" column="2">Search 1 ). Participants are required, given a ranked list of query-related photos retrieved from Flickr 2 , to refine the results by providing a set of images that are at the same time relevant to the query and to provide a diversified summary of it. Compared to the previous editions, this year’s task includes complex and general-purpose multi-concept queries. The requirements of the task are to refine these results by providing a ranked list of up to 50 photos that are both relevant and diverse representations of the query, according to the following definitions: Relevance : a photo is considered to be relevant for the query if it is a common photo representation of the query topics (all at once). Bad quality photos (e.g., severely blurred, out of focus, etc.) are not considered relevant in this sce- nario; Diversity : a set of photos is considered to be diverse if it depicts different visual characteristics of the query topics and subtopics with a certain degree of complementarity, i.e., most of the perceived visual information is different from one photo to another. To carry out the refinement and diversification tasks, participants may use the social metadata associated with the images, the visual characteristics of the images, information related to user tagging credibility (an estimation of the global quality of tag-image content relationships for a user’s contributions) or external resources (e.g., the Internet).</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="32" page="1" column="2">3. DATASET</h1>
      </section>
      <region class="DoCO:TextChunk" id="39" page="1" column="2">The 2016 data consists of a development set ( devset ) containing 70 queries (20,757 Flickr photos — including 35 multi-topic queries related to events and states associated with locations from the 2015 dataset [ <xref ref-type="bibr" rid="R14" id="33" class="deo:Reference">14</xref>]), a user annotation credibility set ( credibilityset ) containing information for ca. 300 location-based queries and 685 users (different than the ones in devset and testset — updated version of the 2015 dataset [<xref ref-type="bibr" rid="R14" id="34" class="deo:Reference">14</xref>]), a set providing semantic vectors for general English terms computed on top of the English Wikipedia 3 ( wikiset ), which could help the participants in developing advanced text models, and a test set ( testset ) containing 65 1 https://images.google.com/ . 2 https://www.flickr.com/ . 3 https://en.wikipedia.org/ .<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> queries (19,017 Flickr photos). Each query is provided with the following information: query text formulation (the actual query formulation used on Flickr to retrieve all the data), a ranked list of up to 300 photos in jpeg format retrieved from Flickr using Flickr’s default “relevance” algorithm (all photos are Creative Com- mons licensed allowing redistribution 4 ), an xml file containing metadata from Flickr for all the retrieved photos (e.g., photo title, photo description, photo id, tags, Creative Com- mon license type, the url link of the photo location from Flickr, the photo owner’s name, user id, the number of times the photo has been displayed, etc), and ground truth for both relevance and diversity. Apart from the metadata, to facilitate participation from various communities, we also provide the following content descriptors: - convolutional neural network based descriptors — generic CNN based on the reference convolutional neural network (CNN) model provided along with the Caffe framework 5 (this model is learned with the 1,000 ImageNet classes used during the ImageNet challenge); and an adapted CNN based on a CNN model obtained with an identical architecture to that of the Caffe reference model. Adaptation is done only for the 2015 location-based multi-topic queries (35 queries from the devset), i.e., the model is learned with 1,000 tourist points of interest classes of which the images were automati- cally collected from the Web [<xref ref-type="bibr" rid="R16" id="36" class="deo:Reference">16</xref>]. For the other queries, the descriptor is computed as the generic one, because queries are diverse enough and do not require any adaptation; - text information that consists as in the previous edition of term frequency information, document frequency information and their ratio, i.e., TF-IDF, which is computed on per image basis, per query basis and per user basis (see [<xref ref-type="bibr" rid="R17" id="37" class="deo:Reference">17</xref>]); - user annotation credibility descriptors that give an au- tomatic estimation of the quality of the users’ tag-image content relationships. These descriptors are extracted by visual or textual content mining: visualScore (measure of user image relevance), faceProportion (the percentage of images with faces), tagSpecificity (average specificity of a user’s tags, where tag specificity is the percentage of users hav- ing annotated with that tag in a large Flickr corpus), lo- cationSimilarity (average similarity between a user’s geo- tagged photos and a probabilistic model of a surrounding cell), photoCount (total number of images a user shared), uniqueTags (proportion of unique tags), uploadFrequency (average time between two consecutive uploads), bulkPro- portion (the proportion of bulk taggings in a user’s stream, i.e., of tag sets that appear identical for at least two distinct photos), meanPhotoViews (mean value of the number of times a user’s image has been seen by other members of the community), meanTitleWordCounts (mean value of the number of words found in the titles associated with users’ photos), meanTagsPerPhoto (mean value of the number of tags users put for their images), meanTagRank (mean rank of a user’s tags in a list in which the tags are sorted in de- scending order according the the number of appearances in a large subsample of Flickr images), and meanImageTagClar- ity (adaptation of the Image Tag Clarity from [<xref ref-type="bibr" rid="R18" id="38" class="deo:Reference">18</xref>] using as individual tag language model a tf/idf language model).</region>
      <region class="DoCO:TextChunk" id="42" confidence="possible" page="2" column="1">4 <ext-link ext-link-type="uri" href="http://creativecommons.org/" id="40">http://creativecommons.org/</ext-link> . 5 <ext-link ext-link-type="uri" href="http://caffe.berkeleyvision.org/" id="41">http://caffe.berkeleyvision.org/</ext-link> .</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="43" page="2" column="2">4. GROUND TRUTH</h1>
        <region class="DoCO:TextChunk" id="44" page="2" column="2">Both relevance and diversity annotations were carried out by expert annotators. For relevance , annotators were asked to label each photo (one at a time) as being relevant (value 1), non-relevant (0) or with “don’t know” (-1). For devset , 9 annotators were involved, for credibilityset 9 and for testset 8. The data was partitioned among annotators such that in the end each image has been marked by 3 different annotators. The final relevance ground truth was determined after a lenient majority voting scheme. For diversity , only the photos that were judged as relevant in the previous step were considered. For each query, annotators were provided with a thumbnail list of all relevant photos. After getting fa- miliar with their contents, they were asked to re-group the photos into clusters with similar visual appearance (up to 25). Devset and testset were annotated by 5 persons, each of them annotating distinct parts of the data (leading to only one annotation). An additional annotator acted as a master annotator and reviewed once more the final annotations.</region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="45" page="2" column="2">5. RUN DESCRIPTION</h1>
        <region class="DoCO:TextChunk" id="46" page="2" column="2">Participants were allowed to submit up to 5 runs. The first 3 are required runs : run1 — automated using visual information only; run2 — automated using text information only; and run3 — automated using text-visual fused without other resources than provided by the organizers. The last 2 runs are general runs : run4 and run5 — every- thing allowed, e.g., human-based or hybrid human-machine approaches, including using data from external sources (e.g., Internet). For generating run1 to run3 participants are allowed to use only information that can be extracted from the provided data (e.g., provided descriptors, descriptors of their own, etc).</region>
      </section>
      <section class="deo:Evaluation">
        <h1 class="DoCO:SectionTitle" id="47" page="2" column="2">6. EVALUATION</h1>
        <region class="DoCO:TextChunk" id="54" page="2" column="2">Performance is assessed for both diversity and relevance. The following metrics are computed: Cluster Recall at X (<email id="48">CR@X</email>) — a measure that assesses how many different clusters from the ground truth are represented among the top X results (only relevant images are considered), Precision at X (<email id="49">P@X</email>) — measures the number of relevant photos among the top X results and F1-measure at X (<email id="50">F1@X</email>) — the har- monic mean of the previous two. Various cut off points are to be considered, i.e., X=5, 10, 20, 30, 40, 50. Official rank- ing metric is the <email id="51">F1@20</email> which gives equal importance to diversity (via <email id="52">CR@20</email>) and relevance (via <email id="53">P@20</email>). This metric simulates the content of a single page of a typical Web image search engine and reflects user behavior, i.e., inspect- ing the first page of results with priority.</region>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="55" page="2" column="2">7. CONCLUSIONS</h1>
        <region class="DoCO:TextChunk" id="56" page="2" column="2">The 2016 Retrieving Diverse Social Images task provides participants with a comparative and collaborative evaluation framework for social image retrieval techniques with explicit focus on result diversification . This year in particu- lar, the task explores the diversification in the context of a challenging, ad-hoc image retrieval system, which should be able to tackle complex and general-purpose multi-concept queries. Details on the methods and results of each individual participant team can be found in the working note papers of the MediaEval 2016 workshop proceedings.</region>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="57" page="3" column="1">8. REFERENCES</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="58" page="3" column="1">[1] A.W.M. Smeulders, M. Worring, S. Santini, A. Gupta, R. Jain, “Content-based Image Retrieval at the End of the Early Years”, IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(12), pp. 1349 - 1380, 2000.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="59" page="3" column="1">[2] R. Datta, D. Joshi, J. Li, J.Z. Wang, “Image Retrieval: Ideas, Influences, and Trends of the New Age”, ACM Computing Surveys, 40(2), pp. 1-60, 2008.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="60" page="3" column="1">[3] R. Priyatharshini, S. Chitrakala, “Association Based Image Retrieval: A Survey”, Mobile Communication and Power Engineering, Springer Communications in Computer and Information Science, 296, pp. 17-26, 2013.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="61" page="3" column="1">[4] R.H. van Leuken, L. Garcia, X. Olivares, R. van Zwol, “Visual Diversification of Image Search Results”, ACM World Wide Web, pp. 341-350, 2009.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="62" page="3" column="1">[5] M.L. Paramita, M. Sanderson, P. Clough, “Diversity in Photo Retrieval: Overview of the ImageCLEF Photo Task 2009”, ImageCLEF 2009.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="63" page="3" column="1">[6] B. Taneva, M. Kacimi, G. Weikum, “Gathering and Ranking Photos of Named Entities with High Precision, High Recall, and Diversity”, ACM Web Search and Data Mining, pp. 431-440, 2010.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="64" page="3" column="1">[7] S. Rudinac, A. Hanjalic, M.A. Larson, “Generating Visual Summaries of Geographic Areas Using Community-Contributed Images”, IEEE Transactions on Multimedia, 15(4), pp. 921-932, 2013.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="65" page="3" column="1">[8] R. Agrawal, S. Gollapudi, A. Halverson, S. Ieong, “Diversifying Search Results”, ACM International Conference on Web Search and Data Mining, pp. 5-14, 2009.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="66" page="3" column="1">[9] Y. Zhu, Y. Lan, J. Guo, X. Cheng, S. Niu, “Learning for Search Result Diversification”, ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 293-302, 2014.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="67" page="3" column="1">[10] H.-T. Yu, F. Ren, “Search Result Diversification via Filling up Multiple Knapsacks”, ACM International Conference on Conference on Information and Knowledge Management, pp. 609-618, 2014.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="68" page="3" column="1">[11] D.-T. Dang-Nguyen, L. Piras, G. Giacinto, G. Boato, F.G.B. De Natale, “A Hybrid Approach for Retrieving Diverse Social Images of Landmarks”, IEEE International Conference on Multimedia and Expo, pp. 1-6, 2015.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="69" page="3" column="1">[12] B. Ionescu, A.-L. Radu, M. Menéndez, H. Müller, A. Popescu, B. Loni, “Div400: A Social Image Retrieval Result Diversification Dataset”, ACM MMSys, Singapore, 2014.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="70" page="3" column="1">[13] B. Ionescu, A. Popescu, M. Lupu, A.L. Gˆ ınsc a, B. Boteanu, H. Müller, “Div150Cred: A Social Image Retrieval Result Diversification with User Tagging Credibility Dataset”, ACM MMSys, Portland, Oregon, USA, 2015.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="71" page="3" column="1">[14] B. Ionescu, A.L. Gˆ ınsc a, B. Boteanu, M. Lupu, A. Popescu, H. Müller, “Div150Multi: A Social Image Retrieval Result Diversification Dataset with Multi-topic Queries”, ACM MMSys, Klagenfurt, Austria, 2016.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="73" page="3" column="1">[15] B. Ionescu, A. Popescu, A.-L. Radu, H. Müller, “Result Diversification in Social Image Retrieval: A Benchmarking Framework”, Multimedia Tools and <marker type="column" number="2"/><marker type="block"/> Applications, 2014.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="74" page="3" column="2">[16] E. Spyromitros-Xioufis, S. Papadopoulos, A. Gˆ ınsc a, A. Popescu, I. Kompatsiaris, I. Vlahavas, “Improving Diversity in Image Search via Supervised Relevance Scoring”, ACM International Conference on Multimedia Retrieval, ACM, Shanghai, China, 2015.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="76" page="3" column="2">[17] B. Ionescu, A. Popescu, M. Lupu, A.L. Gˆ ınsc a, H. Müller, “Retrieving Diverse Social Images at MediaEval 2014: Challenge, Dataset and Evaluation”, CEUR-WS, Vol. 1263, <ext-link ext-link-type="uri" href="http://ceur-ws.org/" id="75">http://ceur-ws.org/</ext-link> Vol-1263/mediaeval2014_submission_1.pdf , Spain, 2014.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="77" page="3" column="2">[18] A. Sun, S.S. Bhowmick, “Image Tag Clarity: in Search of Visual-Representative Tags for Social Images”, SIGMM workshop on Social media, 2009.</ref>
        </ref-list>
      </section>
    </body>
  </article>
</pdfx>
