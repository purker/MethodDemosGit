<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000150">
<note confidence="0.956991">
A FORMAL METHOD FOR SELECTING EVALUATION METRICS FOR IMAGE
SEGMENTATION.
</note>
<author confidence="0.950548">
Abdel Aziz Taha, Allan Hanbury
</author>
<affiliation confidence="0.988225">
Vienna University of Technology
</affiliation>
<sectionHeader confidence="0.893302" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.996331142857143">
Evaluating the quality of segmentations is an important
process in image processing, especially in the medical
domain. Many evaluation metrics have been used in
evaluating segmentation. There exists no formal way
to choose the most suitable metric(s) for a particular
segmentation task and/or particular data. In this pa-
per we propose a formal method for choosing the most
suitable metrics for evaluating the quality of segmenta-
tions with respect to ground truth segmentations. The
proposed method depends on measuring the bias of
metrics towards/against the properties of the the seg-
mentations being evaluated. We firstly demonstrate
how metrics can have bias towards/against particular
properties and then we propose a general method for
ranking metrics according to their overall bias. We fi-
nally demonstrate for 3D medical image segmentations
that ranking produced using metrics with low overall
bias strongly correlate with manual rankings done by
an expert.
Index Terms— image segmentation; evaluation met-
rics; selection
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.747406">
1.1 The need to understand metrics: Many evaluation
</subsectionHeader>
<bodyText confidence="0.9998964">
metrics for image segmentation have been introduced;
most researchers choose the evaluation metrics arbitrar-
ily or according to their popularity. Investigating met-
rics would help researchers to better understand them
and help companies and stakeholders to save effort and
time reaching optimal systems [1]. A poorly defined
metric may lead to inaccurate conclusions like selecting
suboptimal models when comparing the performance
of classifiers [2].
Many researchers have investigated the drawbacks
of particular metrics given particular properties of the
data being classified. As a special case of classification,
image segmentation is also affected by these draw-
backs. The following are some examples: Hausdorff
distance is very sensitive to noise and least squares
</bodyText>
<note confidence="0.900754">
Oscar A. Jimenez del Toro
Univ. of Applied Sciences Western Swizerland
</note>
<bodyText confidence="0.995253211267606">
based evaluation methods are very sensitive to out-
liers [3]. Mutual information doesn’t utilize spatial
information inherited in images because only voxel
relationships are considered but not the neighbor-
hoods [4]. Information theoretical measures have a
non-convergent baseline which depends on the ratio
between the number of data points and the number of
classes. Therefore this class of measure needs chance
correction [5]. Commonly used measures (precision,
recall and F-measures) are biased and don’t consider
the level of chance [6]. Choosing evaluation metrics
is very important and application-dependent; when
evaluating imbalanced datasets, the metric choice is
not obvious [2]. Metrics have different properties with
respect to their correlation with user satisfaction crite-
ria and their ease of interpretation [7]. Benhabiles et
al. [8] validated 250 automatic segmentations against
their corresponding ground truth segmentations using
four different evaluation metrics. The results were then
compared with manual ratings from 40 human ob-
servers. They found that the correlations between the
ranking based on the manual ratings and the rankings
based on the evaluation metrics vary between 30% and
80% depending on the used metric.
Research in the last decades generally results in
the relative system improvement achieved becoming
smaller and smaller. As a result, sensitivity and fi-
delity of evaluation metrics become increasingly criti-
cal. When improvements are small, metrics with high
sensitivity are needed to measure small but real im-
provements and also with high fidelity to distinguish
between improvements based on user preferences and
improvements resulting from biased relevance judg-
ments [9] [10].
1.2 Problem definition and notations: In this paper,
we propose a formal method for selecting the most suit-
able metrics to evaluate image segmentation depend-
ing on the data being segmented and the goal of the
segmentation task. The method is primarily based on
two facts: the first is that effectiveness metrics can be
biased towards or against properties of the images be-
ing segmented, meaning that particular metrics over-
978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014
penalize or over-reward segmentations given particu-
lar properties [4] [6] [11] [2] [3]. The second fact is
that selecting the best evaluation metrics can be sub-
ject to the segmentation goal which means that the bias
towards/against a particular property of the data can
be differently important depending on the segmenta-
tion goal [8] [7]. To meet the context dependency, the
proposed method allows individual weighting of the
influence of each property according to its importance
in case this is known, which increases the effectiveness
of the method.
The problem to be solved in this paper can be
formulated as follows: given a set of metrics M =
{M1, M2, ..., Mr}, a set of image segmentations C =
{C1, C2, ...., Ck}, then the task is to rank the metrics in
M according to their suitability for evaluating the qual-
ity of the segmentations in C provided that for each
segmentation there exists a ground truth segmentation.
The proposed method is general and can be applied
to select evaluation metrics for all types of segmenta-
tions. However, for simplicity, we will consider only the
crisp segmentation task in this paper to present and for-
mulate the method. In particular, we will be analyzing
and testing the method using a special type of segmen-
tation, namely medical volume segmentation e.g. mag-
netic resonance images (MRI) where voxels (3D pixels)
are either assigned or not to a given class (segment) e.g.
an organ or a tumor.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999990181818182">
Jin et al. [12] established a formal method for compar-
ing two different measures and introduced two criteria
for formal comparison of the goodness of evaluation
metrics, namely the degree of consistency (DoC) and
degree of discriminancy (DoD). Applying these criteria,
they showed theoretically and empirically that AUC is
a better measure than accuracy in evaluating the perfor-
mance of classifiers. [13] [14] applied formal constraints
based on axiometry to compare and judge evaluation
metrics depending on the grade of satisfaction of these
constraints. Busin et al. [15] used axiometrics to define
a formal and general notation that fits any effectiveness
metric. Based on this notation, they proposed several
axioms that should be satisfied by an effectiveness met-
ric. They used these axioms as criteria to evaluate met-
rics. All these papers deal with the problem only from
a theoretical axiometrical point of view without taking
into account the classification goal and the nature and
properties of data being classified.
Sakai [11] proposed a method for evaluating evalu-
ation metrics by measuring their sensitivity using Boot-
strap Hypothesis Tests, and used this method in com-
paring seven evaluation metrics. They negate the belief
that commonly used evaluation measures are equally
reliable. Fatourechi et al. [2] proposed a framework
based on Desired Region of Operation (DROP) for se-
lecting the best evaluation metric for evaluating imbal-
anced classifications. Sakai [16] provided comparisons
between metrics depending on the sensitivity and sta-
bility using the Voorhees/Buckley swap method [17].
All these papers lack generality because they are meth-
ods designed either for specific metrics or for specific
metric properties.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="method">
3 Proposed Methods
</sectionHeader>
<bodyText confidence="0.999961944444445">
We propose a method for choosing the most suitable
metric for evaluating image segmentation. In Sec-
tions 3.1 to 3.3, the method is described and discussed
formally. Then this formal description is explained in a
step by step demonstration with a real example in Sec-
tion 4, which also provides an experimental evaluation
of the method.
Given a set of effectiveness metrics M and a set of
segmentations C, each of the segmentations is evalu-
ated against its ground truth segmentation using all
metrics to obtain a ranking per metric. Now, choos-
ing the most suitable metric goes in two main steps:
(i) Constructing different partitions on the segmenta-
tion set C and ranking the subsets of each partition ac-
cording to their average quality regarding each metric.
(ü) Inferring the metric bias from the rank correlations
across all partitions and all metrics. In the following,
each of the steps is described in more details.
</bodyText>
<subsectionHeader confidence="0.99105">
3.1 Constructing partitions and rank structure:
</subsectionHeader>
<listItem confidence="0.998253777777778">
1. For each metric m ∈ M, evaluate each of the segmen-
tations x ∈ C against its ground truth segmentation
to get the score matrix s where s(x, m) is the score of
segmentation x measured by metric m.
2. For each metric m ∈ M, assign each segmentation
x ∈ C a rank depending on its score to get the rank
matrix r where r(x, m) is the rank of segmentation x
measured by metric m.
3. Define a set F of t segmentation properties. These
</listItem>
<bodyText confidence="0.983698222222222">
can be any features thought to impact metrics e.g.
class imbalance, number of segments, segment
size, noise, deviation, shape signatures, sphereness,
boundary smoothness, resolution, moments, etc.
Furthermore, features can also be score-dependent
e.g. precision and recall for utilizing trade-off i.e. for
evaluation that tends to reward precision on cost of
recall and vice versa. If no features are known to
impact metrics, simply use all available features.
</bodyText>
<note confidence="0.529718">
933 ICIP 2014
</note>
<bodyText confidence="0.861918125">
4. Construct t different partitions on the segmentation
set C, each partition according to one feature from
F, i.e. according to the grade of occurrence of the
feature in the segmentations. One gets the set of par-
titions P = {P1, .., Pt}. Each partition should have
the same number of subsets s. The function Pij(x)
assigns the segmentation x to the subset j according
to partition i.
</bodyText>
<equation confidence="0.93280375">
~
1 x ∈ subset j of partition i
Pij(x) = (1)
0 otherwise
</equation>
<listItem confidence="0.8036875">
5. Construct t random partitions Pˇ = { ˇP1, .., ˇPt} by ran-
domly assigning segmentations to s equal subsets in
</listItem>
<bodyText confidence="0.975380333333333">
ˇPij(x) that assigns a seg-
mentation x to the subset j of the random partition i
is defined by
</bodyText>
<equation confidence="0.8680762">
�
1 x ∈ subset j of random partition i
ˇPij(x) =
0 otherwise
(2)
</equation>
<listItem confidence="0.919138666666667">
6. For each metric m ∈ M, for each partition i ∈ P, rank
the subsets j according to the average of the individ-
ual ranks in each subset using the rank function
</listItem>
<equation confidence="0.771285">
r(m, x))/nij (3)
</equation>
<bodyText confidence="0.999356384615384">
where x ∈ C are the individual segmentations and
nij is the number of segmentations in the subset j of
Partition i. Now, use the rank averages from Equa-
tion 3 to compute the rank structure R = R(i, j, m)
that gives the rank of subset j of partition i mea-
sured by metric m according to descending rank av-
erage. Analogously, Rˇ = ˇR(i, j, m) gives the rank of
subset j of the random partition i measured by met-
ric m. Note that ranking the subsets using the aver-
ages of the individual ranks in each subset is a rank-
ing method inspired by the Mann-Whitney-Wilcoxon
(MWW) test [18]. This is because straightforwardly
computing the ranks from score averages is sensitive
to outliers and may produce unreasonable rankings
if the scores are not normally distributed [19].
3.2 Inferring metric bias: Now, the rank structures Rˇ
(rankings of the random partitions) and R (rankings
of the non-random partitions) provide a statistical ba-
sis to infer metric bias by analyzing how rankings of
the different metrics and different partitions are corre-
lated. The analysis is primarily based on comparing
two correlations: the average of the rank correlations
given the random partitions Rˇ (we will call this corre-
lation the base correlation ˇK) and the rank correlation
given a particular partition R (we will call this correla-
tion the biased correlation K). They are given by
</bodyText>
<equation confidence="0.999767">
ˇK(mt) = 1
|ˇP|.|M |i∈∑ Pˇ∑ r[ ˇR(i, ., mt), ˇR(i, ., m)] (4)
m∈M
1
K(mt, p) = |M |∑ r[R(p, ., mt), R(p, ., m)] (5)
</equation>
<bodyText confidence="0.9999475">
where mt is the metric being evaluated, p is a given
partition, and r(x1, x2) is the Pearson’s correlation co-
efficient between the rankings x1 and x2 (the point de-
notes all possible values, e.g. R(p, ., m) means all pos-
sible subset ranks in partition p measured by metric m)
Now, we define the overall bias of metric mt to be
the average of the absolute correlation change B(mt)
which is given by
</bodyText>
<equation confidence="0.986343">
1
B(mt) = |P |∑ abs[K(mt, i) − ˇK(mt)] (6)
i∈P
</equation>
<bodyText confidence="0.995956714285714">
Finally, the metrics in M are ranked according to
their overall bias, where the metric with the lowest bias
is the most suitable.
3.3 Discussion: To understand the key idea, let’s
think about the following two cases: Case 1, partition-
ing the segmentations randomly. Case 2, partitioning
the segmentations according to a particular property
(e.g. class imbalance in the underlying segmentations).
Given a particular metric m, the base correlation ˇK(m)
given by Equation 4 (related to Case 1) depends on the
nature of the metric and is not affected by the proper-
ties of the segmentations, since the partition is random.
Now, if this correlation changes when we consider
Case 2 (i.e. biased correlation K given by Equation 5),
then the change is caused by the impact of the property
used for partitioning (in this case class imbalance) on
the metric and therefore it characterizes the bias of the
metric towards/against this property. If many parti-
tions (many properties) are used, then the sum of the
correlation differences is a measure of the overall bias
for the given metric.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998602">
In this section, the proposed method is demonstrated
and tested with a real example, namely a set of 229 au-
tomatic brain tumor segmentations (MRI 3D volumes)
from the BRATS2012 challenge1. The segmentations
correspond to 47 medical cases and were produced by
</bodyText>
<equation confidence="0.690732571428571">
1MICCAI 2012 Challenge on Multimodal Brain Tumor Segmenta-
tion, www2.imm.dtu.dk/projects/BRATS2012
each partition. The function
smi(j) = ( ∑
Pij(x)=1
m∈M
934 ICIP 2014
</equation>
<bodyText confidence="0.987829772727273">
five different algorithms participating in BRATS chal-
lenge. To build the rank structure (described in Sec-
tion 3.1), all segmentations were evaluated against their
ground truth segmentations using 18 metrics (listed in
Table 1) to get the score matrix s (Step 1). Then global
ranks were calculated from scores to get a ranking per
metric r (Step 2). A set of 7 properties, namely seg-
ment size, noise, class imbalance, connected compo-
nent count, point variance, sphereness, and recall, was
defined (Step 3). Now, 7 partitions of the segmentations
were constructed each time using one of the defined
properties. Each consists of 10 subsets with 229
10 ≈ 22
segmentations (Step 4). A random partition with 10
equal subsets was constructed (Step 5). For each par-
tition, the subsets were ranked using the sum of indi-
vidual global ranks r to get 18 rankings per partition
(126 rankings in total). The random partition was also
ranked to get 18 rankings (Step 6). To infer metric bias,
Equations 4, 5, and 6 in Section 3.2 were applied to
the resulting rank structure. The result of this step is a
metric list ranked according to bias.
To validate the suitability ranking produced by the
proposed approach, a manual ranking done by a ra-
diology expert was used: for each medical case, the
five corresponding segmentations were ranked by their
quality from a medical point of view (we call these the
manual rankings). Analogously, for each medical case,
18 rankings of the five segmentations were produced
each time using one of metrics (we call these the met-
ric rankings). The average correlation between manual
rankings and metric rankings was computed for each
metric and finally the metrics were sorted according to
this average correlation. The resulting metric ranking
(Table 1 column ’manual’) was used as a ground truth
suitability ranking of the metrics to validate the auto-
matic ranking. Table 1 column ’automatic’ contains for
each metric the bias (Equation 6) and the corresponding
suitability rank computed according to ascending bias.
A moderate to strong correlation between the two rank-
ings can be observed. The six best metrics are the same
in both rankings. This correlation shows that metrics
with low bias produce rankings that are more corre-
lated to manual rankings than others.
</bodyText>
<sectionHeader confidence="0.745997" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999871428571429">
For evaluating segmentations, metrics can be chosen
according to their bias (Equation 6) toward/against the
properties of the segmentations being evaluated. Test
results show that the ranking produced by metrics with
low bias generally have higher correlation with man-
ual ranking than rankings produced by other metrics.
In future work, the method will be tested with seg-
</bodyText>
<table confidence="0.997772333333333">
metric manual automatic
correl. rank bias rank
Cohen’s Kappa 0.818 1 33.5 2
Adjusted Rand Index 0.818 1 33.1 1
Interclass Correlation 0.818 1 33.5 2
Probabilistic distance 0.802 2 34.7 5
Dice 0.800 3 33.6 3
Average Distance 0.798 4 33.9 4
Accuracy 0.791 5 64.0 14
Rand Index 0.791 5 64.0 14
Variation of Inform. 0.791 6 62.0 13
Mutual Information 0.753 7 46.5 12
Mahalanobis Distance 0.701 8 37.7 7
Global Consistency Err. 0.670 9 69.8 15
Hausdorff Distance 0.663 10 35.5 6
Area u. curve (AUC) 0.647 11 42.0 8
Sensitivity 0.615 12 44.4 10
Precision 0.608 13 44.5 11
Volumetric Similarity 0.590 14 43.6 9
Specificity 0.398 15 78.6 16
Correl. btw. manual &amp; automatic ranking 0.607
</table>
<tableCaption confidence="0.963848">
Table 1. Manual and automatic metric suitability rank-
</tableCaption>
<bodyText confidence="0.983670611111111">
ings. In column ’manual’, the average correlation be-
tween metric rankings and the manual rankings as well
as corresponding suitability ranks according to decend-
ing correlation. In column ’automatic’, the metric bias
calculated automatically by the proposed method as
well as the ranks according to ascending bias (detailed
data and results available in [20])
mentations of other types and validated against rank-
ings from different experts. A further issue to be in-
vestigated in future work is the influence of weight-
ing the properties in Equation 6 on the metric suit-
ability ranking, if it is known that particular proper-
ties are more/less important for the segmentation goal.
For example, the manual ranking used to validate this
method is done by a radiologist who may emphasise
recall on cost of precision to assure that the tumor is
completely removed. In this case weighting the recall
and precision properly could improve the result.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.990720571428572">
Thanks to Dr. Bjoern H. Menze, ETH Zurich for pro-
viding the MRI brain segmentations from MICCAI 12
BRATS challenge to be used as test data.
The research leading to these results has received
funding from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agreement
318068 (VISCERAL).
</bodyText>
<note confidence="0.871758">
935 ICIP 2014
</note>
<sectionHeader confidence="0.993225" genericHeader="references">
7 References
</sectionHeader>
<reference confidence="0.993328208333333">
[1] T.H.J.M. Peeters, P.R. Rodrigues, A. Vilanova,
and B.M ter Haar Romeny, “Analysis of dis-
tance/similarity measures for diffusion tensor
imaging,” in Visualization and Processing of Tensor
Fields: Advances and Perspectives. Springer, Berlin,
2008.
[2] Mehrdad Fatourechi, Rabab K. Ward, Steven G.
Mason, Jane Huggins, Alois Schloegl, and Gary E.
Birch, “Comparison of evaluation metrics in clas-
sification applications with imbalanced datasets,”
in ICMLA, 2009, pp. 777–782.
[3] Guido Gerig, Matthieu Jomier, and Miranda
Chakos, “Valmet: A new validation tool for as-
sessing and improving 3D object segmentation,”
in Proceedings of the 4th International Conference on
Medical Image Computing and Computer-Assisted In-
tervention, 2001, pp. 516–523.
[4] Daniel B. Russakoff, Carlo Tomasi, Torsten Rohlf-
ing, Calvin R. Maurer, and Jr., “Image similarity
using mutual information of regions,” in 8th Eu-
ropean Conference on Computer Vision, ECCV, 2004,
pp. 596–607.
[5] Nguyen Xuan Vinh, Julien Epps, and James Bai-
ley, “Information theoretic measures for cluster-
ings comparison: is a correction for chance nec-
essary?,” in Proceedings of the 26th Annual Inter-
national Conference on Machine Learning. 2009, pp.
1073–1080, ACM.
[6] David M. W. Powers, “Evaluation: From precision,
recall and F-factor to ROC, informedness, marked-
ness correlation,” Journal of Machine Learning Tech-
nologies, vol. 2, pp. 37–63, 2011.
[7] Chris Buckley and Ellen M. Voorhees, “Evaluat-
ing evaluation measure stability,” in Proceedings of
the 23rd annual international ACM SIGIR conference
on Research and development in information retrieval.
2000, pp. 33–40, ACM.
[8] Halim Benhabiles, Guillaume Lavoue,
Jean Phillipe Vandeborre, and Mohamed Daoudi,
“A subjective experiment for 3d-mesh segmenta-
tion evaluation,” in IEEE International Workshop on
Multimedia Signal Processing (MMSP), 2010.
[9] Filip Radlinski and Nick Craswell, “Comparing
the sensitivity of information retrieval metrics,” in
Proceedings of the 33rd international ACM SIGIR con-
ference on Research and development in information re-
trieval, 2010.
[10] R. Blanco and H. Zaragoza, “Beware of relatively
large but meaningless improvements,” Tech. Rep.,
Yahoo! Research 2011-001, 2011.
[11] Tetsuya Sakai, “Evaluating evaluation metrics
based on the bootstrap,” in Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval. 2006,
pp. 525–532, ACM.
[12] Jin Huang and Charles X. Ling, “Using AUC and
accuracy in evaluating learning algorithms,” IEEE
Transactions on Knowledge and Data Engineering, vol.
17, pp. 299–310, 2005.
[13] Enrique Amigo, Julio Gonzalo, Javier Artiles, and
Felisa Verdejo, “A comparison of extrinsic clus-
tering evaluation metrics based on formal con-
straints,” Inf. Retr, vol. 12, no. 4, pp. 461–486, Au-
gust 2009.
[14] Nguyen Xuan Vinh, Julien Epps, and James Bai-
ley, “Information theoretic measures for cluster-
ings comparison: Variants, properties, normaliza-
tion and correction for chance,” J. Mach. Learn.
Res., vol. 9999, pp. 2837–2854, December 2010.
[15] Luca Busin and Stefano Mizzaro, “Axiometrics:
An axiomatic approach to information retrieval ef-
fectiveness metrics,” in Proceedings of the 2013 Con-
ference on the Theory of Information Retrieval, New
York, NY, USA, 2013, pp. 8:22–8:29.
[16] Tetsuya Sakai, “On the reliability of information
retrieval metrics based on graded relevance,” In-
formation Processing Management, 2007.
[17] Ellen M. Voorhees and Chris Buckley, “The effect
of topic set size on retrieval experiment error,” in
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, 2002, pp. 316–323.
[18] H. B. Mann and D. R. Whitney, “On a test of
whether one of two random variables is stochas-
tically larger than the other,” The Annals of Mathe-
matical Statistics, vol. 18, no. 1, pp. 50–60, 1947.
[19] Janez Demsar, “Statistical comparisons of classi-
fiers over multiple data sets,” J. Mach. Learn. Res.,
vol. 17, pp. 30, 2006.
[20] Abdel Aziz Taha, Allan Hanbury, and Os-
car Jimenez, “Test data and results of
the automatic metric selection method,”
Tech. Rep., Vienna University of Technology,
http://publik.tuwien.ac.at/files/
PubDat 229008.pdf, 2014.
936 ICIP 2014
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808241">
<title confidence="0.9612485">A FORMAL METHOD FOR SELECTING EVALUATION METRICS FOR SEGMENTATION.</title>
<author confidence="0.998611">Abdel Aziz Taha</author>
<author confidence="0.998611">Allan</author>
<affiliation confidence="0.99947">Vienna University of Technology</affiliation>
<abstract confidence="0.998824571428572">Evaluating the quality of segmentations is an important process in image processing, especially in the medical domain. Many evaluation metrics have been used in evaluating segmentation. There exists no formal way to choose the most suitable metric(s) for a particular segmentation task and/or particular data. In this paper we propose a formal method for choosing the most suitable metrics for evaluating the quality of segmentations with respect to ground truth segmentations. The proposed method depends on measuring the bias of metrics towards/against the properties of the the segmentations being evaluated. We firstly demonstrate how metrics can have bias towards/against particular properties and then we propose a general method for ranking metrics according to their overall bias. We finally demonstrate for 3D medical image segmentations that ranking produced using metrics with low overall bias strongly correlate with manual rankings done by an expert. segmentation; evaluation met-</abstract>
<intro confidence="0.895975">rics; selection</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T H J M Peeters</author>
<author>P R Rodrigues</author>
<author>A Vilanova</author>
</authors>
<title>and B.M ter Haar Romeny, “Analysis of distance/similarity measures for diffusion tensor imaging,” in Visualization and Processing of Tensor Fields: Advances and Perspectives.</title>
<date>2008</date>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="1544" citStr="[1]" startWordPosition="227" endWordPosition="227">bitrarily or according to their popularity. Investigating metrics would help researchers to better understand them and help companies and stakeholders to save effort and time reaching optimal systems [1]. A poorly defined metric may lead to inaccurate conclusions like selecting suboptimal models when comparing the performance of classifiers [2]. Many researchers have investigated the drawbacks of par</context>
</contexts>
<marker>[1]</marker>
<rawString>T.H.J.M. Peeters, P.R. Rodrigues, A. Vilanova, and B.M ter Haar Romeny, “Analysis of distance/similarity measures for diffusion tensor imaging,” in Visualization and Processing of Tensor Fields: Advances and Perspectives. Springer, Berlin, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehrdad Fatourechi</author>
<author>Rabab K Ward</author>
<author>Steven G Mason</author>
<author>Jane Huggins</author>
<author>Alois Schloegl</author>
<author>Gary E Birch</author>
</authors>
<title>Comparison of evaluation metrics in classification applications with imbalanced datasets,” in ICMLA,</title>
<date>2009</date>
<pages>777--782</pages>
<contexts>
<context position="1687" citStr="[2]" startWordPosition="247" endWordPosition="247">holders to save effort and time reaching optimal systems [1]. A poorly defined metric may lead to inaccurate conclusions like selecting suboptimal models when comparing the performance of classifiers [2]. Many researchers have investigated the drawbacks of particular metrics given particular properties of the data being classified. As a special case of classification, image segmentation is also affec</context>
<context position="2766" citStr="[2]" startWordPosition="404" endWordPosition="404">) are biased and don’t consider the level of chance [6]. Choosing evaluation metrics is very important and application-dependent; when evaluating imbalanced datasets, the metric choice is not obvious [2]. Metrics have different properties with respect to their correlation with user satisfaction criteria and their ease of interpretation [7]. Benhabiles et al. [8] validated 250 automatic segmentations </context>
<context position="4360" citStr="[2]" startWordPosition="646" endWordPosition="646"> of the images being segmented, meaning that particular metrics over978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014 penalize or over-reward segmentations given particular properties [4] [6] [11] [2] [3]. The second fact is that selecting the best evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be different</context>
<context position="7050" citStr="[2]" startWordPosition="1087" endWordPosition="1087">ity using Bootstrap Hypothesis Tests, and used this method in comparing seven evaluation metrics. They negate the belief that commonly used evaluation measures are equally reliable. Fatourechi et al. [2] proposed a framework based on Desired Region of Operation (DROP) for selecting the best evaluation metric for evaluating imbalanced classifications. Sakai [16] provided comparisons between metrics de</context>
</contexts>
<marker>[2]</marker>
<rawString>Mehrdad Fatourechi, Rabab K. Ward, Steven G. Mason, Jane Huggins, Alois Schloegl, and Gary E. Birch, “Comparison of evaluation metrics in classification applications with imbalanced datasets,” in ICMLA, 2009, pp. 777–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Gerig</author>
<author>Matthieu Jomier</author>
<author>Miranda Chakos</author>
</authors>
<title>Valmet: A new validation tool for assessing and improving 3D object segmentation,”</title>
<date>2001</date>
<booktitle>in Proceedings of the 4th International Conference on Medical Image Computing and Computer-Assisted Intervention,</booktitle>
<pages>516--523</pages>
<contexts>
<context position="2138" citStr="[3]" startWordPosition="315" endWordPosition="315">xamples: Hausdorff distance is very sensitive to noise and least squares Oscar A. Jimenez del Toro Univ. of Applied Sciences Western Swizerland based evaluation methods are very sensitive to outliers [3]. Mutual information doesn’t utilize spatial information inherited in images because only voxel relationships are considered but not the neighborhoods [4]. Information theoretical measures have a non-</context>
<context position="4364" citStr="[3]" startWordPosition="647" endWordPosition="647">the images being segmented, meaning that particular metrics over978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014 penalize or over-reward segmentations given particular properties [4] [6] [11] [2] [3]. The second fact is that selecting the best evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be differently i</context>
</contexts>
<marker>[3]</marker>
<rawString>Guido Gerig, Matthieu Jomier, and Miranda Chakos, “Valmet: A new validation tool for assessing and improving 3D object segmentation,” in Proceedings of the 4th International Conference on Medical Image Computing and Computer-Assisted Intervention, 2001, pp. 516–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B Russakoff</author>
<author>Carlo Tomasi</author>
<author>Torsten Rohlfing</author>
<author>Calvin R Maurer</author>
<author>Jr</author>
</authors>
<title>Image similarity using mutual information of regions,”</title>
<date>2004</date>
<booktitle>in 8th European Conference on Computer Vision, ECCV,</booktitle>
<pages>596--607</pages>
<contexts>
<context position="2292" citStr="[4]" startWordPosition="336" endWordPosition="336">uation methods are very sensitive to outliers [3]. Mutual information doesn’t utilize spatial information inherited in images because only voxel relationships are considered but not the neighborhoods [4]. Information theoretical measures have a non-convergent baseline which depends on the ratio between the number of data points and the number of classes. Therefore this class of measure needs chance c</context>
<context position="4347" citStr="[4]" startWordPosition="643" endWordPosition="643">st properties of the images being segmented, meaning that particular metrics over978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014 penalize or over-reward segmentations given particular properties [4] [6] [11] [2] [3]. The second fact is that selecting the best evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can</context>
</contexts>
<marker>[4]</marker>
<rawString>Daniel B. Russakoff, Carlo Tomasi, Torsten Rohlfing, Calvin R. Maurer, and Jr., “Image similarity using mutual information of regions,” in 8th European Conference on Computer Vision, ECCV, 2004, pp. 596–607.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Xuan Vinh</author>
<author>Julien Epps</author>
<author>James Bailey</author>
</authors>
<title>Information theoretic measures for clusterings comparison: is a correction for chance necessary?,”</title>
<date>2009</date>
<booktitle>in Proceedings of the 26th Annual International Conference on Machine Learning.</booktitle>
<pages>1073--1080</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2505" citStr="[5]" startWordPosition="368" endWordPosition="368">ion theoretical measures have a non-convergent baseline which depends on the ratio between the number of data points and the number of classes. Therefore this class of measure needs chance correction [5]. Commonly used measures (precision, recall and F-measures) are biased and don’t consider the level of chance [6]. Choosing evaluation metrics is very important and application-dependent; when evaluat</context>
</contexts>
<marker>[5]</marker>
<rawString>Nguyen Xuan Vinh, Julien Epps, and James Bailey, “Information theoretic measures for clusterings comparison: is a correction for chance necessary?,” in Proceedings of the 26th Annual International Conference on Machine Learning. 2009, pp. 1073–1080, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M W Powers</author>
</authors>
<title>Evaluation: From precision, recall and F-factor to ROC, informedness, markedness correlation,”</title>
<date>2011</date>
<journal>Journal of Machine Learning Technologies,</journal>
<volume>2</volume>
<pages>37--63</pages>
<contexts>
<context position="2618" citStr="[6]" startWordPosition="385" endWordPosition="385">nts and the number of classes. Therefore this class of measure needs chance correction [5]. Commonly used measures (precision, recall and F-measures) are biased and don’t consider the level of chance [6]. Choosing evaluation metrics is very important and application-dependent; when evaluating imbalanced datasets, the metric choice is not obvious [2]. Metrics have different properties with respect to </context>
<context position="4351" citStr="[6]" startWordPosition="644" endWordPosition="644">roperties of the images being segmented, meaning that particular metrics over978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014 penalize or over-reward segmentations given particular properties [4] [6] [11] [2] [3]. The second fact is that selecting the best evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be </context>
</contexts>
<marker>[6]</marker>
<rawString>David M. W. Powers, “Evaluation: From precision, recall and F-factor to ROC, informedness, markedness correlation,” Journal of Machine Learning Technologies, vol. 2, pp. 37–63, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Evaluating evaluation measure stability,”</title>
<date>2000</date>
<booktitle>in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<pages>33--40</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2904" citStr="[7]" startWordPosition="424" endWordPosition="424">luating imbalanced datasets, the metric choice is not obvious [2]. Metrics have different properties with respect to their correlation with user satisfaction criteria and their ease of interpretation [7]. Benhabiles et al. [8] validated 250 automatic segmentations against their corresponding ground truth segmentations using four different evaluation metrics. The results were then compared with manual</context>
<context position="4615" citStr="[7]" startWordPosition="689" endWordPosition="689">luation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be differently important depending on the segmentation goal [8] [7]. To meet the context dependency, the proposed method allows individual weighting of the influence of each property according to its importance in case this is known, which increases the effectiveness</context>
</contexts>
<marker>[7]</marker>
<rawString>Chris Buckley and Ellen M. Voorhees, “Evaluating evaluation measure stability,” in Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. 2000, pp. 33–40, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halim Benhabiles</author>
<author>Guillaume Lavoue</author>
<author>Jean Phillipe Vandeborre</author>
<author>Mohamed Daoudi</author>
</authors>
<title>A subjective experiment for 3d-mesh segmentation evaluation,”</title>
<date>2010</date>
<booktitle>in IEEE International Workshop on Multimedia Signal Processing (MMSP),</booktitle>
<contexts>
<context position="2927" citStr="[8]" startWordPosition="428" endWordPosition="428">sets, the metric choice is not obvious [2]. Metrics have different properties with respect to their correlation with user satisfaction criteria and their ease of interpretation [7]. Benhabiles et al. [8] validated 250 automatic segmentations against their corresponding ground truth segmentations using four different evaluation metrics. The results were then compared with manual ratings from 40 human </context>
<context position="4611" citStr="[8]" startWordPosition="688" endWordPosition="688"> evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be differently important depending on the segmentation goal [8] [7]. To meet the context dependency, the proposed method allows individual weighting of the influence of each property according to its importance in case this is known, which increases the effective</context>
</contexts>
<marker>[8]</marker>
<rawString>Halim Benhabiles, Guillaume Lavoue, Jean Phillipe Vandeborre, and Mohamed Daoudi, “A subjective experiment for 3d-mesh segmentation evaluation,” in IEEE International Workshop on Multimedia Signal Processing (MMSP), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Nick Craswell</author>
</authors>
<title>Comparing the sensitivity of information retrieval metrics,”</title>
<date>2010</date>
<booktitle>in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<contexts>
<context position="3795" citStr="[9]" startWordPosition="557" endWordPosition="557"> are needed to measure small but real improvements and also with high fidelity to distinguish between improvements based on user preferences and improvements resulting from biased relevance judgments [9] [10]. 1.2 Problem definition and notations: In this paper, we propose a formal method for selecting the most suitable metrics to evaluate image segmentation depending on the data being segmented and </context>
</contexts>
<marker>[9]</marker>
<rawString>Filip Radlinski and Nick Craswell, “Comparing the sensitivity of information retrieval metrics,” in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Blanco</author>
<author>H Zaragoza</author>
</authors>
<title>Beware of relatively large but meaningless improvements,”</title>
<date>2011</date>
<journal>Tech. Rep., Yahoo! Research</journal>
<pages>2011--001</pages>
<contexts>
<context position="3800" citStr="[10]" startWordPosition="558" endWordPosition="558"> needed to measure small but real improvements and also with high fidelity to distinguish between improvements based on user preferences and improvements resulting from biased relevance judgments [9] [10]. 1.2 Problem definition and notations: In this paper, we propose a formal method for selecting the most suitable metrics to evaluate image segmentation depending on the data being segmented and the g</context>
</contexts>
<marker>[10]</marker>
<rawString>R. Blanco and H. Zaragoza, “Beware of relatively large but meaningless improvements,” Tech. Rep., Yahoo! Research 2011-001, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>Evaluating evaluation metrics based on the bootstrap,”</title>
<date>2006</date>
<booktitle>in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<pages>525--532</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4356" citStr="[11]" startWordPosition="645" endWordPosition="645">rties of the images being segmented, meaning that particular metrics over978-1-4799-5751-4/14/$31.00 ©2014 IEEE 932 ICIP 2014 penalize or over-reward segmentations given particular properties [4] [6] [11] [2] [3]. The second fact is that selecting the best evaluation metrics can be subject to the segmentation goal which means that the bias towards/against a particular property of the data can be diffe</context>
<context position="6767" citStr="[11]" startWordPosition="1044" endWordPosition="1044">these papers deal with the problem only from a theoretical axiometrical point of view without taking into account the classification goal and the nature and properties of data being classified. Sakai [11] proposed a method for evaluating evaluation metrics by measuring their sensitivity using Bootstrap Hypothesis Tests, and used this method in comparing seven evaluation metrics. They negate the belief</context>
</contexts>
<marker>[11]</marker>
<rawString>Tetsuya Sakai, “Evaluating evaluation metrics based on the bootstrap,” in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. 2006, pp. 525–532, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Huang</author>
<author>Charles X Ling</author>
</authors>
<title>Using AUC and accuracy in evaluating learning algorithms,”</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>17</volume>
<pages>299--310</pages>
<contexts>
<context position="5744" citStr="[12]" startWordPosition="886" endWordPosition="886">ely medical volume segmentation e.g. magnetic resonance images (MRI) where voxels (3D pixels) are either assigned or not to a given class (segment) e.g. an organ or a tumor. 2 Related Work Jin et al. [12] established a formal method for comparing two different measures and introduced two criteria for formal comparison of the goodness of evaluation metrics, namely the degree of consistency (DoC) and de</context>
</contexts>
<marker>[12]</marker>
<rawString>Jin Huang and Charles X. Ling, “Using AUC and accuracy in evaluating learning algorithms,” IEEE Transactions on Knowledge and Data Engineering, vol. 17, pp. 299–310, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amigo</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints,”</title>
<date>2009</date>
<journal>Inf. Retr,</journal>
<volume>12</volume>
<pages>461--486</pages>
<contexts>
<context position="6133" citStr="[13]" startWordPosition="944" endWordPosition="944">cy (DoC) and degree of discriminancy (DoD). Applying these criteria, they showed theoretically and empirically that AUC is a better measure than accuracy in evaluating the performance of classifiers. [13] [14] applied formal constraints based on axiometry to compare and judge evaluation metrics depending on the grade of satisfaction of these constraints. Busin et al. [15] used axiometrics to define a </context>
</contexts>
<marker>[13]</marker>
<rawString>Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa Verdejo, “A comparison of extrinsic clustering evaluation metrics based on formal constraints,” Inf. Retr, vol. 12, no. 4, pp. 461–486, August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Xuan Vinh</author>
<author>Julien Epps</author>
<author>James Bailey</author>
</authors>
<title>Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance,”</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>9999</volume>
<pages>2837--2854</pages>
<contexts>
<context position="6138" citStr="[14]" startWordPosition="945" endWordPosition="945">oC) and degree of discriminancy (DoD). Applying these criteria, they showed theoretically and empirically that AUC is a better measure than accuracy in evaluating the performance of classifiers. [13] [14] applied formal constraints based on axiometry to compare and judge evaluation metrics depending on the grade of satisfaction of these constraints. Busin et al. [15] used axiometrics to define a forma</context>
</contexts>
<marker>[14]</marker>
<rawString>Nguyen Xuan Vinh, Julien Epps, and James Bailey, “Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance,” J. Mach. Learn. Res., vol. 9999, pp. 2837–2854, December 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luca Busin</author>
<author>Stefano Mizzaro</author>
</authors>
<title>Axiometrics: An axiomatic approach to information retrieval effectiveness metrics,”</title>
<date>2013</date>
<booktitle>in Proceedings of the 2013 Conference on the Theory of Information Retrieval,</booktitle>
<pages>8--22</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="6303" citStr="[15]" startWordPosition="970" endWordPosition="970">e performance of classifiers. [13] [14] applied formal constraints based on axiometry to compare and judge evaluation metrics depending on the grade of satisfaction of these constraints. Busin et al. [15] used axiometrics to define a formal and general notation that fits any effectiveness metric. Based on this notation, they proposed several axioms that should be satisfied by an effectiveness metric. </context>
</contexts>
<marker>[15]</marker>
<rawString>Luca Busin and Stefano Mizzaro, “Axiometrics: An axiomatic approach to information retrieval effectiveness metrics,” in Proceedings of the 2013 Conference on the Theory of Information Retrieval, New York, NY, USA, 2013, pp. 8:22–8:29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>On the reliability of information retrieval metrics based on graded relevance,” Information Processing Management,</title>
<date>2007</date>
<contexts>
<context position="7210" citStr="[16]" startWordPosition="1111" endWordPosition="1111"> are equally reliable. Fatourechi et al. [2] proposed a framework based on Desired Region of Operation (DROP) for selecting the best evaluation metric for evaluating imbalanced classifications. Sakai [16] provided comparisons between metrics depending on the sensitivity and stability using the Voorhees/Buckley swap method [17]. All these papers lack generality because they are methods designed either </context>
</contexts>
<marker>[16]</marker>
<rawString>Tetsuya Sakai, “On the reliability of information retrieval metrics based on graded relevance,” Information Processing Management, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Chris Buckley</author>
</authors>
<title>The effect of topic set size on retrieval experiment error,”</title>
<date>2002</date>
<booktitle>in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="7334" citStr="[17]" startWordPosition="1128" endWordPosition="1128">the best evaluation metric for evaluating imbalanced classifications. Sakai [16] provided comparisons between metrics depending on the sensitivity and stability using the Voorhees/Buckley swap method [17]. All these papers lack generality because they are methods designed either for specific metrics or for specific metric properties. 3 Proposed Methods We propose a method for choosing the most suitabl</context>
</contexts>
<marker>[17]</marker>
<rawString>Ellen M. Voorhees and Chris Buckley, “The effect of topic set size on retrieval experiment error,” in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, 2002, pp. 316–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H B Mann</author>
<author>D R Whitney</author>
</authors>
<title>On a test of whether one of two random variables is stochastically larger than the other,”</title>
<date>1947</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>18</volume>
<pages>50--60</pages>
<contexts>
<context position="10823" citStr="[18]" startWordPosition="1755" endWordPosition="1755">e random partition i measured by metric m. Note that ranking the subsets using the averages of the individual ranks in each subset is a ranking method inspired by the Mann-Whitney-Wilcoxon (MWW) test [18]. This is because straightforwardly computing the ranks from score averages is sensitive to outliers and may produce unreasonable rankings if the scores are not normally distributed [19]. 3.2 Inferrin</context>
</contexts>
<marker>[18]</marker>
<rawString>H. B. Mann and D. R. Whitney, “On a test of whether one of two random variables is stochastically larger than the other,” The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50–60, 1947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Demsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets,”</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>17</volume>
<pages>30</pages>
<contexts>
<context position="11009" citStr="[19]" startWordPosition="1782" endWordPosition="1782">on (MWW) test [18]. This is because straightforwardly computing the ranks from score averages is sensitive to outliers and may produce unreasonable rankings if the scores are not normally distributed [19]. 3.2 Inferring metric bias: Now, the rank structures Rˇ (rankings of the random partitions) and R (rankings of the non-random partitions) provide a statistical basis to infer metric bias by analyzing</context>
</contexts>
<marker>[19]</marker>
<rawString>Janez Demsar, “Statistical comparisons of classifiers over multiple data sets,” J. Mach. Learn. Res., vol. 17, pp. 30, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdel Aziz Taha</author>
<author>Allan Hanbury</author>
<author>Oscar Jimenez</author>
</authors>
<title>Test data and results of the automatic metric selection method,”</title>
<date>2014</date>
<booktitle>Technology, http://publik.tuwien.ac.at/files/ PubDat 229008.pdf, 2014. 936 ICIP</booktitle>
<tech>Tech. Rep.,</tech>
<institution>Vienna University of</institution>
<contexts>
<context position="17466" citStr="[20]" startWordPosition="2865" endWordPosition="2865"> decending correlation. In column ’automatic’, the metric bias calculated automatically by the proposed method as well as the ranks according to ascending bias (detailed data and results available in [20]) mentations of other types and validated against rankings from different experts. A further issue to be investigated in future work is the influence of weighting the properties in Equation 6 on the m</context>
</contexts>
<marker>[20]</marker>
<rawString>Abdel Aziz Taha, Allan Hanbury, and Oscar Jimenez, “Test data and results of the automatic metric selection method,” Tech. Rep., Vienna University of Technology, http://publik.tuwien.ac.at/files/ PubDat 229008.pdf, 2014. 936 ICIP 2014</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>