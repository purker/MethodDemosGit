<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.116623">
<title confidence="0.937836">
MODELLING OF VISUAL FEATURE DERIVATION IN THE VIZIR FRAMEWORK
</title>
<author confidence="0.922666">
Horst Eidenberger
</author>
<bodyText confidence="0.7284875">
Institute of Software Technology and Interactive Systems, Vienna University of Technology
Favoritenstrasse 9-11, A-1040 Wien, Austria (Europe)
phone: +43 1 58801 18853, fax: +43 1 58801 18898, email: hme@ims.tuwien.ac.at
web: www.ims.tuwien.ac.at
</bodyText>
<sectionHeader confidence="0.977466" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999839181818182">
If visual information retrieval should make further progress,
it will be necessary to identify new ways to derive visual
properties from higher levels of understanding than the pixel
level (e.g. from low-level features). The paper outlines the
implementation of modelling of feature hierarchies in the
visual information retrieval framework VizIR (free under
GPL). The approach allows for the derivation of high-level
features from low-level features by aggregation and localisa-
tion as well as semantic enrichment with additional knowl-
edge. The technical implementation is based on the MPEG-7
structures for aggregation and specialisation.
</bodyText>
<sectionHeader confidence="0.995834" genericHeader="keywords">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99994864">
Visual information retrieval (VIR) research is driven by the
desire to derive semantically meaningful information di-
rectly from the content of media objects. Recent years have
seen considerable efforts to overcome the enormous gravity
of the pixel and to extract more than just colour histograms,
edge information and other low-level features. The MPEG-7
standard [1] supports these efforts in two ways: Firstly, it
provides aggregation and localisation structures for spatio-
temporal feature enrichment. Secondly, it standardises cook-
books for the most prominent low-level features and, by
that, provides a foundation for semantic enrichment of low-
level features.
Below, we will regard aggregated/localised and semanti-
cally enriched as two relevant types of high-level features.
The paper describes the way high-level feature structures
(e.g. based on MPEG-7 descriptors) can be implemented in
the VIR framework VizIR. The VizIR project [2] aims at
providing VIR researchers with a workbench of feature ex-
traction, querying and evaluation tools.
The paper is organised as follows: Section 2 gives back-
ground information on relevant MPEG-7 models and princi-
ples of semantic feature design. Section 3 shortly sketches
the VizIR project. Section 4 describes the architecture of
high-level feature modelling in VizIR. Additionally, Subsec-
tion 4.4 covers important implementation aspects.
</bodyText>
<sectionHeader confidence="0.997643" genericHeader="introduction">
2. BACKGROUND
</sectionHeader>
<subsectionHeader confidence="0.979518">
2.1 MPEG-7 aggregation and localisation structures
</subsectionHeader>
<bodyText confidence="0.999966142857143">
The visual part of the MPEG-7 standard (e.g. [4]) provides a
set of content-based descriptors for colour, texture, shape
and motion properties of visual media objects. In addition, it
allows for using static (image) descriptors on video content
and for spatial as well as temporal localisation of descriptors
in media objects.
For temporal aggregation, MPEG-7 provides the Time
Series descriptor. The Time Series descriptor can be used for
regular time series (constant interval between samples; e.g.
Colour Layout descriptions of key-frames) and irregular time
series. In the latter case a media object is sub-sampled at the
minimum frequency and samples at requested points in time
are selected. MPEG-7 describes how Time Series have to be
extracted and how they have to be compared. Distance meas-
urement is non-trivial for irregular time series.
The main construct for localisation is the Spatio-
Temporal Locator. It describes object trajectories over space
and time. Spatio-Temporal Locators exist as Figure Trajecto-
ries for non-rigid objects and as Parameter Trajectories for
rigid objects. Both descriptors make use of Region Locators
and the Temporal Interpolation descriptor. Region Locator is
a simple contour-based description of an object (bounding
rectangle, ellipse or polygon). Temporal Interpolation is used
to estimate unavailable samples by assuming a particular
interpolation function. Figure Trajectories describe moving
objects by the trajectories of object vertices. Parameter Tra-
jectories assume motion models for objects and describe
object movements by parameter sets.
</bodyText>
<subsectionHeader confidence="0.99877">
2.2 Semantic feature enrichment
</subsectionHeader>
<bodyText confidence="0.999942611111111">
As human beings we derive our visual similarity perception
from generally perceived (e.g. colour distributions) and spe-
cifically perceived (recognized) media properties. Today&apos;s
VIR systems are only capable of extracting generally per-
ceived (low-level) features. Therefore, retrieval results are
often unsatisfactory for us. To overcome this shortcoming,
considerable research effort is undertaken to enrich low-
level features with semantic information.
Generally, three sources of information are available to
enhance features: (1) information on the application domain
(e.g. on media content), (2) information on the user (e.g. re-
trieval preferences) and (3) information on the characteristics
of the descriptors used (e.g. statistical properties). Techni-
cally, additional knowledge can be induced by methods from
statistics, artificial intelligence (e.g. neural networks), etc.
For example, domain knowledge about football could be
used to identify ball and players from shape features (e.g.
circularity, edge distribution).
</bodyText>
<sectionHeader confidence="0.987824" genericHeader="method">
3. THE VIZIR PROJECT
</sectionHeader>
<bodyText confidence="0.9998984">
The next three subsections describe the VizIR project in
general and, in greater detail, the environment, in which we
are applying descriptor aggregation, localisation and en-
richment techniques to enhance features and improve con-
tent-based retrieval results.
</bodyText>
<subsectionHeader confidence="0.994968">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999993695652174">
The VizIR framework is an open and extendible software
workbench for content-based video and image retrieval. It
implements state of the art technologies such as the visual
MPEG-7 descriptors, the Multimedia Retrieval Markup
Language [5] for loose coupling of query engines and user
interfaces, etc. and novel paradigms for feature extraction,
querying (e.g. 3D user interfaces for integrated browsing
and retrieval) and evaluation.
VizIR is free software: it is released as source code un-
der GNU Public License. The VizIR project intends to pro-
vide a common software platform that can be used by re-
searchers as a workbench for further VIR research, by soft-
ware engineers for the development of VIR components for
media and database applications and by lecturers for teaching
VIR concepts. The VizIR framework was carefully designed
to guarantee that these requirements can be met.
Technically, the VizIR framework components are based
on the Java programming language, the Java SDK, the Java
Media Framework for media processing and other freely
available software libraries. The current status of the project,
software releases, documentation and development resources
can be found on the project website [6]. VizIR is an open
project: new users and contributors are always welcome.
</bodyText>
<subsectionHeader confidence="0.993582">
3.2 General framework design
</subsectionHeader>
<bodyText confidence="0.999978942857143">
The most important part of VizIR is a class framework for
feature extraction, querying, refinement, VIR user inter-
faces, communication, evaluation and benchmarking. Cen-
tral element is a service kernel. This class is responsible for
media administration and query execution. It communicates
with query engines, user interfaces and media databases
through XML messages (based on the Multimedia Retrieval
Markup Language, MRML [5]) and web services.
Media access is hidden in a class that enables accessing
the view of visual media (at arbitrary resolution and based on
an arbitrary colour model) at any point in time. By that, im-
ages and videos can be accessed with a uniform API. De-
scriptors and media renderer classes make use of the media
access class to derive features and to visualise media objects
in user interfaces. All elements of user interfaces (including
query definition and refinement panels, metadata panels,
sketch drawing panels, etc.) are modelled as independent
classes that interact with each other through well-defined
events and listener methods (locally) or through XML-based
web services (remotely). They can be combined arbitrarily to
create VIR user interfaces and easily be supplemented by
additional querying methods.
VizIR query engines are derived from a common model.
This model defines how queries are executed technically (not
how the querying logic works). This includes the interface to
MRML communication classes, the service kernel and para-
digms for database access. All VizIR components are based
on an object-oriented database model: If desired, the re-
sources of instances of any class in the framework can be
serialised and kept persistent in a database. This feature is
guaranteed by an underlying persistence system. The object-
oriented database system may be chosen arbitrarily. In our
test environment we are currently using a relational database
(MySQL) and an object mapping tool (Hibernate). See [2]
and [3] for more information on the VizIR architecture.
</bodyText>
<subsectionHeader confidence="0.99553">
3.3 Visual feature modelling
</subsectionHeader>
<bodyText confidence="0.986480166666667">
Implementation of visual descriptors is a key requirement in
the VizIR framework. Descriptor data should be calculated
directly from media content and it has to be guaranteed that
the descriptor designer does not have to care for storing the
descriptor data in the database. VizIR provides two types of
classes for the implementation of visual descriptors:
</bodyText>
<listItem confidence="0.999663">
• DescriptorInfo classes
• DescriptorLogic classes
</listItem>
<bodyText confidence="0.994789052631579">
Derivates of DescriptorInfo classes hold the (XML) descrip-
tions extracted by features. These classes have a unique
name and their objects have unique IDs. Objects can be
stored to and read from a database. DescriptorInfo classes
are also factories that can be used to create their associated
DescriptorLogic objects.
DescriptorLogic classes contain just two methods: ex-
tractFeature() to extract feature data from given media con-
tent and calculateDistance() to measure distance to a second
instance of the same descriptor (a DescriptorInfo object).
Since DescriptorLogic classes contain no relevant status in-
formation, they do not need to be made persistent and may
have arbitrary names.
The VizIR framework can easily be extended with new
descriptors by creating a new DescriptorInfo class, creating a
new mapping (if the default mapping is not sufficient) and
implementing a private DescriptorLogic class with the fea-
ture extraction logic. New descriptors can be used as soon as
the database mapping (an XML document) is available.
</bodyText>
<sectionHeader confidence="0.997944" genericHeader="method">
4. FEATURE DERIVATION MODELLING
</sectionHeader>
<bodyText confidence="0.99990225">
Below, it will be discussed how the described descriptor
models can be adapted to allow for feature aggregation, lo-
calisation and semantic enrichment. Additionally, in Subsec-
tion 4.4 relevant implementation details will be sketched.
</bodyText>
<subsectionHeader confidence="0.920078">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.9991888">
Generally, descriptors could simply be added to the frame-
work as new classes that make use of already existing algo-
rithms. To enable this scenario, the algorithms used in De-
scriptorLogic classes (e.g. time to frequency transforma-
tions, edge operators) would have to be implemented with a
well-designed configurable API and collected in software
libraries. Still, for the sake of transparency, good software
design and maximum code reuse, it would be desirable to
have more sophisticated descriptor extension mechanisms
available.
</bodyText>
<figure confidence="0.999324352941177">
DescriptorInfo
create DescriptorLogic use DescriptorModel
DescriptorLogicA
Semantic
feature
enrichment
parameterise
DescriptorLogicB
...
Lower-level descriptor
Aggregation / Higher-level descriptor
localisation
Data
Algorithm
DescriptorInfo
DescriptorLogic
create
</figure>
<figureCaption confidence="0.999985">
Figure 1: General architecture of high-level feature modelling in VizIR.
</figureCaption>
<bodyText confidence="0.983176166666667">
Figure 1 describes the general architecture of descriptor
design and derivation in the VizIR framework. The left side
of the figure shows the data classes. The right side shows the
algorithms. The top half shows a low-level descriptor. The
bottom half shows a derived descriptor. The design depicted
in the figure will be discussed in the next two subsections.
Essentially, the idea is that semantically enriched descriptors
should be derived from data classes, since they may use
completely different algorithms. Aggregated and localised
descriptors should be derived from the algorithm classes,
because they produce different data classes by using the same
or similar algorithms.
</bodyText>
<subsectionHeader confidence="0.998939">
4.2 Feature aggregation and localisation
</subsectionHeader>
<bodyText confidence="0.999977704545455">
In VizIR, descriptors are stored to databases through an ob-
ject-oriented mapping technique. Instances of descriptors
containing media-specific data are serialised to BLOBs. The
BLOBs are referenced by unique IDs (including the class
name). For this approach it is necessary that descriptors that
create specific output have unique names.
Unfortunately, for example, in the MPEG-7 standard,
visual descriptors can be configured to create different de-
scriptions (e.g. Scalable Colour histograms may have 32 or
64 bins). If the descriptor classes creating these descriptions
would be of the same name and be configured for specific
use (e.g. by getters/setters), incompatible descriptions could
not be distinguished in VizIR. Therefore, as a first step for
aggregation/localisation it is necessary to find an appropriate
solution for descriptor configuration.
The solution in VizIR, shown in the top right of Figure
1, is straight-forward: The fundamental description procedure
is implemented in a DescriptorLogic class. This class may be
defined as being abstract. Optionally, the algorithmic details
may be laid down in a separate DescriptorModel class (e.g.
transformations, visual operators). For parameterisation, the
DescriptorLogic class has a constructor with all relevant pa-
rameters. To use a specific version of the descriptor it is sub-
classed with a default descriptor that calls the super descrip-
tor with the desired parameters (e.g. histogram size). Since
the sub-class needs to have a unique name, its instances can
easily be stored to a database and be distinguished from other
versions of the same descriptor.
The same pattern is used for aggregation and localisa-
tion. This feature is implemented as a parameter of type Spa-
tio-Temporal Locator (MPEG-7; in VizIR: a Java class).
Then, basically, a spatially and/or temporally specific version
of a descriptor is a sub-class of the general descriptor algo-
rithm. In case of aggregation, if an additional envelope is
required (e.g. a Time Series container or a Figure Trajectory,
see Subsection 2.1) it is necessary to overload the corre-
sponding DescriptorInfo class of DescriptorLogic as well.
Again, since the class name of the derived class is unique,
this means no problem for the persistence system.
Figure 2 shows a further case of simple localisation and
aggregation. If the specialised feature is the result of sub-
sampling of existing descriptor data, the aggrega-
tion/localisation process can be performed by an XSL trans-
former encapsulated in a DescriptorLogic class.
</bodyText>
<subsectionHeader confidence="0.998966">
4.3 Semantic feature enrichment
</subsectionHeader>
<bodyText confidence="0.999069888888889">
In contrast to aggregation and localisation, completely dif-
ferent workflows and algorithms are used for semantic en-
richment of features. In consequence, it would not make
sense to derive semantic descriptors on the algorithm level
(DescriptorLogic). In VizIR, semantic feature classes are
descriptions (DescriptorInfo objects) derived from existing
descriptions. Figure 3 sketches this process. Descriptor data
and additional knowledge are used to create semantic fea-
tures. If the enrichment process is linear and all required
</bodyText>
<figureCaption confidence="0.999934">
Figure 2: Simple aggregation/localisation of features.
Figure 3: Semantic enrichment of features in VizIR.
</figureCaption>
<figure confidence="0.9964654">
MPEG-7
descriptor
data
Derived
MPEG-7
compliant
descriptor
Descriptor data
Descriptor data
...
Framework components
Additional
knowledge
...
XSL
Trans-
former
High-level
descriptor logic
High-level
descriptor
information
MPEG-7 aggreg-
ation / localisa-
tion structures
</figure>
<figureCaption confidence="0.503805">
information is available as XML structured data, again, an
XSL transformer is sufficient to create the semantic feature.
</figureCaption>
<bodyText confidence="0.999898555555556">
If a more sophisticated logic (e.g. a statistical procedure)
is applied for semantic enrichment, it is recommended to
embed the enrichment process in a DescriptorLogic class that
is not derived from the feature extraction classes used for the
low-level features. This paradigm guarantees the existence of
reasonable data and logic classes for any type of feature.
Consequently, arbitrarily long chains of semantic enrichment,
aggregation and localisation of features can be modelled and
implemented in VizIR.
</bodyText>
<subsectionHeader confidence="0.994869">
4.4 Implementation details
</subsectionHeader>
<bodyText confidence="0.999961333333333">
VizIR is based on the Java programming language. Although
persistence management is based on an object-oriented da-
tabase model, it is an important design goal that descriptors
are stored into the database in a valid XML format (e.g. the
MPEG-7 schemes). Non-VizIR applications (e.g. XSL trans-
formers) have to be able to read the descriptions as well.
Internally, descriptions are represented by appropriate mem-
ory structures (essentially, one class per tag). Handling is
based on the Document Object Model (DOM) of the World
Wide Web Consortium. The JDom package is used for de-
scription parsing and writing. To guarantee that the database
requirement is met, JDom is used to (redundantly) serialise
the description DOMs to string variables. These strings are
stored in the database and used as starting point for semantic
enrichment of features.
As pointed out above, XSL transformers could solve
some of the problems associated with feature aggregation,
localisation and enrichment. At the moment, several frame-
works for XSL transformation are available and under devel-
opment. We are currently preferring the Cocoon framework
of the Apache project [7], as it is Java-based, in a mature
state and easy to install and use. For the applications pro-
posed above, implementing an appropriate transformation
style-sheet would be sufficient.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="conclusions">
5. CONCLUSION
</sectionHeader>
<bodyText confidence="0.9995712">
In this paper, we sketch solutions for the manipulation of
visual features in the visual information retrieval framework
VizIR. Considered manipulations are aggregation of feature
data over time, localisation in time and/or space and seman-
tic enrichment of features with additional knowledge. Solu-
tions are software patterns for the derivation of feature logic
classes and data classes as well as interfaces to helpful ex-
ternal components (e.g. XSL transformers). The presented
solutions will be implemented in the VizIR framework and
be made available to the VIR community as free software.
</bodyText>
<sectionHeader confidence="0.997089" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9973012">
The author would like to thank Christian Breiteneder and
Roman Divotkey for their valuable suggestions for im-
provement. The VizIR project is supported by the Austrian
Scientific Research Fund FWF und grant number P16111-
N05.
</bodyText>
<sectionHeader confidence="0.998942" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999904869565218">
[1] S.F. Chang, T. Sikora and A. Puri, &quot;Overview of the
MPEG-7 standard&quot;, IEEE Transactions on Circuits and
Systems for Video Technology, vol. 11, no. 6, pp. 688-695,
June 2001.
[2] H. Eidenberger and C. Breiteneder, &quot;VizIR – A Frame-
work for Visual Information Retrieval&quot;, Journal of Visual
Languages and Computing, vol. 14, pp. 443-469, Sep-
tember 2003.
[3] H. Eidenberger, &quot;Media Handling for Visual Information
Retrieval in VizIR&quot;, in Proc. SPIE Visual Communica-
tions and Image Processing Conference 2003, SPIE vol.
5150, July 2003, pp. 1078-1088.
[4] B.S. Manjunath, J.R. Ohm, V.V. Vasudevan and A. Ya-
mada, &quot;Color and texture descriptors&quot;, IEEE Transactions
on Circuits and Systems for Video Technology, vol. 11,
no. 6, pp. 703-715, June 2001.
[5] University of Geneva, Multimedia Retrieval Markup
Language website, http://www.mrml.net/, last visited
2004-01-07.
[6] Vienna University of Technology, VizIR project website,
http://vizir.ims.tuwien.ac.at/, last visited 2004-01-07.
[7] Apache project, Cocoon framework website,
http://cocoon.apache.org/, last visited 2004-01-07
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364926">
<title confidence="0.998883">MODELLING OF VISUAL FEATURE DERIVATION IN THE VIZIR FRAMEWORK</title>
<author confidence="0.999413">Horst Eidenberger</author>
<affiliation confidence="0.998861">Institute of Software Technology and Interactive Systems, Vienna University of</affiliation>
<address confidence="0.989768">Favoritenstrasse 9-11, A-1040 Wien, Austria</address>
<email confidence="0.6551905">phone:+4315880118853,fax:+4315880118898,email:web:www.ims.tuwien.ac.at</email>
<abstract confidence="0.993233083333333">If visual information retrieval should make further progress, it will be necessary to identify new ways to derive visual properties from higher levels of understanding than the pixel level (e.g. from low-level features). The paper outlines the implementation of modelling of feature hierarchies in the visual information retrieval framework VizIR (free under GPL). The approach allows for the derivation of high-level features from low-level features by aggregation and localisation as well as semantic enrichment with additional knowledge. The technical implementation is based on the MPEG-7 structures for aggregation and specialisation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S F Chang</author>
<author>T Sikora</author>
<author>A Puri</author>
</authors>
<title>Overview of the MPEG-7 standard&quot;,</title>
<date>2001</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>11</volume>
<pages>688--695</pages>
<contexts>
<context position="1359" citStr="[1]" startWordPosition="191" endWordPosition="191"> years have seen considerable efforts to overcome the enormous gravity of the pixel and to extract more than just colour histograms, edge information and other low-level features. The MPEG-7 standard [1] supports these efforts in two ways: Firstly, it provides aggregation and localisation structures for spatiotemporal feature enrichment. Secondly, it standardises cookbooks for the most prominent low-</context>
</contexts>
<marker>[1]</marker>
<rawString>S.F. Chang, T. Sikora and A. Puri, &quot;Overview of the MPEG-7 standard&quot;, IEEE Transactions on Circuits and Systems for Video Technology, vol. 11, no. 6, pp. 688-695, June 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Eidenberger</author>
<author>C Breiteneder</author>
</authors>
<title>VizIR – A Framework for Visual Information Retrieval&quot;,</title>
<date>2003</date>
<journal>Journal of Visual Languages and Computing,</journal>
<volume>14</volume>
<pages>443--469</pages>
<contexts>
<context position="1932" citStr="[2]" startWordPosition="274" endWordPosition="274">wo relevant types of high-level features. The paper describes the way high-level feature structures (e.g. based on MPEG-7 descriptors) can be implemented in the VIR framework VizIR. The VizIR project [2] aims at providing VIR researchers with a workbench of feature extraction, querying and evaluation tools. The paper is organised as follows: Section 2 gives background information on relevant MPEG-7 m</context>
<context position="8643" citStr="[2]" startWordPosition="1271" endWordPosition="1271">sistence system. The objectoriented database system may be chosen arbitrarily. In our test environment we are currently using a relational database (MySQL) and an object mapping tool (Hibernate). See [2] and [3] for more information on the VizIR architecture. 3.3 Visual feature modelling Implementation of visual descriptors is a key requirement in the VizIR framework. Descriptor data should be calcul</context>
</contexts>
<marker>[2]</marker>
<rawString>H. Eidenberger and C. Breiteneder, &quot;VizIR – A Framework for Visual Information Retrieval&quot;, Journal of Visual Languages and Computing, vol. 14, pp. 443-469, September 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Eidenberger</author>
</authors>
<title>Media Handling for Visual Information Retrieval in VizIR&quot;, in</title>
<date>2003</date>
<booktitle>Proc. SPIE Visual Communications and Image Processing Conference 2003, SPIE</booktitle>
<volume>5150</volume>
<pages>1078--1088</pages>
<contexts>
<context position="8651" citStr="[3]" startWordPosition="1273" endWordPosition="1273"> system. The objectoriented database system may be chosen arbitrarily. In our test environment we are currently using a relational database (MySQL) and an object mapping tool (Hibernate). See [2] and [3] for more information on the VizIR architecture. 3.3 Visual feature modelling Implementation of visual descriptors is a key requirement in the VizIR framework. Descriptor data should be calculated dir</context>
</contexts>
<marker>[3]</marker>
<rawString>H. Eidenberger, &quot;Media Handling for Visual Information Retrieval in VizIR&quot;, in Proc. SPIE Visual Communications and Image Processing Conference 2003, SPIE vol. 5150, July 2003, pp. 1078-1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Manjunath</author>
<author>J R Ohm</author>
<author>V V Vasudevan</author>
<author>A Yamada</author>
</authors>
<title>Color and texture descriptors&quot;,</title>
<date>2001</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>11</volume>
<pages>703--715</pages>
<contexts>
<context position="2489" citStr="[4]" startWordPosition="356" endWordPosition="356">modelling in VizIR. Additionally, Subsection 4.4 covers important implementation aspects. 2. BACKGROUND 2.1 MPEG-7 aggregation and localisation structures The visual part of the MPEG-7 standard (e.g. [4]) provides a set of content-based descriptors for colour, texture, shape and motion properties of visual media objects. In addition, it allows for using static (image) descriptors on video content and</context>
</contexts>
<marker>[4]</marker>
<rawString>B.S. Manjunath, J.R. Ohm, V.V. Vasudevan and A. Yamada, &quot;Color and texture descriptors&quot;, IEEE Transactions on Circuits and Systems for Video Technology, vol. 11, no. 6, pp. 703-715, June 2001.</rawString>
</citation>
<citation valid="false">
<pages>2004--01</pages>
<institution>University of Geneva, Multimedia Retrieval Markup Language</institution>
<note>website, http://www.mrml.net/, last visited</note>
<contexts>
<context position="5622" citStr="[5]" startWordPosition="803" endWordPosition="803"> extendible software workbench for content-based video and image retrieval. It implements state of the art technologies such as the visual MPEG-7 descriptors, the Multimedia Retrieval Markup Language [5] for loose coupling of query engines and user interfaces, etc. and novel paradigms for feature extraction, querying (e.g. 3D user interfaces for integrated browsing and retrieval) and evaluation. VizI</context>
<context position="7126" citStr="[5]" startWordPosition="1033" endWordPosition="1033">ble for media administration and query execution. It communicates with query engines, user interfaces and media databases through XML messages (based on the Multimedia Retrieval Markup Language, MRML [5]) and web services. Media access is hidden in a class that enables accessing the view of visual media (at arbitrary resolution and based on an arbitrary colour model) at any point in time. By that, im</context>
</contexts>
<marker>[5]</marker>
<rawString>University of Geneva, Multimedia Retrieval Markup Language website, http://www.mrml.net/, last visited 2004-01-07.</rawString>
</citation>
<citation valid="false">
<pages>2004--01</pages>
<institution>Vienna University of Technology, VizIR</institution>
<note>project website, http://vizir.ims.tuwien.ac.at/, last visited</note>
<contexts>
<context position="6595" citStr="[6]" startWordPosition="955" endWordPosition="955">k for media processing and other freely available software libraries. The current status of the project, software releases, documentation and development resources can be found on the project website [6]. VizIR is an open project: new users and contributors are always welcome. 3.2 General framework design The most important part of VizIR is a class framework for feature extraction, querying, refineme</context>
</contexts>
<marker>[6]</marker>
<rawString>Vienna University of Technology, VizIR project website, http://vizir.ims.tuwien.ac.at/, last visited 2004-01-07.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Apache project</author>
</authors>
<title>Cocoon framework website,</title>
<pages>2004--01</pages>
<note>http://cocoon.apache.org/, last visited</note>
<contexts>
<context position="17313" citStr="[7]" startWordPosition="2558" endWordPosition="2558">tion, localisation and enrichment. At the moment, several frameworks for XSL transformation are available and under development. We are currently preferring the Cocoon framework of the Apache project [7], as it is Java-based, in a mature state and easy to install and use. For the applications proposed above, implementing an appropriate transformation style-sheet would be sufficient. 5. CONCLUSION In </context>
</contexts>
<marker>[7]</marker>
<rawString>Apache project, Cocoon framework website, http://cocoon.apache.org/, last visited 2004-01-07</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>