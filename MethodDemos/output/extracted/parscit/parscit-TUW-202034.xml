<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.761146666666667">
Clustering Heuristics for the
Hierarchical Ring Network
Problem
</title>
<sectionHeader confidence="0.621056" genericHeader="abstract">
DIPLOMARBEIT
</sectionHeader>
<bodyText confidence="0.46044">
zur Erlangung des akademischen Grades
Diplom-Ingenieur
im Rahmen des Studiums
</bodyText>
<sectionHeader confidence="0.427403" genericHeader="categories and subject descriptors">
Computational Intelligence
</sectionHeader>
<keyword confidence="0.164306">
eingereicht von
</keyword>
<sectionHeader confidence="0.259617" genericHeader="general terms">
Rainer Schuster
</sectionHeader>
<bodyText confidence="0.636965333333333">
Matrikelnummer 0425205
an der
Fakultät für Informatik der Technischen Universität Wien
</bodyText>
<note confidence="0.445835">
Betreuung: ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl
Mitwirkung: Univ.-Ass. Dipl.-Ing. Christian Schauer
Wien, 30.11.2011
(Unterschrift Verfasser) (Unterschrift Betreuung)
Technische Universität Wien
A-1040 Wien - Karlsplatz 13 - Tel. +43-1-58801-0 - www.tuwien.ac.at
</note>
<title confidence="0.752258333333333">
Clustering Heuristics for the
Hierarchical Ring Network
Problem
</title>
<sectionHeader confidence="0.890835" genericHeader="keywords">
MASTER’S THESIS
</sectionHeader>
<bodyText confidence="0.5900772">
submitted in partial fulfillment of the requirements for the degree of
Diplom-Ingenieur
in
Computational Intelligence
by
</bodyText>
<note confidence="0.307836">
Rainer Schuster
</note>
<author confidence="0.873872">
Registration Number 0425205
</author>
<affiliation confidence="0.987667">
to the Faculty of Informatics
at the Vienna University of Technology
</affiliation>
<note confidence="0.3693795">
Advisor: ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl
Assistance: Univ.-Ass. Dipl.-Ing. Christian Schauer
Vienna, 30.11.2011
(Signature of Author) (Signature of Advisor)
Technische Universität Wien
A-1040 Wien - Karlsplatz 13 - Tel. +43-1-58801-0 - www.tuwien.ac.at
</note>
<title confidence="0.432929">
Erklärung zur Verfassung der Arbeit
</title>
<author confidence="0.496762">
Rainer Schuster
Zubergasse 147, 2020 Sonnberg
</author>
<bodyText confidence="0.858477571428572">
Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwen-
deten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit
– einschließlich Tabellen, Karten und Abbildungen –, die anderen Werken oder dem Internet
im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als
Entlehnung kenntlich gemacht habe.
(Ort, Datum) (Unterschrift Verfasser)
i
</bodyText>
<sectionHeader confidence="0.930787" genericHeader="introduction">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996049166666667">
I want to thank my advisors ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl and Univ.-Ass.
Dipl.-Ing. Christian Schauer. Their constructive feedback and their experience was a big help
for writing and improving this thesis.
Special thanks goes to my family and my friends. Studying can be quite time-consuming
and stressful sometimes and their support is invaluable.
ii
</bodyText>
<sectionHeader confidence="0.917847" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.995274545454545">
In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network
Problem (HRNP) is investigated.
When the network is represented as a graph, an informal problem definition for this NP-
complete problem is: Given a set of network sites (nodes) assigned to one of three layers and
the costs for establishing connections between sites (i.e., edge costs) the objective is to find a
minimum cost connected network under certain constraints that are explained in detail in the
thesis. The most important constraint is that the nodes have to be assigned to rings of bounded
size that connect the layers hierarchically.
The ring structure is a good compromise between the robustness of a network and the cost
for establishing it. It is guaranteed, that the network can continue to provide its service if one
network node per ring fails.
The basic idea in this thesis for solving this network design problem was to cluster the sites
with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ring-
finding heuristics. Previous apporaches for related network design problems did not use the
inherent network structure in such a way. Usual approaches are based on greedy heuristics.
Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan-
Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing
large network structures, also in the context of internet communities.
For finding rings three heuristics were implemented too. Strategic variation of the maximum
allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second
heuristic finds rings by searching for paths that are connected to previously found rings. Third a
repair heuristic was implemented that tries to add remaining nodes to existing rings.
Local search heuristics are applied last to improve the solution quality.
To check how the clustering approach performs for solving the problem of this thesis two
test instance generators were implemented. One generates instances randomly and the second
generates instances based on the popular TSPLIB archive.
The evaluation of the random test instances has shown, that all three clustering heuristics
were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid
solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as
clustering algorithm solutions could be found faster on average, but the resulting costs where
slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to
find valid solutions, but for each test instance at least one type of clustering was successful.
iii
</bodyText>
<subsectionHeader confidence="0.31629">
Kurzfassung
</subsectionHeader>
<bodyText confidence="0.9944078">
In dieser Diplomarbeit wird die Anwendung von Clusteringalgorithmen untersucht, um das
Hierarchical Ring Network Problem (HRNP) zu lösen.
Wenn das Netzwerk als Graph repräsentiert ist, ist dieses NP-vollständige Problem wie folgt
definiert: Gegeben ist Menge von Knoten welche jeweils einer von drei Schichten zugewiesen
sind, und eine Kostenfunktion, welche die Verbindungskosten zwischen zwei Knoten
(d.h. Kantenkosten) zuweist. Gesucht ist ein zusammenhängendes Netzwerk mit minimalen
Gesamtkosten, wobei dieses bestimmte Struktureigenschaften zu erfüllen hat, welche im Detail
in der Diplomarbeit beschrieben werden. Die wichtigste dieser Eigenschaften ist, dass Knoten
gemäß einer hierarchischen Struktur zu größenbeschränkten Ringen verbunden werden.
Ringstrukturen sind ein guter Kompromiss zwischen der Verfügbarkeit von Netzwerken und
deren Herstellungskosten. Die Verfügbarkeit ist gewährleistet, solange maximal ein Knoten pro
Ring ausfällt.
Die grundlegende Idee dieser Diplomarbeit um dieses Netzwerkdesign-Problem zu lösen,
ist die Knoten mit Hilfe von hierarchischen Clusteringalgorithmen anzuordnen und die resul-
tierende Hierarchie für nachfolgende Heuristiken zu verwenden, welche die Ringe finden.
Vorhergehende Ansätze für vergleichbare Netzwerkdesign-Probleme haben die inhärente
Netzwerkstruktur nicht auf solche Weise genützt und eher Greedy-Heuristiken eingesetzt.
Um gültige Ringe zu finden, wurden drei Heuristiken implementiert. Strategisches Variieren
der erlaubten Ringgröße hilft der ersten Heuristik Ringe unter Benützung der Cluster-Hierarchie
zu finden. Die zweite Heuristik baut auf den in der vorherigen Schicht gefundenen Ringen auf,
indem sie nach gültigen Pfaden sucht, die an diese Ringe angeschlossen werden können. Drittens
wird eine Reparaturheuristik angewendet, welche versucht verbleibende Knoten zu bestehenden
Ringen zuzuweisen.
Zuletzt werden lokale Suchverfahren eingesetzt, um die Gesamtkosten zu verbessern.
Um zu überprüfen, wie gut dieser Lösungsansatz funktioniert, wurden zwei Testinstanz-
Generatoren implementiert. Der Erste generiert Instanzen zufallsbasiert, der Zweite baut auf
dem bekannten TSPLIB-Archiv auf.
Die Evaluierung der zufallsbasierten Testinstanzen hat gezeigt, dass alle drei Heuristiken
sämtliche Instanzen lösen konnten, wobei Girvan-Newman und Kernighan-Lin in jedem Testlauf
Lösungen gefunden haben, war dies bei K-means nicht der Fall. Mit Kernighan-Lin
konnte im Durchschnitt schneller eine Lösung gefunden werden, aber die Gesamtkosten waren
bei den beiden anderen Algorithmen etwas besser. Mit den TSPLIB-basierten Testinstanzen
konnte nicht mit allen Clusteringalgorithmen eine Lösung erzielt werden, aber zumindest war
für jede Testinstanz mindestens ein Clustering-Verfahren erfolgreich.
iv
</bodyText>
<note confidence="0.528913">
Contents
</note>
<table confidence="0.952229948717949">
1 Introduction 1
1.1 Problem Definition 2
Notation 2
Definitions 2
Input 5
Objective Function 5
Instance Constraints 5
1.2 About the Complexity 9
2 Related Work 10
3 Methodology 13
3.1 Datastructures 13
Dendrogram 13
Cluster-Tree 13
3.2 Algorithms 14
Cluster Analysis 14
Girvan-Newman Clustering 15
K-means Clustering 20
Kernighan-Lin Clustering 20
Merging Rings 21
2-Opt Heuristic 22
Multilevel Heuristics 23
Floyd Algorithm 23
4 Heuristic Solutions / Implementations 26
4.1 Overview 26
4.2 Hierarchical Clustering Techniques 28
Girvan-Newman Hierarchical Clustering 28
K-means Hierarchical Clustering 28
Kernighan-Lin Clustering 30
4.3 Heuristics for Finding Rings in the Dendrogram 30
Heuristic1: Variate Ringsize Heuristic 30
v
Heuristic2: Subtour Heuristic 31
Heuristic3: Node Insertion Heuristic 32
5 Test Results and Critical Reflection 34
5.1 Test Instance Generation 34
Random Instance Generator 34
TSPLIB Instances 35
5.2 Random Test Instances 37
5.3 TSPLIB-based Test Instances 40
</table>
<sectionHeader confidence="0.672638" genericHeader="method">
6 Summary and Future Work 44
Bibliography 58
</sectionHeader>
<bodyText confidence="0.488227">
vi
</bodyText>
<sectionHeader confidence="0.6436645" genericHeader="method">
CHAPTER 1
Introduction
</sectionHeader>
<bodyText confidence="0.99994612">
The design of networks (e.g., telecommunication, transportation etc.) is undoubtedly an im-
portant task in today’s world. As networks grow larger a need for algorithms that can handle
big instances emerges. Traditionally Integer Linear Programming (ILP) Techniques are used
for solving network design problems because it is able to find optimal solutions. Unfortunately
they are often incapable of solving big instances. Heuristics have shown that they can produce
very good results in far less time. Therefore, various heuristic algorithms are investigated in this
work.
A special focus lies on reliability. Reliable networks often have a ring structure in common
that helps building so called self healing networks. If one hub node fails the traffic can be
rerouted so that the other nodes are not affected. It is even possible to keep the network alive
and continue providing services if one node per ring fails. Other network structures would ensure
high availability and robustness features too, but ring structures have the important benefit that
they can be built at relatively cheap costs.
Nevertheless, heuristics are often not applied for rinding ring structures in networks, because
it is usually difficult to find appropriate methods for establishing those structures. In this thesis
clustering heuristics are taken as an approach to tackle this challenge. Moreover the input graph
is in general not complete, which is closer to natural input instances.
As an example telecommunication networks should be mentioned. Typically they are built
on existing infrastructure, for example beside roads or railways. This implies restrictions on
the design of networks. A natural approach for the design would be to use data about existing
infrastructure as an input model on which the network is built. The challenge would be to select
appropriate parts and to extend the model where necessary. This could mean choosing the roads
where new network cables should be laid. In some situations connections are required where no
appropriate infrastructure exists. Then additional connections have to be inserted in the model,
usually with relatively high costs.
</bodyText>
<sectionHeader confidence="0.668692" genericHeader="method">
1
</sectionHeader>
<subsectionHeader confidence="0.994685">
1.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999909">
This section explains the formal specification of the problem. A convention for the symbolic no-
tation is introduced first. An example input instance is illustrated in Figure 1.1 and one possible
solution is shown in Figure 1.2.
</bodyText>
<equation confidence="0.939327636363636">
Notation
V ... Set of nodes.
E ... Set of undirected edges.
G = (V, E) ... Graph G with vertex set V and edge set E. The notation V (G) stands
for the vertex set of graph G.
K ... Number of layers.
k ... The kth layer, where 1 ≤ k ≤ K.
Vk ... Nodes in layer k.
Ek ... Edges in layer k, i.e., (i, j) ∈ Ek ⇔ (i ∈ Vk ∧ j ∈ Vk).
Ek ... Edges between layer k and layer k − 1, i.e.,
(i, j) ∈ E� k ⇔ ((i ∈ Vk ∧ j ∈ Vk−1) ∨ (i ∈ Vk−1 ∧ j ∈ Vk)).
</equation>
<bodyText confidence="0.9194253">
Rk,i... The ith ring in layer k. The notation G[Rk,i] stands for the graph
representing the ring Rk,i.
blk ... Lower bound of nodes for a ring in layer k, i.e., a ring in layer k can
have at least l nodes.
buk . . . Upper bound of nodes for a ring in layer k, i.e., a ring in layer k can
have up to u nodes.
cij ... Cost of edge (i, j) ∈ E.
xij ... Variable which is 1 (true) exactly if the edge (i, j) ∈ E is part of the
solution, otherwise it is 0 (false).
l : V → N ... Function l that assigns a layer (level) to each node.
</bodyText>
<sectionHeader confidence="0.538373" genericHeader="method">
Definitions
</sectionHeader>
<bodyText confidence="0.996852777777778">
A chain Ck is a node disjoint path, i.e., a sequence of disjoint edges. All nodes that are part of
the chain must be on the same level: ∀e ∈ Ck : e ∈ Ek.
A ring R is a node disjoint cycle, i.e., a sequence of disjoint edges e ∈ E, that start and end
in the same node. It can be seen as a closed path. Ring is a figurative expression that symbolizes
the intended structure. A ring Rk consists of two chains, the upper chain and the lower chain
and two connection-edges (uplinks) u1, u2 ∈ Ekk. The upper chain consists of the nodes that are
part of the connected ring (a path between the hub nodes). The lower chain is a path between
the hub nodes that essentially consists of the nodes to be connected to the upper layer. For a ring
Rk the upper chain is in layer k − 1 and the lower chain is in layer k.
</bodyText>
<page confidence="0.65598">
2
</page>
<figure confidence="0.993588666666667">
6
1
13
12
11
14
10
5
2
15
9
16
0
7
3
17
8
4
</figure>
<figureCaption confidence="0.6769215">
Figure 1.1: Sample input instance. Blue nodes (0 – 2) are assigned to layer 1, green nodes (3 –
8) to layer 2 and red nodes (9 – 17) to layer 3.
</figureCaption>
<figure confidence="0.995239157894737">
3
6
1
13
12
11
14
10
5
2
15
9
16
0
7
3
17
8
4
</figure>
<figureCaption confidence="0.986544">
Figure 1.2: Solution for sample instance (see Figure 1.1).
</figureCaption>
<page confidence="0.709307">
4
</page>
<subsectionHeader confidence="0.401043">
Partition Types / Cluster Types
</subsectionHeader>
<bodyText confidence="0.952902666666667">
In this thesis the special case of hierarchical clustering with ring structures is used. The term
cluster can have various intended meanings. The meanings used in this thesis are explained in
the following:
</bodyText>
<listItem confidence="0.969882666666667">
• Type R: Ring
A Type R cluster contains the complete ring Rk.
• Type A: Ring + Subrings
A Type A cluster contains the nodes of the ring and all the nodes of the lower levels that
are connected to the ring.
• Type Lc
</listItem>
<bodyText confidence="0.5822465">
A Type Lc cluster contains only the lower chain.
Input
</bodyText>
<listItem confidence="0.9976495">
• Connected (undirected) simple graph G = (V, E)
• Each edge e = (i, j) has an assigned weight: cij
• Each node v has an assigned layer (level): l(v)
• The number of layers K equals 3
• The graph is preprocessed so that it contains no edge connecting nodes in V1 and V3 is
contained
</listItem>
<subsectionHeader confidence="0.379133">
Objective Function
</subsectionHeader>
<equation confidence="0.6560665">
�min cij · xij (1.1)
i,jEV
</equation>
<bodyText confidence="0.87364">
Minimize the sum of edge weights of the solution.
</bodyText>
<subsectionHeader confidence="0.447873">
Instance Constraints
</subsectionHeader>
<bodyText confidence="0.985042">
The formal problem description is split into two cases: The definition for the backbone ring V1,
which can be treated as a separate problem, and the definition for the other layers.
</bodyText>
<equation confidence="0.879077428571429">
Formulation for k = 1:
X xij = 2, i ∈ V1 (1.2)
(i,j)EE1
E xij ≥ 2, ∀S ⊂ V1, S =6 ∅, V1 (1.3)
(i,j)EE1liES,j /ES ∈ {0, 1}, ∀(i, j) ∈ V1 (1.4)
xij
5
The constraints for the case k = 1 ensure that the nodes in the first layer build a Hamiltonian
cycle. It is a classical TSP formulation [3].
Formulation for 1 &lt; k &lt; K:
For each layer k a set of rings {Rk,1, ... Rk,mk } has to be found with the following constraints:
Rk,i C E, bi = 1, ... , mk (1.5)
|V (G[Rk,i]) n Vk |&gt; blk, bi = 1, ... , mk (1.6)
|V (G[Rk,i]) n Vk |&lt; buk, bi = 1, ... , mk (1.7)
|V (G[Rk,i]) n Vk−1 |&gt; 2, bi = 1, ... , mk (1.8)
(V (G[Rk,i]) n Vk) n (V (G[Rk,j]) n Vk) = 0, bi, j E 1, ... , mk ∧ i =6 j (1.9)
mk
V (G[ � Rk,i]) n Vk = Vk (1.10)
i=1
mk &gt; 1 (1.11)
xij = 1, b(i, j) E {Rk,1, ... Rk,mk} (1.12)
</equation>
<bodyText confidence="0.989030625">
Constraints 1.6 and 1.7 restrict the chain size to the lower and upper bound. Each chain has
to be homed to two hubs in the upper chain 1.8. The lower chains (of the same level) must be
node disjoint 1.9. Each node has to be contained in one ring 1.10. All levels must consist of at
least one node 1.11 e.g., it is not possible that there is no ring in level 2. Constraint 1.12 connects
these constraints with the objective function 1.1 which means that for all edges contained in rings
the objective variable x has to be set to 1 (true).
Informal constraints for the heuristics:
In the following constraints are rewritten informally for a better understanding.
</bodyText>
<listItem confidence="0.999650444444444">
• The level difference of the endpoints of each edge e = (v1, v2) in a ring solution must be
&lt; 1. For example an edge between a level 1 node and a level 3 node must not exist.
• Each ring with k &gt; 1 has to be connected to 2 hub nodes (2-connectivity). This is also
called dual-homing. For a violation see Figure 1.4b and Figure 1.4d.
• Exactly three layers are used, i.e., K = 3.
• The hubs a ring is connected to must be on the same ring. For a violation see Figure 1.3a.
• The hubs a ring is connected to must be distinct (see Figure 1.3b).
• All nodes must be connected to form a single component.
• Level of hub = level of connected node + 1 (see Figure 1.4a).
</listItem>
<figure confidence="0.986954806451613">
6
(a) Violation 1: The layer 3 ring is homed to
two different layer 2 rings.
(b) Violation 2: The layer 2 ring is homed to
the same layer 1 node twice.
15 8 11
16
17
18
6
7
5
19
9
2
1
10
12
3
13
14
4
3
2
1
4
9
5
8
6
7
</figure>
<figureCaption confidence="0.991725">
Figure 1.3: Some violations of the instance constraints. An upper bound of u = 5 is assumed.
</figureCaption>
<listItem confidence="0.9973006">
• The ringsize is bounded by buk (see Figure 1.4c). This avoids degenerate solutions where
too long rings are built, similar to the TSP.
• If blk = 1 it is allowed that a lower chain consists of only one node. This helps finding
solutions in layer k, but avoids homing subrings of layer k + 1 to this ring because it
prohibits dual-homing.
</listItem>
<page confidence="0.661248">
7
</page>
<figure confidence="0.991150280701754">
18
17
13
19
12
14
16
4
11
15
3
10
1
2
5
9
6
8
7
3
2
1
4
9
5
8
6
7
(a) Violation 3: The layer three ring is connected to a layer 1 ring
(but it must be connected to a layer 2 ring).
13
14
12
3
4
11
2
1
10
5
9
6
8
7
(c) Violation 5: The layer 2 ring is too long (it ex-
ceeds the bound u = 5).
(b) Violation 4: The layer 2 ring is only con-
nected once (no dual homing).
2 3
1
4 5
9
8
6
7
(d) Violation 6: The solution rings are not con-
nected.
</figure>
<figureCaption confidence="0.985814">
Figure 1.4: Some violations of the instance constraints. An upper bound of u = 5 is assumed.
</figureCaption>
<page confidence="0.808969">
8
</page>
<subsectionHeader confidence="0.984242">
1.2 About the Complexity
</subsectionHeader>
<bodyText confidence="0.99944245">
To get a better understanding how difficult it is to solve a combinatorial problem is, it can
be investigated by means of complexity analysis, which is a major topic in computer science.
One of the most interesting questions is, if the considered problem belongs to the set of NP-
complete problems, which means that no algorithm with polynomial time complexity can exist
to solve this problem, as long as we can assume that P =6 NP. If one could find an algorithm
of polynomial time complexity that can solve at least one of the NP-complete problems, this
would mean that all problems in this complexity class could be solved with a polynomial time
algorithm. This would also have the impact that the assumption that P =6 NP holds could be
rejected.
In the case of the HRNP the first layer can be treated independently as a Traveling Salesman
Problem (TSP), see [14], which is NP-complete. The optimal ring connection of all nodes of
V1 resembles an optimal TSP tour for these nodes.
The problem to find appropriate rings for the other layers can be reduced from the Traveling
Salesman Problem with Precedence Constraints (TSPPC), see [13], which was shown to be
NP-hard.
For layer 2 (layer 3) this means that if exactly two uplinks are contained that determine the
precedence and bu2 = |V2 |(bu3 = |V3|), the single ring connecting all nodes of V2 (V3) resembles
a TSPPC tour.
To know that there is no polynomial algorithm that solves the Hierarchical Ring Network
Problem motivates the use of heuristics to tackle larger instances, as it was done in this thesis.
</bodyText>
<equation confidence="0.3854855">
9
CHAPTER 2
</equation>
<sectionHeader confidence="0.308144" genericHeader="related work">
Related Work
</sectionHeader>
<subsectionHeader confidence="0.869189">
Hierarchical Network Design Problem
</subsectionHeader>
<bodyText confidence="0.999958142857143">
Current introduces the basic Hierarchical Network Design Problem (HNDP) in [5]. It consists of
primary nodes building a primary path that from a start- to an end-node. The other nodes are the
secondary nodes that have to be connected to the primary nodes via secondary paths. The total
cost is the cost of the primary path plus the sum of costs of all secondary paths. The objective is
to minimize the total cost. The paper contains an ILP formulation and a heuristic approach. The
heuristic calculates the M shortest paths between the start- and the end-node and then calculates
an MST for each of them and takes the minimum of the M solutions.
</bodyText>
<subsectionHeader confidence="0.720171">
Multi-level Network Design Problem
</subsectionHeader>
<bodyText confidence="0.996805166666667">
In [1] the Multi-level Network Design (MLND) problem is introduced. It is a generalization
of the Hierarchical Network Design (HNDP) problem, where K levels are used to describe the
importance of nodes. If two levels are used then this is called the Two-level Network Design
(TLND) problem. The generalization of the HNDP is that the primary layer can consist of more
than exactly two designated nodes. An ILP formulation based on steiner trees and one based on
multicommodity flow are provided.
</bodyText>
<subsectionHeader confidence="0.960489">
Aspects of Network Design
</subsectionHeader>
<bodyText confidence="0.999563">
Klincewicz [12] investigates various aspects of network design, like the cost, capacity, reliability,
performance and demand pattern of networks. The cost can be important for hubs (nodes) and
links either for creating or for using them. Also for both of them capacity constraints can apply.
Reliability can be improved by multi-homing or by the more general approach that multiple
paths between nodes are available for rerouting. The performance measures if enough networks
resources are available for a given demand, e.g., if enough capacity is available along some path.
</bodyText>
<page confidence="0.534638">
10
</page>
<bodyText confidence="0.999906666666667">
Demand patterns describe the needed communication between the nodes, for example many-to-
many or many-to-one node relations. The paper also gives a good survey about combinations of
network structures between backbone (primary) and tributionary (secondary) topologies.
</bodyText>
<subsectionHeader confidence="0.989249">
Survivable Networks with Bounded Rings
</subsectionHeader>
<bodyText confidence="0.97933">
Fortz uses in this phd thesis [8] bounded rings for the reliability of networks. The resulting
network has to be connected. A branch-and-cut approach and various heuristics are used to find
the rings.
Some of the heuristics are shortly explained below (the names of the heuristics are from the
original thesis):
</bodyText>
<subsectionHeader confidence="0.8414">
Ear-inserting method
</subsectionHeader>
<bodyText confidence="0.9843869">
This method creates one ring after the other. At first a minimum length cycle is created. Then
the node is chosen to be inserted into the ring that has the least insertion cost (i.e., replacing
an existing edge by the two new ones that connect the node to the ring) until no node can be
inserted without violating ring constraints. Then a new cycle is created and the procedure is
repeated until all nodes are inserted.
Cutting cycles into two equal parts
A solution should be found where the bound constraint is relaxed. This may lead to a Hamilto-
nian cycle. Later a cycle, where the bound is violated, is split into two parts and edges are added
at the splitting points to get two cycles. This procedure is repeated recursively, until the bound
constraints are fulfilled.
</bodyText>
<subsectionHeader confidence="0.752925">
Path following method
</subsectionHeader>
<bodyText confidence="0.9411362">
At first a Hamiltonian cycle is created. Next the algorithm follows this tour (starting at some
arbitrary point in some arbitrary direction) as long as the bound constraint is satisfied. Then an
edge back to the starting point is added to close the cycle and the next point (following the tour)
becomes the new starting point. This procedure is repeated until the first starting point is reached
again.
</bodyText>
<subsectionHeader confidence="0.932792">
Stringy method
</subsectionHeader>
<bodyText confidence="0.999724">
The noticeable feature of this method is that it starts with all edges and removes edges system-
atically. The crucial criterion for removing an edge is that the graph stays 2-connected after the
removal.
</bodyText>
<page confidence="0.673141">
11
</page>
<subsectionHeader confidence="0.806403">
Hierarchical Network Topologies
</subsectionHeader>
<bodyText confidence="0.9999515">
Thomadsen’s phd thesis [18] focuses on hierarchical network topologies. Many properties are
described as ILP formulations. A chapter about ring structures is provided. The thesis contains a
formulation of the Fixed Charge Network Design (FCND) problem. It involves demand between
nodes, edge costs and the cost to use edges (satisfy demand).
</bodyText>
<subsectionHeader confidence="0.845732">
Ring-chain Dual Homing
</subsectionHeader>
<bodyText confidence="0.43773975">
Lee [15] describes Self Healing Rings (SHR) in the context of the Ring-chain Dual Homing
(RCDH) problem. An SHR is a cycle of network nodes that can reroute the traffic in case of a
node failure. A chain is a path of nodes that is linked (homed) to distinct hub nodes on the SHR
at each chain end.
</bodyText>
<page confidence="0.31336">
12
</page>
<sectionHeader confidence="0.326189" genericHeader="method">
CHAPTER 3
</sectionHeader>
<subsectionHeader confidence="0.557251">
Methodology
</subsectionHeader>
<bodyText confidence="0.998515">
In this chapter the theoretical background is explained. It mainly contains the data structures
and algorithms that are used for the implementation.
</bodyText>
<subsectionHeader confidence="0.988908">
3.1 Datastructures
</subsectionHeader>
<bodyText confidence="0.999684">
Different ways are used to model application data. Especially the hierarchical aspect and the
ring structure are important.
</bodyText>
<subsectionHeader confidence="0.753159">
Dendrogram
</subsectionHeader>
<bodyText confidence="0.9999198">
A dendrogram is a tree that can be used for the hierarchical representation of data. Nodes that
are closer to the root (i.e., the path length from the node to the root is shorter) are hierachically
on a higher level. The root stands for all values, and the leafs represent the values that are
hierachically structured. E.g., if the values are nodes in a graph, each subtree in the dendrogram
can be seen as a cluster. For an example see Figure 3.1.
</bodyText>
<subsectionHeader confidence="0.528495">
Cluster-Tree
</subsectionHeader>
<bodyText confidence="0.9997688">
A cluster-tree is the basic abstraction for the (partial) solution graph. In the case of this topic
each node in the tree represents a cluster and therefore, it stands for many nodes in the network.
To be more precise it is intended to build a ring from the nodes of the cluster.
The edges of the cluster-tree stand for the connections between the rings. This means if two
nodes from the cluster-tree are connected via an edge their rings are connected (“homed”) in the
graph. In the case of dual-homing the edge stands for the two connections from the subring to
the hubs of the upper ring. Since uplinks are only allowed to the same ring, one edge is sufficient
for representing this connection. It is important to note that the nodes and edges building the
cluster-tree are abstract representations of the elements in the original graph, but they are not
contained there.
</bodyText>
<page confidence="0.521596">
13
</page>
<figure confidence="0.780078">
12 3 4 5 6
1 2 3 4 5 6 7
1 2 3 4 5 6 7 8 9 10 11 12 13 14
1 2 3 4 5 6 7 8 9 10 11 12 13 14
8 9 10 11 12 13 14
9 10 11 12 13 14
</figure>
<figureCaption confidence="0.755132">
Figure 3.1: Dendrogram sample. Nodes 1, 2 and 3, for example are hierachically on a lower
</figureCaption>
<bodyText confidence="0.977812333333333">
level than node 7.
The cluster-tree is similar to the dendrogram structure of clustered graphs, with two main
differences:
</bodyText>
<listItem confidence="0.998460333333333">
1. Each node in the cluster-tree should represent a ring, whereas nodes in the dendrogram
stand for regions and general clusters, which need not have the same granularity.
2. A dendrogram node stands for a cluster including all subclusters (i.e. Type A). In the
cluster-tree a node only stands for the nodes in the respective ring (i.e. Type R). One
complete subtree (i.e., the union of its nodes) of the cluster-tree would correspond to one
dendrogram node.
</listItem>
<subsectionHeader confidence="0.999317">
3.2 Algorithms
</subsectionHeader>
<bodyText confidence="0.999991">
The focus in this section lies on clustering algorithms. Moreover, improvement and repair
heuristics are explained here too. The last part is about other algorithms that are important
for the implementation.
</bodyText>
<subsectionHeader confidence="0.96801">
Cluster Analysis
</subsectionHeader>
<bodyText confidence="0.9999216">
Clustering is a process where a set of data is divided into subsets so that the elements in each
subset are similar according to some distance function. The subsets are called clusters and can
be seen as groups of similar items. The terms cluster analysis and clustering are used synomy-
mously. Cluster analysis is commonly used in statistics, where the data are usually statistical
observations. For a simple illustration of a clustering example, see Figure 3.2.
The elements are typically represented as vectors in n-dimensional space, in which each
dimension defines an element’s property.
A distance metric is a function that takes two vectors as input and assigns a non-negative
number based on the relative positions of the vectors to each other as output. A common distance
metric is the Euclidean distance, where the distance function d for the vectors x and y is defined
</bodyText>
<figure confidence="0.864046">
14
(a) Original set. (b) Result of the cluster analysis.
</figure>
<figureCaption confidence="0.996333">
Figure 3.2: Cluster analysis example. Elements are clustered according to their color property.
</figureCaption>
<bodyText confidence="0.826685">
as follows:
</bodyText>
<equation confidence="0.5575855">
d(x, y) = v u uXn (xi − yi)2
ti=1
</equation>
<bodyText confidence="0.996102">
Other metrics for example are the Manhattan distance, Hamming distance or Mahalanobis dis-
tance (e.g., where normalization of the vector is needed).
</bodyText>
<subsectionHeader confidence="0.87109">
Girvan-Newman Clustering
</subsectionHeader>
<bodyText confidence="0.986207909090909">
The Girvan-Newman algorithm [6] is based on finding components in a network. It was success-
fully applied to various social networks. One usecase is the detection of online communities.
A simple approach to cluster a network is to detect its connected components. Since real
world networks are highly connected so that usually just one giant component is contained, the
betweenness measure is used instead.
Betweenness can be defined on the nodes V and on the edges E of a graph G = (V, E).
In the following σst is defined as the number of shortest paths (SP) between the nodes s and t,
σst(v) is defined as the number of SP between the nodes s and t that “run through” the vertex v
and σst(e) is defined as the number of SP between the nodes s and t that “run along” the edge e.
This leads to the following formula definitions [9, 4]:
Node betweenness of vertex v: Z
</bodyText>
<equation confidence="0.498396">
s6=v6=t∈V
</equation>
<bodyText confidence="0.86776">
Edge betweenness of edge e: Z
</bodyText>
<equation confidence="0.591602">
s6=t∈V
</equation>
<bodyText confidence="0.9712858">
Girvan-Newman algorithm in pseudocode is shown in Algorithm 3.1.
The algorithm works as follows: At the beginning the graph consists of one component. In
each iteration the algorithm determines the edge e with the highest betweenness value. This
edge is removed. Then the algorithm checks if the graph can be split, which means that one
component can be separated. If this is the case, the original component (before splitting) that
</bodyText>
<figure confidence="0.930334058823529">
15
σst(v)
σst
σst(e)
σst
Algorithm 3.1: The Girvan-Newman algorithm.
input : A connected graph G = (V, E)
output: A hierarchical clustering of the graph G
1 while edges left in graph G do
2 calculate betweenness values for edges e E E;
3 delete edge e with highest betweenness;
4 if graph splits in more components then
5 build a new cluster for each component;
6 add each new cluster as subcluster to its parent cluster;
7 end
8 end
9 return cluster hierarchy;
</figure>
<bodyText confidence="0.92842">
stands for a cluster is split into two subclusters, which are hierarchically attached to the original
cluster. This gives two corresponding subdendrograms. After that the next iteration is started.
This procedure is continued until no edges are left so that the graph is fully transformed into a
dengrogram and the hierarchical clustering process is finished.
Note that no edge weights are considered. Only the shortest paths influence the betweenness.
To illustrate the Girvan-Newman algorithm a detailed example follows.
Example
Given an undirected weighted graph G = (V, E) with vertices
</bodyText>
<equation confidence="0.978814">
V = {1,..., 11}
edges
E = { (1, 2) , (1, 3) , (2, 3) , (2, 4) , (3, 4) ,
(4, 5) , (5, 6) , (5, 7) , (6, 7) , (7, 8) ,
(8, 9) , (8, 10) , (9, 10) , (9, 11) , (10, 11) }
</equation>
<bodyText confidence="0.603099">
and (edge) weight function
</bodyText>
<equation confidence="0.940136">
w(e) = 1, be E E
</equation>
<bodyText confidence="0.986750428571429">
which leads to the adjacency matrix in Table 3.4. The graph is shown in Figure 3.3.
To calculate the edge betweenness values the first step is to calculate the number of shortest
paths between nodes. An efficient method that is based on Breadth-First-Search is described in
[6]. The resulting SP values are shown in Table 3.1.
Next, the proportion of the number of SP passing trough each edge has to be calculated. For
each node pair (s, t) with s =74 t E V the set of all SP has to be determined and for each edge
e E E the number of paths that contain e will give the value for σst(e).
</bodyText>
<figure confidence="0.981404909090909">
16
3
1
6 7
4
2
5
8
9
10
11
</figure>
<figureCaption confidence="0.967541">
Figure 3.3: Visual representation
of the example graph.
</figureCaption>
<figure confidence="0.999538583333333">
1 2 3 4 5 6 7 8 9 10 11
111
2 111
3 111
4 111
5 111
6 11
7 111
8 111
9 111
10 111
11 11
</figure>
<figureCaption confidence="0.9981782">
Figure 3.4: Adjacency matrix of the example graph.
Empty values indicate that there is no edge between the
specific nodes (0-values were omitted for readability).
Figure 3.5: The example graph.
Figure 3.6: Both shortest paths between the nodes 1 and 4.
</figureCaption>
<bodyText confidence="0.99813525">
For example for s = 1 and t = 4 there are 2 shortest paths (therefore σ1,4 = 2), namely
((1, 2) , (2, 4)) and ((1, 3) , (3, 4)). This set of shortest paths contains the edges (1, 2), (1, 3),
(2, 4) and (3, 4). Each of these edges occurs in exactly one shortest path (between 1 and 4), so
σ1,4((1, 2)) = σ1,4((1, 3)) = σ1,4((2, 4)) = σ1,4((3, 4)) = 1. Therefore, each of the propor-
</bodyText>
<figure confidence="0.985980064516129">
17
(a) First shortest path (b) Second shortest
(green) between node path (green) between
1 and node 4. node 1 and node 4.
1
2
3
4
1
2
3
4
tionsσ1,4(e)
σ1,4
21
illustration.
is
. See Figure 3.6 for
The resulting betweenness values after the first iteration are shown in Table 3.2 and the graph
1 2 3 4 56 7 8 9 10 11
111 2 2 2 2 2 2 24
2 11111111 2
3 1111111 2
4 111111 2
5 11111 2
6 1111 2
7 111 2
8 11 2
9 11
10 1
11
</figure>
<tableCaption confidence="0.7727365">
Table 3.1: Number of shortest paths between nodes (σst). Only the upper triangle matrix is
shown, the other half is symmetric since the graph is undirected.
</tableCaption>
<figure confidence="0.993084181818182">
1 2 3 4 56 7 8 9 10 11
1 10 10
2 2 24
3 24
4 56
5 10 50
6 10
7 56
8 24 24
9 2 10
10 10
</figure>
<tableCaption confidence="0.925899">
Table 3.2: Betweenness values after the first iteration.
</tableCaption>
<page confidence="0.83464">
11
</page>
<bodyText confidence="0.998894428571429">
in Figure 3.7a. The edges (4, 5) and (7, 8) have the highest betweenness value of 56. If binary
clustering is desired, only one of those edges will be removed, otherwise all edges with the
highest value are removed from the graph.
In this case the graph splits into three components.
After that the betweenness calculation process starts again (for each component). For the
next iteration, the exact values are omitted, but the resulting graph is shown in Figure 3.7b.
The result of the clustering process can now be seen in Figure 3.8.
</bodyText>
<figure confidence="0.98647588">
18
(a) Graph after the first iteration.
(b) Graph after the second iteration.
1
3
2
4
5
9
7
8
11
6
10
1
3
2
4
5
7
6
9
8
11
10
</figure>
<figureCaption confidence="0.998325666666667">
Figure 3.7: Example graph after iteration 1 and 2. Edges of highest betweenness value (red) are
removed at that iteration.
Figure 3.8: Resulting clustering of the example graph respresented as a dendrogram.
</figureCaption>
<figure confidence="0.707440428571428">
1 2 3 4 5 6 7 8 9 10 11
1 2 3 4
2 3
9 10
1 2 3 4 5 6 7 8 9 10 11
5 6 7 8 9 10 11
19
</figure>
<subsectionHeader confidence="0.799441">
K-means Clustering
</subsectionHeader>
<bodyText confidence="0.96653625">
K-means is a popular clustering technique that partitions the node set of a graph into k distinct
clusters. For each cluster one node acts as a special element (the centroid) from which the
distance can be calculated.
K-means in pseudocode can be found in Algorithm 3.2.
</bodyText>
<figure confidence="0.847072857142857">
w(a,b)
20
Algorithm 3.2: Pseudocode of the K-means algorithm.
input : The set V of elements to be clustered. A weight function w. The number of
expected clusters k.
output: An assignment from elements to centroids.
1 Choose k distinct elements from V as the centroids;
2 repeat
Assign each element from V to the nearest centroid;
Calculate the new center of each cluster;
5 until stop criterion;
6 return assignment;
3
4
</figure>
<bodyText confidence="0.993051727272727">
A typical stop critertion is met if the centroids did not change between two iterations, or if a
maximum number of iterations was executed.
The initial centroids (first step in pseudocode) are usually chosen randomly. The Floyd-
Algorithm 3.6 is one possible method to find such a random selection. K-means++ is a variant
that chooses the initial centroids in a more uniformly distributed way to avoid a selection where
the centroids are too close.
A fast method for K-means clustering is described in [17].
Partitioning Around Medoids is a variant of K-means. The medoids are analogous to the
centroids from K-means. This algorithm systematically checks new medoid assignments by
swapping current medoids with non-medoids one-by-one and checks if this new assignment
gives an improvement. The pseudocode is shown in Algorithm 3.3.
</bodyText>
<subsectionHeader confidence="0.759631">
Kernighan-Lin Clustering
</subsectionHeader>
<bodyText confidence="0.981610285714286">
The Kernighan-Lin-Algorithm [11] is another approach to solve the graph partitioning problem.
It splits the set of vertices of a weighted graph into two subsets. The subsets have to be disjoint
and of equal size. The sum of weights of the edges between the subsets has to be minimized.
For the pseudocode see Algorithm 3.4.
More formally: Given a weighted graph G = (V, E) with weight function we a partition of
V into the sets A and B with A n B = 0 and IAI = IBI should be found, with the following
objective function:
</bodyText>
<equation confidence="0.649178">
min E
(a,b)∈E:a∈A∧b∈B
Algorithm 3.3: Pseudocode of the Partitioning Around Medoids algorithm.
input : The set V of elements to be clustered. A weight function w. The number of
</equation>
<bodyText confidence="0.808269">
expected clusters k.
</bodyText>
<listItem confidence="0.510205333333333">
output: An assignment from elements to medoids.
1 Choose k distinct elements from V as the medoids;
2 repeat
</listItem>
<bodyText confidence="0.765813166666667">
Assign each element from V to the nearest medoid;
foreach medoid m do
foreach element e that is no medoid do
Swap m and e;
if new minimal cost assignment found then
Save assignment as new minimum;
</bodyText>
<figure confidence="0.926529714285714">
3
4
5
6
7
8
9
10
11
end
end
end
12 until medoids not changed;
13 return assignment;
V must contain an equal number of elements (if this is not the case, an artificial element can
be added).
Additionally the following terms are introduced:
The external cost Ea of an element a E A is defined as P
bEB
The internal cost Ia of an element a E A is defined as Z w(a,b).
bEA
</figure>
<bodyText confidence="0.955946692307692">
The cost difference Da is defined as Da = Ea — Ia.
If a node a E A is moved to B and a node b E B is moved to A, the cost reduction can be
calculated by Da + Db — 2w(a,b). This formula is an important ingredient for the Kernighan-Lin
algorithm, which tries to maximize the cost reduction.
In each iteration of the algorithm the cost difference for each element is calculated (i.e., the
cost difference if the element is moved to the other set). For IV I
2 a pair of items (one item from
each set) is searched to maximize the cost reduction. This gives a sequence of cost reductions
with IVI
2 items. Note that cost reduction can also be positive. The subsequence starting at the
first element that has the best total cost reduction is chosen in each iteration. The corresponding
items are then swapped between the sets. These iterations are repeated until no cost reduction
(gain) can be found.
</bodyText>
<subsectionHeader confidence="0.942374">
Merging Rings
</subsectionHeader>
<bodyText confidence="0.99996425">
Some ring-finding heuristics tend to produce small rings. To improve the resulting cost, and
– even more important – for finding rings in lower layers, rings that are close to the upper
bound are better. Merging rings is a simple improvement heuristic, that tries to connect pairs
of rings by concatenating their lower chains. There are four possibilities how the chains can
</bodyText>
<page confidence="0.495576">
21
</page>
<figure confidence="0.942259673076923">
w(a,b).
Algorithm 3.4: Pseudocode of the Kernighan-Lin algorithm.
input : The set V of elements to be partitioned. A weight function w.
output: The resulting partitions A and B.
1 split V into equal initial sets A and B;
2 repeat
A← A;
B← B;
foreach a E A do
compute Da;
end
foreach b E B do
compute Db;
end
for p ← 1 to |V|
2 do
find a¯ E A and b E B that maximize cost reduction ¯g;
a1, ← ¯a, b1, ← b, g1, ← ¯g;
move a¯ to B;
move b to A;
update affected D values;
end
find k that maximizes gain ←
if gain &gt; 0 then
move a1 ... ak to B;
move b1 ... bk to A;
gi;
Pk
i←1
end
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23 until gain &lt; 0;
24 return A and B;
</figure>
<bodyText confidence="0.9792125">
be concatenated at their endpoints. A reversal of the order of edges in the sequence might be
necessary, depending on the representation of chains.
</bodyText>
<sectionHeader confidence="0.644919" genericHeader="method">
2-Opt Heuristic
</sectionHeader>
<bodyText confidence="0.977648636363636">
The 2-Opt heuristic is an optimization heuristic, that is often used to improve TSP solutions.
For all pairs (therefore 2) of edges {(a, b) , (c, d)} the algorithm checks whether a new ordering
of the four considered vertices {(a, d) , (c, b)} improves the solution or not. If a solution in
Eucledian space contains closing edges (i.e., they are part of the same cycle and they “cross”),
it can be improved by the 2-Opt heuristic.
A generalization of 2-Opt is the k-Opt heuristic, where an enhancement of k edges are
checked. A common variant of the k-Opt heuristic is the Lin-Kernighan algorithm [16]. If k-
22
Opt heuristic with higher k is used better solutions can be found, but the processing is also much
higher. The variant 3-Opt is usually a good compromise between runtime and optimization
gained.
</bodyText>
<subsectionHeader confidence="0.613711">
Multilevel Heuristics
</subsectionHeader>
<bodyText confidence="0.999970555555555">
The multilevel paradigm can be used for optimization problems, especially in the case of com-
binatorial optimization problems [20]. The approach is to coarsen the problem to get an approx-
imate solution. Each coarsening iteration stands for a level in the multilevel algorithm.
Various methods exist that make use of this paradigm. Concerning problems in the field of
graph theory for example, one could try to coarsen the graph (e.g., by reducing nodes) and to
solve the problem on the reduced graph.
In this thesis a multilevel approach will be used for the variation of the bound constraint (i.e.,
the restricted ring size buk). Note that a valid solution for a ring bound ¯bu k with ¯bu k ≤ buk is also a
valid solution for the problem with ring bound buk.
</bodyText>
<subsectionHeader confidence="0.836847">
Floyd Algorithm
</subsectionHeader>
<bodyText confidence="0.999848555555556">
Selecting a random subset of items is a problem that occurs in various situations. A good exam-
ple would be the test instance generator (from Section 5.1). In this case k neighbors should be
chosen randomly for each node (where k can also vary randomly).
A naive approach would be to choose items randomly until k distinct items have been se-
lected, see Algorithm 3.5. Nevertheless, much better approaches exist (e.g., [19], [7] and [10]),
especially when a large fraction of all the items has to be chosen. In this case the naive algo-
rithm would have to generate many random values until one element is selected, that was not
selected before, since the probability of choosing an item, that was already chosen grows with
every successful iteration, making this approach unusable for practical applications.
</bodyText>
<construct confidence="0.305870666666667">
Algorithm 3.5: Naive Sampling Algorithm [2]
input : The number of integers k that should be selected out of n.
output: A set S of randomly selected integers.
</construct>
<figure confidence="0.930723">
1 S �-- 0;
2 while ISI &lt; k do
3 t �-- RandomInteger(1, n);
4 if t ∈/ S then
5 insert t in S;
6 end
7 end
8 return S;
In this thesis Floyd’s algorithm [2] was chosen to generate random subsets, see Algorithm
3.6. In each iteration an element is chosen. Either a random number from 1 to j is added, or the
current value of the iterator variable j.
23
Algorithm 3.6: Floyd’s Iterative Sampling Algorithm [2]
input : The number of integers k that should be selected out of n.
output: A set S of randomly selected integers.
1 S �-- 0;
2 for j �-- n — k + 1 to n do
3 t �-- RandomInteger(1, j);
4 if t ∈/ S then
5 insert t in S;
6 else
7 insert j in S;
8 end
9 end
10 return S;
Algorithm 3.7: Floyd’s Permutation Algorithm [2]
input : The number of integers k that should be selected out of n.
output: A sequence S (i.e., permutation) of randomly selected integers.
1 S �-- ();
2 for j �-- n — k + 1 to n do
3 t �-- RandomInteger(1, j);
4 if t ∈/ S then
5 prefix t to S;
6 else
7 insert j in S after t;
8 end
9 end
10 return S;
</figure>
<bodyText confidence="0.997757375">
The permutation version, of the Algorithm 3.7, uses the same basic approach, but uses a
list (sequence) structure instead of a set to obtain an order of the elements. The insertion order
differs, as can be seen in pseudocode.
For implementation purposes the elements, from which a subset has to be selected, are con-
tained in some collection data structure. The mapping from integers to elements can be done
quite easily by constructing an ordered data structure (if it is not already ordered) and interpret-
ing the integer as the element’s index. Therefore, generic methods can be implemented, that
produce those random subsets.
</bodyText>
<page confidence="0.56898">
24
</page>
<table confidence="0.593017333333333">
Full example for Floyd’s permutation algorithm with mapped elements (see Algorithm 3.7):
Index 1 2 3 4 5 6 7 8 9
Element A B C D E F G H I
</table>
<equation confidence="0.785635571428571">
Execution (k = 3):
Start: n = 9, S = ()
Iteration 1: j = 7, t = Randomly chosen number 6, S = (F)
Iteration 2: j = 8, t = Randomly chosen number 3, S = (C, F)
Iteration 3: j = 9, t = Randomly chosen number 3 (conflict), S = (C, I, F)
25
CHAPTER 4
</equation>
<subsectionHeader confidence="0.887934">
Heuristic Solutions / Implementations
</subsectionHeader>
<bodyText confidence="0.998968">
This chapter uses the theoretical foundation of the previous chapter and explains how the tech-
niques are combined to solve the problem of this thesis. Some insight on the implementation is
also given.
</bodyText>
<subsectionHeader confidence="0.969928">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999956095238095">
The generic approach used for this thesis is explained in pseudocode (see Algorithm 4.1), the
heuristics mentioned there will be described in detail later within this chapter.
First the input graph is clustered hierarchically and then rings are searched according to the
hierarchy. For clustering three algorithms were implemented: The Girvan-Newman algorithm,
K-means-clustering and Kernighan-Lin-clustering. Girvan-Newman is already a hierarchical
clustering technique, but the other two had to be slightly adopted to produce the desired den-
drogram. Therefore they are applied in a recursive manner, which means that starting from one
cluster (i.e., the whole graph) the same clustering technique is applied to each subcluster again
until all clusters are split into singletons (i.e., containing only one node that cannot be clustered
anymore).
The second step is to find rings in the dendrogram, according to the constraints of the Hi-
erarchical Ring Network Problem. For this purpose three heuristics were developed. They are
applied layer-by-layer (i.e., from layer 2 to layer 3). Per layer the heuristics are applied sequen-
tially, and each heuristic is repeatedly executed as long as nodes can be assigned to rings. The
first layer is treated separately because it is a Hamiltonian Cycle Problem and need not consider
uplink constraints.
The first heuristic walks through the cluster hierarchy and tries to find valid rings within
clusters. In each cluster that is investigated the Hamiltonian Path Problem is applied to check
for rings (i.e., the lower chain of subrings) within this cluster. The order in which the heuristic
visits each cluster was chosen to vary depending on the upper bound. Resulting rings strongly
depend on the quality of the clustering.
</bodyText>
<figure confidence="0.897082485714286">
26
Algorithm 4.1: Generic algorithm.
input : A graph G = (V, E). A weight function w.
output: A solution graph.
1 Hierarchical clustering;
// Solve Hamiltonian Cycle Problem on V1 nodes
2 Find Hamiltonian cycle in V1;
3 if no Hamiltonian cycle found then
4 return Error!
5 end
6 for l ← 2 to 3 do
// Variate Ringsize Heuristic
Heuristic1(l);
foreach ring r in Vl−1 do
// Subtour Heuristic
Heuristic2(l,r);
end
// Node Insertion Heuristic
Heuristic3(l);
if nodes in Vl left then
return Error!
end
Merge rings;
7
8
9
10
11
12
13
14
15
16 end
17 Improvement heuristics;
18 return solution graph;
</figure>
<bodyText confidence="0.998525666666667">
Since it is already difficult to find valid solutions at all (because the graph is in general not
complete), the second heuristic was intended to consume as many remaining nodes as possible.
It starts from existing rings (from the previous heuristic) and tries to find chains that can be
added to those rings. A depth-first-seach (DFS) based method is used to find the chains that start
and end in rings from the upper layer. Because this DFS can have a very long running time, it
had to be slightly modified.
To insert remaining nodes in rings the third heuristic tries to insert remaining nodes between
edges of rings from the layer.
In some cases rather small rings are created. This can lead to difficulties in the lower layers to
satisfy the dual-homing constraint. Therefore an improvement algorithm is applied, that merges
rings at the end of each layer iteration.
The last phase after all nodes were assigned to rings is local improvement. Therefore, tech-
niques like 2-Opt, 3-Opt and node exchange between rings were implemented. Node exchange
is a local search technique that swaps two nodes from two distinct rings from the same layer if
this improves the total cost.
</bodyText>
<figure confidence="0.971421666666667">
27
2
4 10
5 7 6 8
3
11 16 14 13 17 15
12
10
9
</figure>
<figureCaption confidence="0.949461666666667">
Figure 4.1: Girvan-Newman clustering of the sample instance (see Figure 1.1).
An example input instance is illustrated in Figure 1.1 and one possible solution is shown in
Figure 1.2.
</figureCaption>
<subsectionHeader confidence="0.994742">
4.2 Hierarchical Clustering Techniques
</subsectionHeader>
<bodyText confidence="0.9997125">
The goal of the first step in Algorithm 4.1 is to cluster the input data hierarchically to obtain a
dendrogram structure. This section explains how to obtain a dendrogram from an input graph.
</bodyText>
<subsectionHeader confidence="0.761041">
Girvan-Newman Hierarchical Clustering
</subsectionHeader>
<bodyText confidence="0.999980333333333">
The Girvan-Newman algorithm already produces a dendrogram structure as a result of the clus-
tering process. The output of the algorithm is a tree of regions, where each region is a cluster
that contains the subclusters as subregions. This determines the hierarchy, because nodes of a
subregion are hierarchically lower than the nodes of their parent region.
For illustration purposes the sample instance (see Figure 1.1) was clustered with this method
and its result is shown in Figure 4.1. A corresponding dendrogram can be seen in Figure 4.2.
</bodyText>
<subsectionHeader confidence="0.857623">
K-means Hierarchical Clustering
</subsectionHeader>
<bodyText confidence="0.999877">
To get a hierarchy from K-means clustering, k = 2 was chosen. The initial graph is seen as
one cluster, which is then clustered by the K-means algorithm. The result are subclusters of the
initial single root-cluster. This process is repeated recursively for each of the subclusters until
</bodyText>
<page confidence="0.799607">
28
</page>
<figureCaption confidence="0.987312">
Figure 4.2: Dendrogram respresentation of the sample instance (see Figure 1.1) that was clus-
tered by the Girvan-Newman algorithm.
</figureCaption>
<figure confidence="0.996957">
29
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
0 1 2 3 4 5 6 7 8 11 12 13 14 15 16 17 9 10
11
0 1 2 3 4 5 6 7 8 12 13 14 15 16 17
9 10
3 4 5 12 13 14 15 16 17 0 1 2 6 7 8
15 3 4 5 12 13 14 16 17
8
0 1 2 6 7
17 3 4 5 12 13 14 16
1
0 2 6 7
16 3 4 5 12 13 14
2
0 6 7
13
3 4 5 12 14
6
0 7
12 3 4 5 14
14 3 4 5
0
7
3
4 5
4 5
</figure>
<bodyText confidence="0.8370055">
only single node clusters are left. The result can also be seen as a binary tree that determines the
dendrogram structure.
</bodyText>
<subsectionHeader confidence="0.670507">
Kernighan-Lin Clustering
</subsectionHeader>
<bodyText confidence="0.999912">
The Kernighan-Lin algorithm partitions a graph into two clusters. The process of recursive
application of the algorithm is analogous to K-means hierarchical clustering described in the
previous section.
</bodyText>
<subsectionHeader confidence="0.990393">
4.3 Heuristics for Finding Rings in the Dendrogram
</subsectionHeader>
<bodyText confidence="0.999892">
In this section various heuristics will be described that will use the dendrogram to extract rings
for the solution.
</bodyText>
<sectionHeader confidence="0.514904" genericHeader="method">
Heuristic1: Variate Ringsize Heuristic
</sectionHeader>
<bodyText confidence="0.999974821428571">
The intention of Heuristic1 (see Algorithm 4.2 and Algorithm 4.3) is to determine clusters
with a maximum number of buk nodes at level k and to find a ring, i.e., solve the Hamiltonian
Path Problem (HPP) within this cluster. If the HPP can be solved and the uplink constraints can
be satisfied, a valid ring was found. Finding a Hamiltonian path is sufficient, but if a complete
tour is found it is easier to choose the connections to the upper-level hubs. For larger chains the
use of Ant Colony Optimization or Genetic Algorithms would lead to better tours, but since the
chains are rather small, a simple approach is recommendable.
Since the graph is not complete, typical TSP heuristics cannot be used directly. A possible
solution would be to extend the cluster to a complete subgraph by adding edges with very high
cost (like ∞) and then to apply heuristics like the nearest-neighbour-algorithm. In this thesis the
HPP was solved directly on the non-complete graph. It works by iterating over distinct pairs of
nodes and applying DFS – starting at the first node of the pair – in each iteration. If all nodes
can be visited in the iteration and the second node from the pair can be visited as last one a valid
Hamiltonian path was found. If there is even a closing edge between the two nodes of the pair
a Hamiltonian cycle was found. Note that the order in which the DFS iterates over neighbors is
not determined. Ordering the neighbors (for example by their distance) may improve the time
needed for computation.
An essential enhancement is the variation part. The maximum ring size constraint is tight-
ened such that it varies from 2 to the bound buk. This enhances the probability of finding a cluster
that contains a valid ring.
A fundamental part of the heuristic is to find subdendrograms of subregions with nodes of
the appropriate level. The algorithm uses depth-first-search until it reaches a subdendrogram of
appropriate size, which is the varying bound. Whenever such a subdendrogram is reached, the
algorithm tries to solve the HPP. If it is successful, it searches for valid uplinks (two uplinks with
disjoint endpoints that link to the same parent ring). If such valid uplinks can be found the ring
is added to the solution.
The rings found by this heuristic depend strongly on the quality of the clustering. With good
clustering techniques this heuristic can find rings very efficiently.
</bodyText>
<figure confidence="0.939058620689655">
30
Algorithm 4.2: Heuristic1: Variate Ringsize Heuristic
input : The current level k.
output: All rings that were found in this heuristic.
1 for varsize �-- 2 to buk do
// Check TSP in cluster according to hierarchy with
maximum size varsize over unused nodes of Vk
2 ClusterDFS(cluster, k, varsize);
3 end
4 return solution rings;
Algorithm 4.3: ClusterDFS: The DFS part of the Variate Ringsize Heuristic.
input : The current (sub-)cluster cluster. The current level k. The current variating
upper bound varsize.
1 fetch unused nodes of level k in cluster;
2 if lunused nodesl &lt; varsize then
3 try to find Hamiltonian path in unused nodes;
4 if path found then
5 try to find 2 valid uplinks;
6 if uplinks found then
7 add ring to partial solution;
8 mark nodes as used;
9 end
10 end
11 else
12 foreach subcluster in cluster do
13 ClusterDFS(subcluster, k, varsize);
14 end
15 end
Heuristic2: Subtour Heuristic
</figure>
<bodyText confidence="0.9994815">
Heuristic2, see Algorithm 4.4, works on rings of the previous layer starting with layer 2.
This means for layer 2 the previous ring is the backbone ring (i.e., layer 1 ring). For level 3 the
set of rings contains all rings from the current partial solution that were found in level 2 (from
all heuristics).
For each ring the heuristic tries to find rings of lower level that are connected to this ring. If
the (parent) ring is of level k — 1, all unused nodes of level k are investigated in the search.
The algorithm works as follows: For all distinct pairs of nodes (v, u) of level k — 1 from the
parent ring r, try to find a path of unused nodes of level k between v and u. A pseudocode for
iterating over the pairs can be found in Algorithm 6.1.
To find a path simple depth first search is used. Only unused nodes from the layer Vk U u are
</bodyText>
<page confidence="0.947516">
31
</page>
<bodyText confidence="0.999857">
considered. The search starts at node v and looks for node u. A path has to fulfill the ring size
constraint. This also means that the depth of DFS is restricted by the maximum ring size buk. The
ring size constraint blk also prohibts that the DFS only takes the edge between v and u if it exists.
To improve the possibility of finding a path by this DFS the neighbors are ordered according to
their insertion cost, which is the cost of the edge of the current node to the neighbor plus the cost
from the neighbor to the target node. Another enhancement is that neighbors within the same
cluster are preferred.
</bodyText>
<subsectionHeader confidence="0.382335">
Heuristic3: Node Insertion Heuristic
</subsectionHeader>
<bodyText confidence="0.999826642857143">
After the other heuristics were executed a repair heuristic is applied for unassigned nodes.
Heuristic3, see Algorithm 4.5, tries to insert the remaining nodes into rings that were found
by the previous heuristics, without violating any constraints.
There are many ways in which order the rings should be visited. The simplest way would be
to iterate in the same order as the rings were added to the solution. To reduce the cost of adding
a node a greedy method that visits the rings by the minimum distance between node and ring
is recommendable, but it has the disadvantage, that it reduces the probability of finding rings
in lower levels slightly. Another method is to order the rings by the number of nodes already
contained and begin with the ring with least nodes. In this implementation the first method was
chosen because it provided the best prerequisites for finding valid rings and could in some cases
enhanced by the node exchange heuristic (similar to the second choice).
For each remaining node v from layer k = l(v) the heuristic checks the lower chain of each
ring Rk. If any edge e = (i, j) in the lower chain can be replaced by edges (v, i)EEk and
(v, j)EEk the node can be successfully inserted and marked as used.
</bodyText>
<figure confidence="0.951324259259259">
32
end
end
Algorithm 4.4: Heuristic2: Subtour Heuristic
input : The current level k. The parent ring r to be investigated.
output: All rings that were found in this heuristic.
// Check if a path of unused nodes from v to u, u, v E Vk−1
can be found in Vk
1 for i �-- 1;i &lt; lparent ring rl — 1; i �-- i + 1 do
for j �-- i + 1;j &lt; lparent ring rl; j �-- j + 1 do
u �-- ith node in parent ring;
v �-- jth node in parent ring;
try to find restricted path from u to v using DFS;
if path found then
add ring to partial solution;
mark nodes as used;
11 end
12 return solution rings;
2
3
4
5
6
7
8
9
10
Algorithm 4.5: Heuristic3: Node Insertion Heuristic
input : The current level k
1 foreach unused node v in Vk do
// Check if node can be inserted in any ring from level k
foreach ring r in partial solution from level k do
foreach edge e in lower chain of ring r do
if node v can be inserted between the start of e and the end of e then
insert node v between endpoints of edge e into ring r;
mark node v as used;
remove edge e;
continue in first loop with next unused node;
2
3
4
5
6
7
8
9
10
11
end
end
end
12 end
33
CHAPTER 5
</figure>
<subsectionHeader confidence="0.481878">
Test Results and Critical Reflection
</subsectionHeader>
<bodyText confidence="0.999828166666667">
In the end the algorithm presented in Chapter 4 was intensely tested. Therefore, a set of test
instances was generated. Table 5.1 shows, which properties were chosen for the instances. For
example the first block (|V  |&lt; 60) can be explained as follows: The graph of a testinstance of
this block less than 60 nodes. In layer 1 there are 3 to 5 nodes which is about 10 percent of all
nodes. In the second layer there should be 10 to 15 nodes which makes about 20 to 30 percent
of all nodes. The rest should be layer 3 nodes. Instances of this block should be solved with
a level 2 bound-parameter of 5 (i.e., bu2 = 5) and with a level 3 bound-parameter of 5 and 7,
respectively.
Each single configuration was performed 30 times on a single core of an Intel Xeon E5540
with 2,53 GHz and 3 GB RAM.
Two kinds of test instances were generated, random instances and instances based on the
TSPLIB. See section 5.1 for more information.
</bodyText>
<subsectionHeader confidence="0.999043">
5.1 Test Instance Generation
</subsectionHeader>
<bodyText confidence="0.999915333333333">
Good test instances are needed to measure the quality of the algorithms and their solutions. To
provide realistic instances the network needs to be big enough and the connectivity should have
a natural distribution.
</bodyText>
<subsectionHeader confidence="0.545592">
Random Instance Generator
</subsectionHeader>
<bodyText confidence="0.999980857142857">
The random instance generator assumes a normalized circular area, where the radius r is 1. The
polar coordinate system is used to determine positions of nodes. If there should be n rings in
level two, the graph is split into segments of 2Πn . Each of those segments is then split again
into the according number of level 3 rings. After that, one ring is generated in each of the
segments, where the coordinates of the nodes are generated randomly. The parameter values
like the number of nodes in a chain, or the number of rings that are connected to a ring are also
determined in a restricted random way.
</bodyText>
<equation confidence="0.987840647058824">
34
IV I &lt; 60 L1: 3 – 5 nodes — 10%
L2: 10 – 15 nodes 20% – 30%
bu2 L2: 5
bu3 L3: 5 / 7
60 &lt; IV I &lt; 100 L1: 5 – 8 nodes — 7%
L2: 20 – 30 nodes 20% – 30%
bu2 L2: 5/8
bu3 L3: 5/8/12
100 &lt; IV I &lt; 200 L1: 8 – 12 nodes — 5%
L2: 35 – 50 nodes 17% – 25%
bu2 L2: 8 / 12
bu3 L3: 8 / 12 / 15
200 &lt; IV I &lt; 500 L1: 12 – 17 nodes — 3%
L2: 80 – 120 nodes 18% – 24%
bu2 L2: 12 / 17
bu3 L3: 12 / 17 / 20
</equation>
<tableCaption confidence="0.843319">
Table 5.1: Test instance specification.
</tableCaption>
<bodyText confidence="0.962776090909091">
Up to this point a valid solution is generated that has to be extended to a reasonable input
instance.
A desireable property is 2-connectivity for each level. This can be achieved by computing
two edge disjoint Minimum Spanning Trees (MSTs) on a complete version of the graph. The
edges of both MSTs are added to the test instance.
Then additional random edges are added to the graph. Those are added within each Type
R cluster, between nodes of the same layer and between nodes of different layers (i.e., more
random uplinks).
At last all edges between V1 and V3 are removed.
An overview of the generated testinstances that were used for evaluation can be found in
Table 6.1a.
</bodyText>
<sectionHeader confidence="0.791167" genericHeader="method">
TSPLIB Instances
</sectionHeader>
<footnote confidence="0.81881725">
TSPLIB951 is a collection of TSP instances. Also instances of the Hamiltonian Cycle Problem
(HCP), Asymmetric Traveling Salesman Problem (ATSP), Sequential Ordering Problem (SOP)
and Capacitated Vehicle Routing Problem (CVRP) are available.
1http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/
</footnote>
<page confidence="0.947541">
35
</page>
<bodyText confidence="0.995657307692308">
The specification for the TSPLIB format is available in the online documentation. A simple
adapter was implemented to import the TSPLIB files into the test instance generation framework.
To assign levels to the instances a simple partitioning algorithm is applied: K-means. For
the first layer K-means is applied with specified k-value. The centroid nodes are assigned to
layer 1. For the second layer this process is repeated for the whole graph, but nodes that were
already assigned to layer 1 are not part of the consideration. The remaining nodes are assigned
to layer 3.
Edges must be added too, since they are usually either not part of the TSPLIB instances or
contain no valid solution for the Hierarchical Ring Network Problem. The layer 1 nodes build a
complete graph. For layer 2 and 3 two edge disjoint MSTs are created to ensure 2-connectivity.
Then edges are added to ensure a valid solution. As a last step random edges are added.
The instances have to be postprocessed, so that no edges between layer 1 and 3 exist.
The generated test instances can be found in Table 6.1b.
</bodyText>
<figure confidence="0.9090252">
36
Average Invalid Nodes per Input Instance
Girvan-Newman
Kernighan-Lin
K-means
</figure>
<figureCaption confidence="0.998737">
Figure 5.1: The average number of nodes that could not be assigned to rings, see Table 6.2.
</figureCaption>
<subsectionHeader confidence="0.998581">
5.2 Random Test Instances
</subsectionHeader>
<bodyText confidence="0.999964117647059">
The primary and very challenging goal in the Hierarchical Ring Network Problem is to find a
valid solution. With the Girvan-Newman clustering and Kernighan-Lin each single run for each
test instance could be solved by the proposed heuristics. With K-means 768 of 3000 test runs
could not be solved completely. The average number of nodes that could not be assigned to rings
is shown in Figure 5.1.
For the valid solutions the resulting costs are investigated next. An overview of the aver-
age costs (over valid solutions) are shown in Figure 5.2. It can be seen that Girvan-Newman
clustering usually gives the lowest total cost, followed by K-means. Kernighan-Lin performs
slightly worse, but with the smallest deviaton of the three algorithms. Therefore, the result of
Kernighan-Lin is more predictable. The results for each of the clustering algorithms were also
statistically tested with the student t-test at a significance level of 5% (see Table 6.4). Those
tests confirm the observations that Girvan-Newman and Kernighan-Lin lead to better results on
average.
The average computation time needed for the solutions is shown in Figure 5.3. Instances
with up to 200 nodes could be solved within some seconds. On average Kernighan-Lin per-
formed best. Girvan-Newman needed the most time, which was partially due to the fact that the
clustering algorithm itself was the most expensive regarding computation effort.
</bodyText>
<figure confidence="0.999466580645161">
invalid nodes 70
60
50
40
30
20
10
0
test_45_3_14_28
test_49_4_12_33
test_95_5_30_60
test_38_5_10_23
test_34_4_10_20
test_88_6_20_62
test_72_6_22_44
test_472_12_84_376
test_170_9_38_123
test_151_10_42_99
test_493_13_86_394
test_441_17_89_335
test_91_6_20_65
test_43_5_12_26
test_407_14_95_298
test_182_8_45_129
test_166_10_42_114
test_162_8_40_114
test_96_7_21_68
test_494_16_89_389
37
Average Costs per Input Instance
Average Time per Input Instance (logarithmic scale)
</figure>
<figureCaption confidence="0.97714">
Figure 5.2: The average resulting cost of the solutions, see Table 6.4.
</figureCaption>
<figure confidence="0.999326596774193">
test_72_6_22_44
test_43_5_12_26
test_96_7_21_68
test_88_6_20_62
test_49_4_12_33
test_45_3_14_28
test_38_5_10_23
test_34_4_10_20
test_95_5_30_60
test_162_8_40_114
test_170_9_38_123
test_151_10_42_99
test_91_6_20_65
test_182_8_45_129
test_472_12_84_376
test_494_16_89_389
test_493_13_86_394
test_407_14_95_298
test_166_10_42_114
test_441_17_89_335
costs
25000
20000
30000
15000
10000
5000
0
Girvan-Newman
Kernighan-Lin
K-means
38
test_72_6_22_44
test_43_5_12_26
test_96_7_21_68
test_88_6_20_62
test_49_4_12_33
test_45_3_14_28
test_38_5_10_23
test_34_4_10_20
test_95_5_30_60
test_162_8_40_114
test_170_9_38_123
test_151_10_42_99
test_91_6_20_65
test_182_8_45_129
test_472_12_84_376
test_493_13_86_394
test_407_14_95_298
test_166_10_42_114
test_494_16_89_389
test_441_17_89_335
log(seconds)
10000
1000
100
0.1
10
1
Girvan-Newman
Kernighan-Lin
K-means
</figure>
<figureCaption confidence="0.99663">
Figure 5.3: The average time in seconds needed to find a solution, see Table 6.5.
</figureCaption>
<figure confidence="0.637637">
Average Gain per Input Instance
</figure>
<figureCaption confidence="0.8791312">
Figure 5.4: The average relative gain achieved (cf. Table 6.3 and Table 6.4). This shows how
much the optimization step could improve the solution.
The cost reduction that could be achieved by the improvement heuristics is shown in
Figure 5.4. Instances that were solved with the Kernighan-Lin clustering could usually be im-
proved most, altough the total cost was higher than the total cost of other algorithms. The costs
before and after the optimization were tested with the student t-test at a significance level of
5% (see Table 6.3 and Table 6.4). The relative performance of the clustering techniques is the
same before and after the optimization. But it can also be seen from the statistical tests, that the
difference between Girvan-Newman and Kernighan-Lin is smaller after the optimization so that
the total costs are more similar.
</figureCaption>
<figure confidence="0.990303606060606">
test_72_6_22_44
test_38_5_10_23
test_34_4_10_20
test_95_5_30_60
test_49_4_12_33
test_88_6_20_62
test_151_10_42_99
test_43_5_12_26
test_182_8_45_129
test_170_9_38_123
test_96_7_21_68
test_91_6_20_65
test_45_3_14_28
test_162_8_40_114
test_407_14_95_298
test_494_16_89_389
test_493_13_86_394
test_472_12_84_376
test_166_10_42_114
test_441_17_89_335
relative costs gained
0.14
0.12
0.08
0.06
0.04
0.02
0.1
Girvan-Newman
Kernighan-Lin
K-means
39
Average Invalid Nodes per Input Instance
</figure>
<subsectionHeader confidence="0.990364">
5.3 TSPLIB-based Test Instances
</subsectionHeader>
<bodyText confidence="0.984426111111111">
A valid solution for the TSPLIB-based test instances could not be found in all cases. Here the
student t-tests were omitted, because for some instances too few valid solutions were available to
compare all the clustering heuristics. For each instance at least one of the clustering techniques
lead to a valid solution. In Figure 5.5 the average number of nodes that could not be assigned to
rings can be seen. For most test instances only few nodes could not be assigned to rings.
The average cost per instance can be seen in Figure 5.6. Missing bars indicate that the
corresponding clustering algorithm could not even find a valid solution in 1 of the 30 test runs
performed. Since some instances have very low total costs, an appropriate scaled view of those
instances can be seen in Figure 5.7.
</bodyText>
<figureCaption confidence="0.981357">
Figure 5.5: The average number of nodes that could not be assigned to rings, see Table 6.6.
</figureCaption>
<figure confidence="0.999603636363636">
eil51.tsp.5-15
gr96.tsp.8-30
gr96.tsp.5-20
eil76.tsp.5-20
gr96.tsp.7-25
eil51.tsp.4-10
att48.tsp.4-10
eil76.tsp.7-25
kroA100.tsp.5-20
ch150.tsp.10-40
gr431.tsp.12-80
ch150.tsp.12-45
ch150.tsp.8-35
pr439.tsp.12-80
pr299.tsp.12-80
bier127.tsp.8-35
kroB100.tsp.5-20
berlin52.tsp.5-15
berlin52.tsp.4-10
kroB100.tsp.8-30
kroA100.tsp.8-30
bier127.tsp.10-40
kroA200.tsp.8-35
kroB200.tsp.8-35
kroB100.tsp.7-25
kroA100.tsp.7-25
kroA200.tsp.12-45
kroB200.tsp.12-45
kroA200.tsp.10-40
kroB200.tsp.10-40
ulysses22.tsp.3-10
invalid nodes
25
20
15
10
5
0
Girvan-Newman
Kernighan-Lin
K-means
40
Average Cost per Input Instance
Average Cost per Input Instance
</figure>
<figureCaption confidence="0.977279">
Figure 5.6: The average resulting cost of the solutions, see Table 6.4.
</figureCaption>
<figure confidence="0.999335352941176">
eil51.tsp.5-15
gr96.tsp.8-30
gr96.tsp.5-20
berlin52.tsp.4-10
eil76.tsp.5-20
eil51.tsp.4-10
gr96.tsp.7-25
ch150.tsp.8-35
pr299.tsp.12-80
bier127.tsp.8-35
gr431.tsp.12-80
ch150.tsp.12-45
ch150.tsp.10-40
att48.tsp.4-10
pr439.tsp.12-80
eil76.tsp.7-25
kroA100.tsp.5-20
kroB100.tsp.8-30
kroB100.tsp.5-20
kroA100.tsp.8-30
berlin52.tsp.5-15
kroA100.tsp.7-25
kroB200.tsp.8-35
kroA200.tsp.8-35
kroB100.tsp.7-25
kroA200.tsp.10-40
kroB200.tsp.10-40
bier127.tsp.10-40
kroB200.tsp.12-45
kroA200.tsp.12-45
ulysses22.tsp.3-10
costs
250000
200000
350000
300000
150000
100000
50000
0
Girvan-Newman
Kernighan-Lin
K-means
41
eil51.tsp.5-15
eil76.tsp.5-20
eil51.tsp.4-10
ch150.tsp.10-40
gr96.tsp.5-20m
gr96.tsp.8-30m
gr431.tsp.12-80
gr96.tsp.7-25m
eil76.tsp.7-25
ch150.tsp.8-35
ch150.tsp.12-45
ulysses22.tsp.3-10
berlin52.tsp.4-10
berlin52.tsp.5-15
costs
25000
20000
15000
10000
5000
0
Girvan-Newman
Kernighan-Lin
K-means
</figure>
<figureCaption confidence="0.995901">
Figure 5.7: The average resulting cost of the solutions of instances with low total cost.
</figureCaption>
<figure confidence="0.86008075">
Average Time per Input Instance (logarithmic scale)
Girvan-Newman
Kernighan-Lin
K-means
</figure>
<figureCaption confidence="0.997428">
Figure 5.8: The average time in seconds needed to find a solution, see Table 6.9.
</figureCaption>
<bodyText confidence="0.9999276">
The time overview (see Figure 5.3) shows that the instances, where valid solutions were
found, could be solved fast. Most of the smaller instances could be solved in less than 1 second.
The improvement gained by the local search (see Figure 5.9) is for most instances higher
than the gain achieved for the random instances. In this case Kernighan-Lin cannot benefit as
much from the improvement as it is the case with the random instances.
</bodyText>
<figure confidence="0.999301368421053">
log(seconds) 1000
100
10
1
0.1
att48.tsp.4-10
gr96.tsp.8-30
gr96.tsp.5-20
eil51.tsp.5-15
gr96.tsp.7-25
eil76.tsp.7-25
eil76.tsp.5-20
eil51.tsp.4-10
ch150.tsp.8-35
kroA100.tsp.7-25
kroB100.tsp.8-30
kroA100.tsp.5-20
pr299.tsp.12-80
kroA200.tsp.8-35
berlin52.tsp.5-15
gr431.tsp.12-80
ch150.tsp.12-45
pr439.tsp.12-80
kroB200.tsp.8-35
kroB100.tsp.5-20
kroA100.tsp.8-30
berlin52.tsp.4-10
ch150.tsp.10-40
bier127.tsp.8-35
kroB100.tsp.7-25
kroA200.tsp.12-45
kroB200.tsp.12-45
kroA200.tsp.10-40
bier127.tsp.10-40
kroB200.tsp.10-40
ulysses22.tsp.3-10
42
Average Gain per Input Instance
</figure>
<figureCaption confidence="0.9995525">
Figure 5.9: The average relative gain achieved(cf. Table 6.7 and Table 6.8). This shows how
much the optimization step could improve the solution.
</figureCaption>
<figure confidence="0.999824152173913">
att48.tsp.4-10
gr96.tsp.8-30
gr96.tsp.5-20
eil51.tsp.5-15
gr96.tsp.7-25
eil76.tsp.7-25
eil76.tsp.5-20
eil51.tsp.4-10
kroA100.tsp.7-25
kroB100.tsp.8-30
kroA100.tsp.5-20
gr431.tsp.12-80
pr299.tsp.12-80
kroA200.tsp.8-35
berlin52.tsp.5-15
pr439.tsp.12-80
kroB200.tsp.8-35
ch150.tsp.12-45
ch150.tsp.10-40
kroB100.tsp.5-20
kroA100.tsp.8-30
berlin52.tsp.4-10
ch150.tsp.8-35
bier127.tsp.8-35
kroB100.tsp.7-25
kroA200.tsp.12-45
kroB200.tsp.12-45
kroA200.tsp.10-40
bier127.tsp.10-40
kroB200.tsp.10-40
ulysses22.tsp.3-10
relative costs gained
0.18
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.2
0.1
0
Girvan-Newman
Kernighan-Lin
K-means
</figure>
<page confidence="0.657855">
43
</page>
<sectionHeader confidence="0.358" genericHeader="method">
CHAPTER 6
</sectionHeader>
<subsectionHeader confidence="0.391446">
Summary and Future Work
</subsectionHeader>
<bodyText confidence="0.999858571428571">
Using heuristics for finding hierarchical ring structures on non-complete graphs is a difficult
task. In this thesis heuristic approaches for solving the Hierarchical Ring Network Problem
were investigated. A formal definition for this problem was given. Some related work was
explained too.
Clustering heuristics were implemented to get a hierachical structure of the input instances.
This hierarchy was an important basis for the subsequent solution heuristics.
For clustering three algorithms were implemented, the Girvan-Newman algorithm, K-means
and Kernighan-Lin clustering. K-means and Kernighan-Lin had to be adopted to get a hierarchi-
cal structure, because the original algorithms would have created only a single clustering layer.
The result of this process was represented as a dendrogram.
Next three heuristics were implemented to form rings out of the nodes, according to the
constraints. The Variate Ringsize Heuristic uses the hierarchy (from the previous clustering
step) to find find rings within clusters. The clusters are chosen to match a variating ringsize
constraint. More precisely, the variation takes place between 2 and the maximum bound buk.
The second heuristic uses depth first search (DFS) to find chains that can be attached to existing
rings. This heuristic has shown to be inefficient in some cases in terms of performance. This
gives room for improvements in future work. A third heuristic was implemented that tries to add
single remaining nodes to rings, that could not be used in the first two heuristics. That heuristic
is simple, but also necessary, because some instances could only be completely solved with the
help of this heuristic.
As a last step local improvement heuristics were applied. 2-Opt was worth applying, altough
not all instances could be improved by this post optimization. Merging rings was an important
improvement, since many rings of small size were found. Accepting small rings was crucial to
find solutions at all, but reduced the probability to find valid rings in the third layer. By merging
rings this disadvantage could be lifted.
Using hierarchical clustering for network design is a noteworthy approach. Considering it
for more applications would be worth the effort. Moreover, it would be interesting to adopt and
apply the algorithms designed for this thesis to related problems.
</bodyText>
<page confidence="0.609948">
44
</page>
<bodyText confidence="0.999965">
In the case of the Hierarchical Ring Network Problem more clustering heuristics could be
investigated. For some clustering candidates see [21]. There is still room for improvements
for finding rings with support of cluster hierarchies. One could consider centrality measures
like modularity to find good subclusters. Metaheuristics like the Variable Neighborhood Search
(VNS) or Greedy Randomized Adaptive Search Procedures (GRASP) may enhance the generic
solution algorithm. Clever use of clustering techniques can speed up heuristics for finding rings
and improve the solutions.
</bodyText>
<page confidence="0.864005">
45
</page>
<sectionHeader confidence="0.995894" genericHeader="method">
Appendix
</sectionHeader>
<subsectionHeader confidence="0.9348">
Input Data Format Description
</subsectionHeader>
<bodyText confidence="0.999590117647059">
As an input format the Graph Modelling Language (GML) is used. A detailed description of the
format can be found at the website 1.
The format is text based and has a tree structure. It is recursively defined as a list of key/value
pairs where the key is an identifier and the value can be of type integer, real, string or list(!),
where a list has to be enclosed in square brackets.
A graph starts with the key graph and a list as value. This value (i.e. the list) contains
node properties which must have an id property of type integer. A graph might also contain
edges, that can have an id property and additionally a source and a target property that
reference the id-values of previously defined nodes.
In this thesis nodes will have two additional properties. First the coordinates have to be
specified with x and y properties within a graphics and a center property. The graphics
property is often found in GML files as an information for the graphical representation. Here
it is also needed to calculate distances. The center property is a way of expressing that the
coordinate stands for the center of the node (and not for example for the upper left corner). The
second additonal property layer specifies the layer every node belongs to. It can have a value
between 0 and 2 which stands for the layer 1 to 3, resp. The edges do not require any additional
properties.
</bodyText>
<figure confidence="0.950351096774194">
Example file:
graph [
node [
id 0
graphics [
center [
x 11.59845781564943
y 25.504837521414355
]
]
layer 0
]
1http://www.fim.uni-passau.de/fileadmin/files/lehrstuhl/brandenburg/
projekte/gml/gml-technical-report.pdf
46
node [
id 1
graphics [
center [
x -13.616151400557554
y 23.787374664549787
]
]
layer 1
]
edge [
id 1
source 0
target 1
]
]
</figure>
<sectionHeader confidence="0.502354" genericHeader="method">
Additional Techniques
</sectionHeader>
<subsectionHeader confidence="0.821807">
Loop Transformation
</subsectionHeader>
<bodyText confidence="0.999824">
In imperative programming languages loops are usually written in iterative form, with the state-
ments the language provides, like while, do while or for. Due to the fact that the algo-
rithms used for this thesis use backtracking, this approach is often problematic, because one
usually has to provide methods to undo the operations done so far.
Therefore, it is proposed to use recursive variants of loops. In each step only one iteration is
done, so that the backtracking can be implemented much easier.
In the following a typical while-loop is considered to be implemented recursively:
The while statement has the form
</bodyText>
<figure confidence="0.880459894736842">
while ( Expression ) Statement
Unwinding it one time would lead to the following structure:
if ( Expression ) {
Statement
while ( Expression ) Statement
}
To get recursion, some function is needed. Therefore, the loop is (without restriction) con-
tained in a method:
function iterative() {
while ( Expression ) Statement
}
47
This can now be written as a recursive function:
function recursive() {
if ( Expression ) {
Statement
recursive()
}
}
</figure>
<bodyText confidence="0.989441">
In practice additional context has to be provided by method parameters and also additional
statements occur. Both should not impose problems on this transformation.
</bodyText>
<sectionHeader confidence="0.558431" genericHeader="method">
Distinct Pairs Iterator
</sectionHeader>
<bodyText confidence="0.913">
Combinatorial enumeration of distinct pairs over a set or sequence is a task that is often needed.
To avoid mistakes a simple skeleton code is provided in Algorithm 6.1.
</bodyText>
<listItem confidence="0.403221666666667">
Algorithm 6.1: Iterating over all distinct pairs of nodes in the list.
input : The list of input elements list.
1 for i �-- 1 to llistl - 1 do
2 for j �-- i + 1 to llistl do
3 do something with distinct index pair (i, j), where the pair of elements is
(list[i], list[j]);
</listItem>
<sectionHeader confidence="0.8134605" genericHeader="conclusions">
4 end
5 end
</sectionHeader>
<page confidence="0.74499">
48
</page>
<table confidence="0.975859388888889">
Test Result Tables
An overview of the tested instances is given in Table 6.1.
instance IV I IEI instance IV I IEI
test_34_4_10_20 34 143 ulysses22.tsp.3-10 22 67
test_38_5_10_23 38 169 att48.tsp.4-10 48 167
test_43_5_12_26 43 183 eil51.tsp.4-10 51 172
test_45_3_14_28 45 193 eil51.tsp.5-15 51 185
test_49_4_12_33 49 210 berlin52.tsp.4-10 52 197
test_72_6_22_44 72 341 berlin52.tsp.5-15 52 201
test_88_6_20_62 88 423 eil76.tsp.5-20 76 281
test_91_6_20_65 91 426 eil76.tsp.7-25 76 299
test_95_5_30_60 95 464 gr96.tsp.5-20 96 383
test_96_7_21_68 96 450 gr96.tsp.7-25 96 395
test_151_10_42_99 151 768 gr96.tsp.8-30 96 384
test_162_8_40_114 162 816 kroA100.tsp.5-20 100 429
test_166_10_42_114 166 834 kroA100.tsp.7-25 100 421
test_170_9_38_123 170 854 kroA100.tsp.8-30 100 420
test_182_8_45_129 182 914 kroB100.tsp.5-20 100 429
test_407_14_95_298 407 2145 kroB100.tsp.7-25 100 432
test_441_17_89_335 441 2301 kroB100.tsp.8-30 100 426
test_472_12_84_376 472 2426 bier127.tsp.8-35 127 559
test_493_13_86_394 493 2512 bier127.tsp.10-40 127 559
test_494_16_89_389 494 2570 ch150.tsp.8-35 150 672
(a) Generated random test instances. ch150.tsp.10-40 150 684
ch150.tsp.12-45 150 681
kroA200.tsp.8-35 200 940
kroB200.tsp.10-40 200 964
kroA200.tsp.12-45 200 1005
kroB200.tsp.8-35 200 952
kroA200.tsp.10-40 200 964
kroB200.tsp.12-45 200 1005
pr299.tsp.12-80 299 1566
gr431.tsp.12-80 431 2152
pr439.tsp.12-80 439 2194
(b) Generated test instances based on the
TSPLIB.
</table>
<tableCaption confidence="0.995702">
Table 6.1: Test instance overview.
</tableCaption>
<page confidence="0.978215">
49
</page>
<note confidence="0.73866">
Results from the Random Instances
</note>
<tableCaption confidence="0.9591445">
Table 6.2: The average number of invalid nodes per instance and their standard deviation are
shown in this table. For Girvan-Newman and Kernighan-Lin all instances could be solved.
</tableCaption>
<table confidence="0.996070434782609">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
test_34_4_10_20 34 143 0.00 0.00 0.00 0.00 1.50 0.50
test_38_5_10_23 38 169 0.00 0.00 0.00 0.00 2.00 1.41
test_43_5_12_26 43 183 0.00 0.00 0.00 0.00 1.80 0.40
test_45_3_14_28 45 193 0.00 0.00 0.00 0.00 2.50 1.12
test_49_4_12_33 49 210 0.00 0.00 0.00 0.00 0.00 0.00
test_72_6_22_44 72 341 0.00 0.00 0.00 0.00 5.63 3.99
test_88_6_20_62 88 423 0.00 0.00 0.00 0.00 9.00 2.77
test_91_6_20_65 91 426 0.00 0.00 0.00 0.00 9.36 3.78
test_95_5_30_60 95 464 0.00 0.00 0.00 0.00 4.50 2.87
test_96_7_21_68 96 450 0.00 0.00 0.00 0.00 11.83 4.83
test_151_10_42_99 151 768 0.00 0.00 0.00 0.00 15.58 6.06
test_162_8_40_114 162 816 0.00 0.00 0.00 0.00 15.74 6.98
test_166_10_42_114 166 834 0.00 0.00 0.00 0.00 11.92 5.06
test_170_9_38_123 170 854 0.00 0.00 0.00 0.00 15.88 6.34
test_182_8_45_129 182 914 0.00 0.00 0.00 0.00 18.29 5.76
test_407_14_95_298 407 2145 0.00 0.00 0.00 0.00 49.34 9.73
test_441_17_89_335 441 2301 0.00 0.00 0.00 0.00 53.73 12.95
test_472_12_84_376 472 2426 0.00 0.00 0.00 0.00 64.68 16.85
test_493_13_86_394 493 2512 0.00 0.00 0.00 0.00 68.38 14.84
test_494_16_89_389 494 2570 0.00 0.00 0.00 0.00 62.08 13.50
50
</table>
<tableCaption confidence="0.965145">
Table 6.3: This table contains the average cost values (and their standard deviation)
</tableCaption>
<reference confidence="0.82440925">
before the optimization. The clustering heuristics were compaired pairwise with the statisti-
cal student t-test on a significance level of 5%. pA stands for the comparison between Girvan-
Newman and Kernighan-Lin, pB between Kernighan-Lin and K-means, and pC between Girvan-
Newman and K-means.
</reference>
<table confidence="0.970662045454546">
instance IV I IEI Girvan-Newman PA Kernighan-Lin PB K-means PC
mean dev mean dev mean dev
test_34_4_10_20 34 143 2592.32 206.15 &lt; 2800.34 68.63 &gt; 2399.71 201.33 &gt;
test_38_5_10_23 38 169 2419.22 136.37 &lt; 3079.41 71.93 &gt; 2728.22 224.59 &lt;
test_43_5_12_26 43 183 2613.77 128.24 &lt; 2718.09 33.73 &gt; 2623.52 155.11 pe
test_45_3_14_28 45 193 2591.00 157.50 &lt; 3264.10 82.50 &gt; 2715.53 144.99 &lt;
test_49_4_12_33 49 210 3517.08 395.33 &gt; 3191.40 95.74 &lt; 3467.01 336.20 pe
test_72_6_22_44 72 341 4817.46 225.08 pe 4804.19 489.00 &gt; 4338.89 367.64 &gt;
test_88_6_20_62 88 423 5656.04 371.71 pe 5623.56 407.55 pe 5601.14 442.94 pe
test_91_6_20_65 91 426 5961.23 380.60 &lt; 6755.90 375.13 &gt; 5665.59 542.62 &gt;
test_95_5_30_60 95 464 5251.04 373.36 &lt; 5472.78 370.04 pe 5522.57 523.15 &lt;
test_96_7_21_68 96 450 5656.75 352.16 &lt; 6313.49 491.00 &gt; 5474.53 503.09 &gt;
test_151_10_42_99 151 768 8843.29 506.19 &lt; 9309.03 348.64 &gt; 8769.04 448.31 pe
test_162_8_40_114 162 816 9170.89 423.98 &lt; 10106.08 378.97 &gt; 9188.13 397.35 pe
test_166_10_42_114 166 834 9497.89 396.48 pe 9514.29 354.33 &gt; 8940.10 522.41 &gt;
test_170_9_38_123 170 854 9447.74 457.26 &lt; 9872.69 595.08 &gt; 9164.44 648.80 &gt;
test_182_8_45_129 182 914 9515.14 155.78 &lt; 10325.14 274.09 &gt; 9947.06 612.15 &lt;
test_407_14_95_298 407 2145 20568.98 566.98 &lt; 21988.48 335.30 &gt; 21092.78 975.90 &lt;
test_441_17_89_335 441 2301 25263.67 920.29 &lt; 25617.17 634.99 &gt; 24757.12 968.42 &gt;
test_472_12_84_376 472 2426 23841.81 438.76 &lt; 24860.34 578.74 &gt; 24207.02 966.78 &lt;
test_493_13_86_394 493 2512 25208.76 1049.73 &lt; 26687.84 810.11 &gt; 26409.71 1556.31 &lt;
test_494_16_89_389 494 2570 26929.54 882.96 &lt; 29110.80 1119.83 &gt; 27680.42 1455.72 &lt;
</table>
<page confidence="0.690796">
51
</page>
<tableCaption confidence="0.9409398">
Table 6.4: This table contains the average cost values (and their standard deviation) after
the optimization. The clustering heuristics were compaired pairwise with the statistical student
t-test on a significance level of 5%. pA stands for the comparison between Girvan-Newman and
Kernighan-Lin, pB between Kernighan-Lin and K-means, and pC between Girvan-Newman and
K-means.
</tableCaption>
<table confidence="0.998287409090909">
instance IV I IEI Girvan-Newman PA Kernighan-Lin PB K-means PC
mean dev mean dev mean dev
test_34_4_10_20 34 143 2466.61 200.47 pe 2450.25 144.92 &gt; 2272.31 199.92 &gt;
test_38_5_10_23 38 169 2190.95 181.62 &lt; 2907.08 81.20 &gt; 2467.81 195.51 &lt;
test_43_5_12_26 43 183 2456.40 91.79 pe 2454.64 27.06 &gt; 2381.13 150.88 &gt;
test_45_3_14_28 45 193 2425.55 174.76 &lt; 2858.26 52.56 &gt; 2570.18 152.93 &lt;
test_49_4_12_33 49 210 3324.98 392.35 &gt; 3001.75 31.20 &lt; 3263.44 330.35 pe
test_72_6_22_44 72 341 4562.97 217.29 &gt; 4422.07 570.24 &gt; 4066.32 365.14 &gt;
test_88_6_20_62 88 423 5250.94 388.25 pe 5268.41 384.92 pe 5236.42 408.65 pe
test_91_6_20_65 91 426 5530.19 339.18 &lt; 6115.62 301.98 &gt; 5208.86 504.47 &gt;
test_95_5_30_60 95 464 4850.54 372.32 &lt; 5216.63 369.79 &gt; 5039.75 503.30 &lt;
test_96_7_21_68 96 450 5414.95 315.59 &lt; 5881.73 523.29 &gt; 5154.12 464.56 &gt;
test_151_10_42_99 151 768 8335.13 457.80 &lt; 8700.35 336.04 &gt; 8189.90 393.37 &gt;
test_162_8_40_114 162 816 8618.47 370.67 &lt; 9124.67 379.17 &gt; 8568.53 374.32 pe
test_166_10_42_114 166 834 9001.16 373.79 &gt; 8828.83 330.99 &gt; 8439.90 524.49 &gt;
test_170_9_38_123 170 854 8757.22 418.50 &lt; 9358.19 551.41 &gt; 8702.07 578.01 pe
test_182_8_45_129 182 914 9094.63 166.13 &lt; 9853.01 332.87 &gt; 9567.29 609.29 &lt;
test_407_14_95_298 407 2145 19736.36 530.69 &lt; 21177.50 318.80 &gt; 20275.79 889.13 &lt;
test_441_17_89_335 441 2301 24313.60 895.51 pe 24425.53 647.75 &gt; 23942.06 985.10 &gt;
test_472_12_84_376 472 2426 22960.97 528.67 &lt; 24276.89 447.93 &gt; 23470.15 932.35 &lt;
test_493_13_86_394 493 2512 24266.31 1065.72 &lt; 25497.62 737.20 &gt; 25464.88 1442.48 &lt;
test_494_16_89_389 494 2570 25886.62 880.14 &lt; 28106.46 1099.50 pe 26775.41 1419.14 &lt;
</table>
<page confidence="0.607995">
52
</page>
<tableCaption confidence="0.960227">
Table 6.5: The average time needed for finding a valid solution is listed here. Values are
scaled in milliseconds.
</tableCaption>
<table confidence="0.998841454545455">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
test_34_4_10_20 34 143 779.42 215.24 426.35 130.70 360.33 112.49
test_38_5_10_23 38 169 897.00 202.17 523.07 154.99 435.93 106.29
test_43_5_12_26 43 183 999.70 215.95 572.67 164.19 445.07 130.41
test_45_3_14_28 45 193 1025.97 264.12 634.55 180.81 495.76 155.90
test_49_4_12_33 49 210 1197.80 247.52 543.40 193.32 567.65 212.85
test_72_6_22_44 72 341 2151.73 412.15 1020.72 358.66 962.91 375.22
test_88_6_20_62 88 423 2825.67 367.36 956.17 347.06 812.82 280.13
test_91_6_20_65 91 426 3522.37 564.02 1778.87 824.56 1214.82 426.99
test_95_5_30_60 95 464 3832.99 479.19 1585.72 556.56 1372.24 567.16
test_96_7_21_68 96 450 3689.13 659.73 1893.84 908.76 1312.68 495.13
test_151_10_42_99 151 768 12753.56 953.71 1861.11 627.29 1872.24 1173.21
test_162_8_40_114 162 816 16840.67 1110.27 2371.99 872.27 2922.67 2813.73
test_166_10_42_114 166 834 17064.56 1038.38 2165.17 595.70 2202.84 1453.69
test_170_9_38_123 170 854 19859.08 1443.88 2322.46 620.24 2749.57 2621.55
test_182_8_45_129 182 914 24914.21 1464.62 2421.59 838.50 2260.06 1243.20
test_407_14_95_298 407 2145 433817.37 32292.82 24021.82 19330.93 412309.07 2212615.53
test_441_17_89_335 441 2301 663820.41 38670.66 41232.09 21988.88 139776.22 216975.98
test_472_12_84_376 472 2426 802348.48 39844.76 28302.61 21940.88 331089.58 1638352.81
test_493_13_86_394 493 2512 922606.57 47059.81 135591.82 194549.87 880866.70 2279635.13
test_494_16_89_389 494 2570 1035306.86 54769.42 196222.99 303653.02 1041982.37 2782957.35
</table>
<page confidence="0.848347">
53
</page>
<note confidence="0.453463">
Results from the TSPLIB-based Instances
</note>
<tableCaption confidence="0.9053495">
Table 6.6: The average number of invalid nodes per instance and their standard deviation are
shown in this table.
</tableCaption>
<table confidence="0.998484787878788">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
ulysses22.tsp.3-10 22 67 8.00 1.00 4.00 1.00 2.70 1.78
att48.tsp.4-10 48 167 9.50 2.50 10.50 3.50 4.39 2.86
eil51.tsp.4-10 51 172 4.00 0.00 7.00 1.00 3.64 2.28
eil51.tsp.5-15 51 185 0.00 0.00 2.50 0.50 2.55 1.68
berlin52.tsp.4-10 52 197 2.45 1.40 7.50 1.50 5.56 4.20
berlin52.tsp.5-15 52 201 6.00 0.00 8.50 3.50 4.04 2.14
eil76.tsp.5-20 76 281 2.99 1.14 4.83 1.57 4.03 2.68
eil76.tsp.7-25 76 299 3.00 1.15 4.00 2.38 3.12 2.24
gr96.tsp.5-20 96 383 6.34 4.13 5.00 2.68 7.50 4.93
gr96.tsp.7-25 96 395 4.40 3.32 5.17 2.19 5.82 3.78
gr96.tsp.8-30 96 384 6.51 4.71 1.80 0.75 4.58 3.44
kroA100.tsp.5-20 100 429 5.30 3.00 15.20 7.60 7.96 4.85
kroA100.tsp.7-25 100 421 5.88 3.55 7.33 2.56 4.59 3.32
kroA100.tsp.8-30 100 420 3.29 1.86 7.33 2.13 5.39 3.96
kroB100.tsp.5-20 100 429 18.22 4.09 11.33 4.42 9.03 7.38
kroB100.tsp.7-25 100 432 4.83 2.74 5.67 3.64 4.49 2.93
kroB100.tsp.8-30 100 426 2.79 1.18 4.20 2.64 4.74 3.78
bier127.tsp.8-35 127 559 11.97 7.95 8.50 2.81 7.32 4.27
bier127.tsp.10-40 127 559 4.00 4.05 3.00 1.90 3.51 2.96
ch150.tsp.8-35 150 672 2.25 0.43 3.60 2.58 3.03 2.24
ch150.tsp.10-40 150 684 2.07 1.51 4.40 2.65 4.39 2.69
ch150.tsp.12-45 150 681 1.60 0.80 2.80 1.83 3.51 2.84
kroA200.tsp.8-35 200 940 5.31 2.24 10.20 3.87 5.17 3.75
kroA200.tsp.10-40 200 964 6.66 3.78 4.17 2.27 3.09 1.86
kroA200.tsp.12-45 200 1005 4.50 3.40 6.17 4.88 6.15 4.62
kroB200.tsp.8-35 200 952 4.75 1.48 4.17 3.98 7.53 6.74
kroB200.tsp.10-40 200 964 3.69 1.80 5.83 3.89 5.35 4.47
kroB200.tsp.12-45 200 1005 3.67 2.05 4.20 3.12 6.38 5.92
pr299.tsp.12-80 299 1566 1.80 1.60 2.50 1.12 3.11 2.61
gr431.tsp.12-80 431 2152 2.61 0.95 4.40 2.94 12.86 10.26
pr439.tsp.12-80 439 2194 22.64 20.39 2.50 1.61 12.81 8.70
</table>
<page confidence="0.676058">
54
</page>
<tableCaption confidence="0.76301475">
Table 6.7: This table contains the average cost values (and their standard deviation) before
the optimization. For these instances a statistical test was not performed, because too few valid
solutions were available for a statistically significant statement. Missing values indicate, that no
valid solution was found with the corresponding clustering technique.
</tableCaption>
<table confidence="0.960062939393939">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
ulysses22.tsp.3-10 22 67 139.29 1.52
att48.tsp.4-10 48 167 79829.42 2847.31
eil51.tsp.4-10 51 172 955.63 0.00 988.25 34.74
eil51.tsp.5-15 51 185 1029.26 8.88 1005.25 39.92
berlin52.tsp.4-10 52 197 17391.68 396.38
berlin52.tsp.5-15 52 201 17659.09 0.00 16845.10 301.33
eil76.tsp.5-20 76 281 1191.89 30.38 1201.44 30.96
eil76.tsp.7-25 76 299 1283.12 39.48
gr96.tsp.5-20 96 383 1275.39 0.00 1337.27 59.16
gr96.tsp.7-25 96 395 1470.70 24.84 1397.71 43.69
gr96.tsp.8-30 96 384 1472.15 0.00 1386.63 70.22
kroA100.tsp.5-20 100 429 58415.57 0.00 61696.27 2965.25
kroA100.tsp.7-25 100 421 63880.86 4204.33
kroA100.tsp.8-30 100 420 61055.40 2760.80 65012.50 4006.51
kroB100.tsp.5-20 100 429 60702.60 3332.30
kroB100.tsp.7-25 100 432 64867.73 3944.71
kroB100.tsp.8-30 100 426 66489.93 3399.81 70425.01 0.00 67952.05 3098.30
bier127.tsp.8-35 127 559 300284.54 8709.27 310733.37 0.00 295720.75 14806.29
bier127.tsp.10-40 127 559 311425.84 8964.34
ch150.tsp.8-35 150 672 17633.88 165.55 18892.15 0.00 18348.63 450.84
ch150.tsp.10-40 150 684 18395.72 244.73 19649.49 0.00 18777.73 511.83
ch150.tsp.12-45 150 681 18808.69 0.00 21815.09 0.00 19210.06 714.57
kroA200.tsp.8-35 200 940 92110.19 4388.56
kroA200.tsp.10-40 200 964 98762.71 1947.35 96122.10 0.00 93422.87 3042.44
kroA200.tsp.12-45 200 1005 100334.28 1120.95 96620.25 3626.95
kroB200.tsp.8-35 200 952 97032.00 585.51 83834.45 0.00 90279.74 3088.04
kroB200.tsp.10-40 200 964 94205.60 4828.49 93253.56 2734.53
kroB200.tsp.12-45 200 1005 96745.32 2475.37 97947.67 4372.56
pr299.tsp.12-80 299 1566 168499.60 2141.54 169112.22 7826.76
gr431.tsp.12-80 431 2152 5845.89 0.00 5837.78 312.91
pr439.tsp.12-80 439 2194 349691.59 6821.11
</table>
<page confidence="0.663482">
55
</page>
<tableCaption confidence="0.75819125">
Table 6.8: This table contains the average cost values (and their standard deviation) after
the optimization. For these instances a statistical test was not performed, because too few valid
solutions were available for a statistically significant statement. Missing values indicate, that no
valid solution was found with the corresponding clustering technique.
</tableCaption>
<table confidence="0.945223333333333">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
ulysses22.tsp.3-10 22 67 134.01 2.26
att48.tsp.4-10 48 167 73334.12 2873.18
eil51.tsp.4-10 51 172 918.38 0.00 921.05 33.10
eil51.tsp.5-15 51 185 926.76 7.05 921.20 33.51
berlin52.tsp.4-10 52 197 16367.85 657.32
berlin52.tsp.5-15 52 201 16806.68 0.00 15779.50 253.80
eil76.tsp.5-20 76 281 1068.30 19.20 1111.04 37.27
eil76.tsp.7-25 76 299 1167.91 39.99
gr96.tsp.5-20 96 383 1165.98 0.00 1231.78 54.51
gr96.tsp.7-25 96 395 1194.06 18.54 1191.70 30.60
gr96.tsp.8-30 96 384 1255.34 0.00 1216.58 42.73
kroA100.tsp.5-20 100 429 54562.78 0.00 55475.48 1797.28
kroA100.tsp.7-25 100 421 54673.38 2172.82
kroA100.tsp.8-30 100 420 50549.72 950.83 54572.78 1622.92
kroB100.tsp.5-20 100 429 56097.94 1790.72
kroB100.tsp.7-25 100 432 56475.88 1972.91
kroB100.tsp.8-30 100 426 54807.79 1818.98 61485.37 0.00 57300.69 1989.45
bier127.tsp.8-35 127 559 266143.87 2424.85 281308.72 0.00 267210.45 8912.25
bier127.tsp.10-40 127 559 280890.62 8280.53
ch150.tsp.8-35 150 672 15748.55 134.39 17117.63 0.00 16362.82 438.19
ch150.tsp.10-40 150 684 16359.75 303.18 17653.24 0.00 16674.03 415.89
ch150.tsp.12-45 150 681 16225.87 0.00 19394.48 0.00 16968.55 568.45
kroA200.tsp.8-35 200 940 80681.50 2341.70
kroA200.tsp.10-40 200 964 84877.65 1043.81 83283.31 0.00 81348.50 2206.81
kroA200.tsp.12-45 200 1005 84315.91 521.23 82788.86 2456.76
kroB200.tsp.8-35 200 952 83315.39 585.51 75002.83 0.00 79603.75 2120.49
kroB200.tsp.10-40 200 964 82344.44 2003.62 79925.20 2303.95
kroB200.tsp.12-45 200 1005 83292.68 2498.21 84901.59 3736.16
pr299.tsp.12-80 299 1566 140888.81 1981.84 143990.83 5334.69
gr431.tsp.12-80 431 2152 5164.10 0.00 5164.62 306.84
pr439.tsp.12-80 439 2194 305452.69 8985.96
</table>
<page confidence="0.511785">
56
</page>
<tableCaption confidence="0.922727">
Table 6.9: The average time needed for finding a valid solution is listed here. Values are
scaled in milliseconds. Empty cells indicate, that no valid solution was found with the corre-
sponding clustering technique.
</tableCaption>
<table confidence="0.866182484848485">
instance |V  ||E |Girvan-Newman Kernighan-Lin K-means
mean dev mean dev mean dev
ulysses22.tsp.3-10 22 67 393.82 63.11
att48.tsp.4-10 48 167 498.11 128.38
eil51.tsp.4-10 51 172 1124.17 188.53 406.00 127.76
eil51.tsp.5-15 51 185 1098.72 245.12 571.11 156.20
berlin52.tsp.4-10 52 197 753.33 234.89
berlin52.tsp.5-15 52 201 1213.20 148.36 667.33 157.56
eil76.tsp.5-20 76 281 1434.00 372.23 969.07 419.63
eil76.tsp.7-25 76 299 982.20 415.77
gr96.tsp.5-20 96 383 1764.80 517.24 2526.33 896.76
gr96.tsp.7-25 96 395 3042.17 1072.26 1483.35 626.91
gr96.tsp.8-30 96 384 1876.23 711.13 1256.29 482.36
kroA100.tsp.5-20 100 429 2951.10 1163.51 2099.75 860.32
kroA100.tsp.7-25 100 421 1080.97 413.59
kroA100.tsp.8-30 100 420 2340.43 663.15 1337.30 508.55
kroB100.tsp.5-20 100 429 1629.16 600.75
kroB100.tsp.7-25 100 432 1597.07 684.15
kroB100.tsp.8-30 100 426 2542.73 705.47 1561.40 646.42 1463.95 537.66
bier127.tsp.8-35 127 559 4528.10 640.24 3803.83 1077.28 2123.56 499.66
bier127.tsp.10-40 127 559 2873.33 763.96
ch150.tsp.8-35 150 672 5848.57 442.34 4233.70 1319.93 2808.27 631.01
ch150.tsp.10-40 150 684 6853.78 661.18 8196.17 1570.85 3195.87 715.06
ch150.tsp.12-45 150 681 58334.81 669.10 49185.70 1244.60 48985.03 1258.87
kroA200.tsp.8-35 200 940 5909.42 2410.54
kroA200.tsp.10-40 200 964 12013.37 686.94 11380.40 2191.00 6595.05 1906.44
kroA200.tsp.12-45 200 1005 71852.48 2082.34 54663.54 1913.65
kroB200.tsp.8-35 200 952 15057.07 688.18 11049.40 2038.93 6330.35 1150.47
kroB200.tsp.10-40 200 964 11906.62 1320.17 5920.29 1541.49
kroB200.tsp.12-45 200 1005 69448.70 1953.91 53501.21 2016.59
pr299.tsp.12-80 299 1566 305541.69 110524.01 217407.84 172403.10
gr431.tsp.12-80 431 2152 433551.97 8976.75 395681.80 291287.06
pr439.tsp.12-80 439 2194 864114.33 1015110.56
</table>
<page confidence="0.827779">
57
</page>
<sectionHeader confidence="0.703984" genericHeader="references">
Bibliography
</sectionHeader>
<reference confidence="0.998490333333333">
[1] A. Balakrishnan, T. L. Magnanti, and P. Mirchandani. The multi-level network design
problem. Working papers 3366-91., Massachusetts Institute of Technology (MIT), Sloan
School of Management, 1991.
[2] J. Bentley and B. Floyd. Programming pearls: a sample of brilliance. Communications
ACM, 30:754–757, 1987.
[3] D. Bertsimas and R. Weismantel. Optimization over Integers. Dynamic Ideas, 1st edition,
2005.
[4] U. Brandes. A faster algorithm for betweenness centrality. Journal of Mathematical Soci-
ology, 25:163–177, 2001.
[5] J. R. Current, C. S. ReVelle, and J. L. Cohon. The hierarchical network design problem.
European Journal of Operational Research, 27(1):57–66, 1986.
[6] D. Easley and J. Kleinberg. Networks, Crowds, and Markets: Reasoning About a Highly
Connected World. Cambridge University Press, 2010.
[7] C. T. Fan, M. E. Muller, and I. Rezucha. Development of sampling plans by using sequen-
tial (item by item) selection techniques and digital computers. Journal of the American
Statistical Association, 57:387–402, 1962.
[8] B. Fortz. Design of Survivable Networks with Bounded Rings. PhD thesis, Universite libre
de Bruxelles, 2000.
[9] M. Girvan and M. E. J. Newman. Community structure in social and biological networks.
Proceedings of the National Academy of Science, 99:7821–7826, 2002.
[10] T. G. Jones. A note on sampling a tape-file. Communications ACM, 5:343, 1962.
[11] B. W. Kernighan and S. Lin. An Efficient Heuristic Procedure for Partitioning Graphs. The
Bell system technical journal, 49(1):291–307, 1970.
[12] J. G. Klincewicz. Hub location in backbone/tributary network design: a review. Location
Science, 6(1-4):307 – 335, 1998.
58
[13] G. Laporte and I. R. Martín. Locating a cycle in a transportation or a telecommunications
network. Networks, 50:92–108, 2007.
[14] E. L. Lawler, J. K. Lenstra, A. H. G. R. Kan, and D. B. Shmoys. The Traveling Salesman
Problem: A Guided Tour of Combinatorial Optimization. Wiley, New York, 1985.
[15] C. Y. Lee and S. J. Koh. A design of the minimum cost ring-chain network with dual-
homing survivability: A tabu search approach. Computers &amp; Operations Research,
24(9):883 – 897, 1997.
[16] S. Lin and B. W. Kernighan. An effective heuristic algorithm for the travelling-salesman
problem. Operations Research, 21:498–516, 1973.
[17] R. Salman, V. Kecman, Q. Li, R. Strack, and E. Test. Fast k-means algorithm clustering.
CoRR, abs/1108.1351, 2011.
[18] T. Thomadsen. Hierarchical Network Design. PhD thesis, Technical University of Den-
mark, 2005.
[19] J. S. Vitter. An efficient algorithm for sequential random sampling. In ACM Transactions
Mathematical Software, volume 13, pages 58–67, New York, NY, USA, 1987. ACM.
[20] C. Walshaw. Multilevel refinement for combinatorial optimisation problems. Annals of
Operations Research, 131:325–372, 2004.
[21] J. Xie, S. Kelley, and B. K. Szymanski. Overlapping community detection in networks:
the state of the art and comparative study. CoRR, abs/1110.5813, 2011.
</reference>
<page confidence="0.745234">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.895650333333333">Clustering Heuristics for the Hierarchical Ring Network Problem DIPLOMARBEIT zur Erlangung des akademischen Grades Diplom-Ingenieur im Rahmen des Studiums Computational Intelligence eingereicht von</title>
<author confidence="0.901813">Rainer Schuster</author>
<address confidence="0.628075">Matrikelnummer 0425205</address>
<author confidence="0.694933">an der</author>
<affiliation confidence="0.429935">Fakultät für Informatik der Technischen Universität Wien</affiliation>
<address confidence="0.229302">Betreuung: ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl</address>
<author confidence="0.725857">Dipl-Ing Christian Schauer</author>
<address confidence="0.970474">Wien, 30.11.2011</address>
<affiliation confidence="0.6662465">(Unterschrift Verfasser) (Unterschrift Betreuung) Technische Universität Wien</affiliation>
<address confidence="0.371363">Wien 13 +43-1-58801-0</address>
<title confidence="0.679236">Clustering Heuristics for the Hierarchical Ring Network Problem MASTER’S THESIS submitted in partial fulfillment of the requirements for the degree of Diplom-Ingenieur in Computational Intelligence</title>
<author confidence="0.9367955">by Rainer Schuster</author>
<degree confidence="0.576441">Registration Number 0425205 to the Faculty of Informatics at the Vienna University of Technology Advisor: ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl Assistance: Univ.-Ass. Dipl.-Ing. Christian Schauer</degree>
<address confidence="0.992265">Vienna, 30.11.2011</address>
<affiliation confidence="0.9654815">(Signature of Author) (Signature of Advisor) Technische Universität Wien</affiliation>
<address confidence="0.611385">Wien 13 +43-1-58801-0</address>
<title confidence="0.523788">Erklärung zur Verfassung der Arbeit</title>
<author confidence="0.87726">Rainer Schuster</author>
<abstract confidence="0.982928294117647">Zubergasse 147, 2020 Sonnberg Hiermit erkläre ich, dass ich diese Arbeit selbständig verfasst habe, dass ich die verwendeten Quellen und Hilfsmittel vollständig angegeben habe und dass ich die Stellen der Arbeit – einschließlich Tabellen, Karten und Abbildungen –, die anderen Werken oder dem Internet im Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als Entlehnung kenntlich gemacht habe. (Ort, Datum) (Unterschrift Verfasser) i Acknowledgements I want to thank my advisors ao.Univ.-Prof. Dipl.-Ing. Dr. techn. Günther Raidl and Univ.-Ass. Dipl.-Ing. Christian Schauer. Their constructive feedback and their experience was a big help for writing and improving this thesis. Special thanks goes to my family and my friends. Studying can be quite time-consuming and stressful sometimes and their support is invaluable. ii Abstract In this thesis the application of clustering algorithms for solving the Hierarchical Ring Network Problem (HRNP) is investigated. When the network is represented as a graph, an informal problem definition for this NPcomplete problem is: Given a set of network sites (nodes) assigned to one of three layers and the costs for establishing connections between sites (i.e., edge costs) the objective is to find a minimum cost connected network under certain constraints that are explained in detail in the thesis. The most important constraint is that the nodes have to be assigned to rings of bounded size that connect the layers hierarchically. The ring structure is a good compromise between the robustness of a network and the cost for establishing it. It is guaranteed, that the network can continue to provide its service if one network node per ring fails. The basic idea in this thesis for solving this network design problem was to cluster the sites with hierarchical clustering heuristics and to use the resulting hierarchy as support for the ringfinding heuristics. Previous apporaches for related network design problems did not use the inherent network structure in such a way. Usual approaches are based on greedy heuristics. Three clustering heuristics were implemented: Girvan-Newman, K-means and Kernighan- Lin. Especially the first algorithm is interesting, because it was successfully applied analyzing large network structures, also in the context of internet communities. For finding rings three heuristics were implemented too. Strategic variation of the maximum allowed ring size helps the first heuristic to find rings using the cluster hierarchy. The second heuristic finds rings by searching for paths that are connected to previously found rings. Third a repair heuristic was implemented that tries to add remaining nodes to existing rings. Local search heuristics are applied last to improve the solution quality. To check how the clustering approach performs for solving the problem of this thesis two test instance generators were implemented. One generates instances randomly and the second generates instances based on the popular TSPLIB archive. The evaluation of the random test instances has shown, that all three clustering heuristics were able to solve those test instances, while Girvan-Newman and Kernighan-Lin found valid solutions in each test run this was not possible for K-means. When Kernighan-Lin was used as clustering algorithm solutions could be found faster on average, but the resulting costs where slightly higher. For the TSPLIB based instances the clustering algorithms had more problems to find valid solutions, but for each test instance at least one type of clustering was successful. iii Kurzfassung In dieser Diplomarbeit wird die Anwendung von Clusteringalgorithmen untersucht, um das Hierarchical Ring Network Problem (HRNP) zu lösen. Wenn das Netzwerk als Graph repräsentiert ist, ist dieses NP-vollständige Problem wie folgt definiert: Gegeben ist Menge von Knoten welche jeweils einer von drei Schichten zugewiesen sind, und eine Kostenfunktion, welche die Verbindungskosten zwischen zwei Knoten (d.h. Kantenkosten) zuweist. Gesucht ist ein zusammenhängendes Netzwerk mit minimalen Gesamtkosten, wobei dieses bestimmte Struktureigenschaften zu erfüllen hat, welche im Detail in der Diplomarbeit beschrieben werden. Die wichtigste dieser Eigenschaften ist, dass Knoten gemäß einer hierarchischen Struktur zu größenbeschränkten Ringen verbunden werden. Ringstrukturen sind ein guter Kompromiss zwischen der Verfügbarkeit von Netzwerken und deren Herstellungskosten. Die Verfügbarkeit ist gewährleistet, solange maximal ein Knoten pro Ring ausfällt. Die grundlegende Idee dieser Diplomarbeit um dieses Netzwerkdesign-Problem zu lösen, ist die Knoten mit Hilfe von hierarchischen Clusteringalgorithmen anzuordnen und die resultierende Hierarchie für nachfolgende Heuristiken zu verwenden, welche die Ringe finden. Vorhergehende Ansätze für vergleichbare Netzwerkdesign-Probleme haben die inhärente Netzwerkstruktur nicht auf solche Weise genützt und eher Greedy-Heuristiken eingesetzt. Um gültige Ringe zu finden, wurden drei Heuristiken implementiert. Strategisches Variieren der erlaubten Ringgröße hilft der ersten Heuristik Ringe unter Benützung der Cluster-Hierarchie zu finden. Die zweite Heuristik baut auf den in der vorherigen Schicht gefundenen Ringen auf, indem sie nach gültigen Pfaden sucht, die an diese Ringe angeschlossen werden können. Drittens wird eine Reparaturheuristik angewendet, welche versucht verbleibende Knoten zu bestehenden Ringen zuzuweisen. Zuletzt werden lokale Suchverfahren eingesetzt, um die Gesamtkosten zu verbessern. Um zu überprüfen, wie gut dieser Lösungsansatz funktioniert, wurden zwei Testinstanz- Generatoren implementiert. Der Erste generiert Instanzen zufallsbasiert, der Zweite baut auf dem bekannten TSPLIB-Archiv auf. Die Evaluierung der zufallsbasierten Testinstanzen hat gezeigt, dass alle drei Heuristiken sämtliche Instanzen lösen konnten, wobei Girvan-Newman und Kernighan-Lin in jedem Testlauf Lösungen gefunden haben, war dies bei K-means nicht der Fall. Mit Kernighan-Lin konnte im Durchschnitt schneller eine Lösung gefunden werden, aber die Gesamtkosten waren bei den beiden anderen Algorithmen etwas besser. Mit den TSPLIB-basierten Testinstanzen konnte nicht mit allen Clusteringalgorithmen eine Lösung erzielt werden, aber zumindest war für jede Testinstanz mindestens ein Clustering-Verfahren erfolgreich. iv</abstract>
<intro confidence="0.814229">Contents</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>before the optimization. The clustering heuristics were compaired pairwise with the statistical student t-test on a significance level of 5%. pA stands for the comparison between GirvanNewman and Kernighan-Lin, pB between Kernighan-Lin and K-means, and pC between GirvanNewman and K-means.</title>
<marker></marker>
<rawString> before the optimization. The clustering heuristics were compaired pairwise with the statistical student t-test on a significance level of 5%. pA stands for the comparison between GirvanNewman and Kernighan-Lin, pB between Kernighan-Lin and K-means, and pC between GirvanNewman and K-means.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balakrishnan</author>
<author>T L Magnanti</author>
<author>P Mirchandani</author>
</authors>
<title>The multi-level network design problem. Working papers 3366-91.,</title>
<date>1991</date>
<institution>Massachusetts Institute of Technology (MIT), Sloan School of Management,</institution>
<contexts>
<context position="19885" citStr="[1]" startWordPosition="3384" endWordPosition="3384">uristic calculates the M shortest paths between the start- and the end-node and then calculates an MST for each of them and takes the minimum of the M solutions. Multi-level Network Design Problem In [1] the Multi-level Network Design (MLND) problem is introduced. It is a generalization of the Hierarchical Network Design (HNDP) problem, where K levels are used to describe the importance of nodes. If </context>
</contexts>
<marker>[1]</marker>
<rawString>A. Balakrishnan, T. L. Magnanti, and P. Mirchandani. The multi-level network design problem. Working papers 3366-91., Massachusetts Institute of Technology (MIT), Sloan School of Management, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bentley</author>
<author>B Floyd</author>
</authors>
<title>Programming pearls: a sample of brilliance.</title>
<date>1987</date>
<journal>Communications ACM,</journal>
<volume>30</volume>
<contexts>
<context position="40897" citStr="[2]" startWordPosition="7182" endWordPosition="7182">e the probability of choosing an item, that was already chosen grows with every successful iteration, making this approach unusable for practical applications. Algorithm 3.5: Naive Sampling Algorithm [2] input : The number of integers k that should be selected out of n. output: A set S of randomly selected integers. 1 S �-- 0; 2 while ISI &lt; k do 3 t �-- RandomInteger(1, n); 4 if t ∈/ S then 5 insert </context>
<context position="41416" citStr="[2]" startWordPosition="7286" endWordPosition="7286">rithm 3.6. In each iteration an element is chosen. Either a random number from 1 to j is added, or the current value of the iterator variable j. 23 Algorithm 3.6: Floyd’s Iterative Sampling Algorithm [2] input : The number of integers k that should be selected out of n. output: A set S of randomly selected integers. 1 S �-- 0; 2 for j �-- n — k + 1 to n do 3 t �-- RandomInteger(1, j); 4 if t ∈/ S the</context>
</contexts>
<marker>[2]</marker>
<rawString>J. Bentley and B. Floyd. Programming pearls: a sample of brilliance. Communications ACM, 30:754–757, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bertsimas</author>
<author>R Weismantel</author>
</authors>
<title>Optimization over Integers. Dynamic Ideas, 1st edition,</title>
<date>2005</date>
<contexts>
<context position="14550" citStr="[3]" startWordPosition="2322" endWordPosition="2322">V1 (1.3) (i,j)EE1liES,j /ES ∈ {0, 1}, ∀(i, j) ∈ V1 (1.4) xij 5 The constraints for the case k = 1 ensure that the nodes in the first layer build a Hamiltonian cycle. It is a classical TSP formulation [3]. Formulation for 1 &lt; k &lt; K: For each layer k a set of rings {Rk,1, ... Rk,mk } has to be found with the following constraints: Rk,i C E, bi = 1, ... , mk (1.5) |V (G[Rk,i]) n Vk |&gt; blk, bi = 1, ... ,</context>
</contexts>
<marker>[3]</marker>
<rawString>D. Bertsimas and R. Weismantel. Optimization over Integers. Dynamic Ideas, 1st edition, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Brandes</author>
</authors>
<title>A faster algorithm for betweenness centrality.</title>
<date>2001</date>
<journal>Journal of Mathematical Sociology,</journal>
<volume>25</volume>
<contexts>
<context position="28524" citStr="[9, 4]" startWordPosition="4850" endWordPosition="4851">ween the nodes s and t that “run through” the vertex v and σst(e) is defined as the number of SP between the nodes s and t that “run along” the edge e. This leads to the following formula definitions [9, 4]: Node betweenness of vertex v: Z s6=v6=t∈V Edge betweenness of edge e: Z s6=t∈V Girvan-Newman algorithm in pseudocode is shown in Algorithm 3.1. The algorithm works as follows: At the beginning the g</context>
</contexts>
<marker>[4]</marker>
<rawString>U. Brandes. A faster algorithm for betweenness centrality. Journal of Mathematical Sociology, 25:163–177, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Current</author>
<author>C S ReVelle</author>
<author>J L Cohon</author>
</authors>
<title>The hierarchical network design problem.</title>
<date>1986</date>
<journal>European Journal of Operational Research,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="19276" citStr="[5]" startWordPosition="3276" endWordPosition="3276">tics to tackle larger instances, as it was done in this thesis. 9 CHAPTER 2 Related Work Hierarchical Network Design Problem Current introduces the basic Hierarchical Network Design Problem (HNDP) in [5]. It consists of primary nodes building a primary path that from a start- to an end-node. The other nodes are the secondary nodes that have to be connected to the primary nodes via secondary paths. Th</context>
</contexts>
<marker>[5]</marker>
<rawString>J. R. Current, C. S. ReVelle, and J. L. Cohon. The hierarchical network design problem. European Journal of Operational Research, 27(1):57–66, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Easley</author>
<author>J Kleinberg</author>
</authors>
<title>Networks, Crowds, and Markets: Reasoning About a Highly Connected World.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="27722" citStr="[6]" startWordPosition="4702" endWordPosition="4702">ther metrics for example are the Manhattan distance, Hamming distance or Mahalanobis distance (e.g., where normalization of the vector is needed). Girvan-Newman Clustering The Girvan-Newman algorithm [6] is based on finding components in a network. It was successfully applied to various social networks. One usecase is the detection of online communities. A simple approach to cluster a network is to d</context>
<context position="30580" citStr="[6]" startWordPosition="5221" endWordPosition="5221">gure 3.3. To calculate the edge betweenness values the first step is to calculate the number of shortest paths between nodes. An efficient method that is based on Breadth-First-Search is described in [6]. The resulting SP values are shown in Table 3.1. Next, the proportion of the number of SP passing trough each edge has to be calculated. For each node pair (s, t) with s =74 t E V the set of all SP h</context>
</contexts>
<marker>[6]</marker>
<rawString>D. Easley and J. Kleinberg. Networks, Crowds, and Markets: Reasoning About a Highly Connected World. Cambridge University Press, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Fan</author>
<author>M E Muller</author>
<author>I Rezucha</author>
</authors>
<title>Development of sampling plans by using sequential (item by item) selection techniques and digital computers.</title>
<date>1962</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>57</volume>
<contexts>
<context position="40474" citStr="[7]" startWordPosition="7114" endWordPosition="7114">ere k can also vary randomly). A naive approach would be to choose items randomly until k distinct items have been selected, see Algorithm 3.5. Nevertheless, much better approaches exist (e.g., [19], [7] and [10]), especially when a large fraction of all the items has to be chosen. In this case the naive algorithm would have to generate many random values until one element is selected, that was not s</context>
</contexts>
<marker>[7]</marker>
<rawString>C. T. Fan, M. E. Muller, and I. Rezucha. Development of sampling plans by using sequential (item by item) selection techniques and digital computers. Journal of the American Statistical Association, 57:387–402, 1962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fortz</author>
</authors>
<title>Design of Survivable Networks with Bounded Rings.</title>
<date>2000</date>
<tech>PhD thesis,</tech>
<institution>Universite libre de Bruxelles,</institution>
<contexts>
<context position="21318" citStr="[8]" startWordPosition="3604" endWordPosition="3604">o gives a good survey about combinations of network structures between backbone (primary) and tributionary (secondary) topologies. Survivable Networks with Bounded Rings Fortz uses in this phd thesis [8] bounded rings for the reliability of networks. The resulting network has to be connected. A branch-and-cut approach and various heuristics are used to find the rings. Some of the heuristics are short</context>
</contexts>
<marker>[8]</marker>
<rawString>B. Fortz. Design of Survivable Networks with Bounded Rings. PhD thesis, Universite libre de Bruxelles, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Girvan</author>
<author>M E J Newman</author>
</authors>
<title>Community structure in social and biological networks.</title>
<date>2002</date>
<booktitle>Proceedings of the National Academy of Science,</booktitle>
<pages>99--7821</pages>
<contexts>
<context position="28524" citStr="[9, 4]" startWordPosition="4850" endWordPosition="4851">ween the nodes s and t that “run through” the vertex v and σst(e) is defined as the number of SP between the nodes s and t that “run along” the edge e. This leads to the following formula definitions [9, 4]: Node betweenness of vertex v: Z s6=v6=t∈V Edge betweenness of edge e: Z s6=t∈V Girvan-Newman algorithm in pseudocode is shown in Algorithm 3.1. The algorithm works as follows: At the beginning the g</context>
</contexts>
<marker>[9]</marker>
<rawString>M. Girvan and M. E. J. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Science, 99:7821–7826, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Jones</author>
</authors>
<title>A note on sampling a tape-file.</title>
<date>1962</date>
<journal>Communications ACM,</journal>
<volume>5</volume>
<contexts>
<context position="40483" citStr="[10]" startWordPosition="7116" endWordPosition="7116">n also vary randomly). A naive approach would be to choose items randomly until k distinct items have been selected, see Algorithm 3.5. Nevertheless, much better approaches exist (e.g., [19], [7] and [10]), especially when a large fraction of all the items has to be chosen. In this case the naive algorithm would have to generate many random values until one element is selected, that was not selected b</context>
</contexts>
<marker>[10]</marker>
<rawString>T. G. Jones. A note on sampling a tape-file. Communications ACM, 5:343, 1962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Kernighan</author>
<author>S Lin</author>
</authors>
<title>An Efficient Heuristic Procedure for Partitioning Graphs. The Bell system technical journal,</title>
<date>1970</date>
<volume>49</volume>
<issue>1</issue>
<contexts>
<context position="34969" citStr="[11]" startWordPosition="6087" endWordPosition="6087">ping current medoids with non-medoids one-by-one and checks if this new assignment gives an improvement. The pseudocode is shown in Algorithm 3.3. Kernighan-Lin Clustering The Kernighan-Lin-Algorithm [11] is another approach to solve the graph partitioning problem. It splits the set of vertices of a weighted graph into two subsets. The subsets have to be disjoint and of equal size. The sum of weights </context>
</contexts>
<marker>[11]</marker>
<rawString>B. W. Kernighan and S. Lin. An Efficient Heuristic Procedure for Partitioning Graphs. The Bell system technical journal, 49(1):291–307, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Klincewicz</author>
</authors>
<title>Hub location in backbone/tributary network design: a review.</title>
<date>1998</date>
<journal>Location Science, 6(1-4):307 –</journal>
<volume>335</volume>
<contexts>
<context position="20416" citStr="[12]" startWordPosition="3469" endWordPosition="3469"> primary layer can consist of more than exactly two designated nodes. An ILP formulation based on steiner trees and one based on multicommodity flow are provided. Aspects of Network Design Klincewicz [12] investigates various aspects of network design, like the cost, capacity, reliability, performance and demand pattern of networks. The cost can be important for hubs (nodes) and links either for creat</context>
</contexts>
<marker>[12]</marker>
<rawString>J. G. Klincewicz. Hub location in backbone/tributary network design: a review. Location Science, 6(1-4):307 – 335, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Laporte</author>
<author>I R Martín</author>
</authors>
<title>Locating a cycle in a transportation or a telecommunications network.</title>
<date>2007</date>
<journal>Networks,</journal>
<volume>50</volume>
<contexts>
<context position="18711" citStr="[13]" startWordPosition="3181" endWordPosition="3181">1 resembles an optimal TSP tour for these nodes. The problem to find appropriate rings for the other layers can be reduced from the Traveling Salesman Problem with Precedence Constraints (TSPPC), see [13], which was shown to be NP-hard. For layer 2 (layer 3) this means that if exactly two uplinks are contained that determine the precedence and bu2 = |V2 |(bu3 = |V3|), the single ring connecting all no</context>
</contexts>
<marker>[13]</marker>
<rawString>G. Laporte and I. R. Martín. Locating a cycle in a transportation or a telecommunications network. Networks, 50:92–108, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Lawler</author>
<author>J K Lenstra</author>
<author>A H G R Kan</author>
<author>D B Shmoys</author>
</authors>
<title>The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization.</title>
<date>1985</date>
<publisher>Wiley,</publisher>
<location>New York,</location>
<contexts>
<context position="18438" citStr="[14]" startWordPosition="3137" endWordPosition="3137">is would also have the impact that the assumption that P =6 NP holds could be rejected. In the case of the HRNP the first layer can be treated independently as a Traveling Salesman Problem (TSP), see [14], which is NP-complete. The optimal ring connection of all nodes of V1 resembles an optimal TSP tour for these nodes. The problem to find appropriate rings for the other layers can be reduced from the</context>
</contexts>
<marker>[14]</marker>
<rawString>E. L. Lawler, J. K. Lenstra, A. H. G. R. Kan, and D. B. Shmoys. The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization. Wiley, New York, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lee</author>
<author>S J Koh</author>
</authors>
<title>A design of the minimum cost ring-chain network with dualhoming survivability: A tabu search approach.</title>
<date>1997</date>
<journal>Computers &amp; Operations Research,</journal>
<volume>24</volume>
<issue>9</issue>
<contexts>
<context position="23443" citStr="[15]" startWordPosition="3954" endWordPosition="3954">he thesis contains a formulation of the Fixed Charge Network Design (FCND) problem. It involves demand between nodes, edge costs and the cost to use edges (satisfy demand). Ring-chain Dual Homing Lee [15] describes Self Healing Rings (SHR) in the context of the Ring-chain Dual Homing (RCDH) problem. An SHR is a cycle of network nodes that can reroute the traffic in case of a node failure. A chain is a</context>
</contexts>
<marker>[15]</marker>
<rawString>C. Y. Lee and S. J. Koh. A design of the minimum cost ring-chain network with dualhoming survivability: A tabu search approach. Computers &amp; Operations Research, 24(9):883 – 897, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lin</author>
<author>B W Kernighan</author>
</authors>
<title>An effective heuristic algorithm for the travelling-salesman problem.</title>
<date>1973</date>
<journal>Operations Research,</journal>
<volume>21</volume>
<contexts>
<context position="39047" citStr="[16]" startWordPosition="6870" endWordPosition="6870"> improved by the 2-Opt heuristic. A generalization of 2-Opt is the k-Opt heuristic, where an enhancement of k edges are checked. A common variant of the k-Opt heuristic is the Lin-Kernighan algorithm [16]. If k22 Opt heuristic with higher k is used better solutions can be found, but the processing is also much higher. The variant 3-Opt is usually a good compromise between runtime and optimization gain</context>
</contexts>
<marker>[16]</marker>
<rawString>S. Lin and B. W. Kernighan. An effective heuristic algorithm for the travelling-salesman problem. Operations Research, 21:498–516, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salman</author>
<author>V Kecman</author>
<author>Q Li</author>
<author>R Strack</author>
<author>E Test</author>
</authors>
<title>Fast k-means algorithm clustering.</title>
<date>2011</date>
<location>CoRR, abs/1108.1351,</location>
<contexts>
<context position="34586" citStr="[17]" startWordPosition="6035" endWordPosition="6035">eans++ is a variant that chooses the initial centroids in a more uniformly distributed way to avoid a selection where the centroids are too close. A fast method for K-means clustering is described in [17]. Partitioning Around Medoids is a variant of K-means. The medoids are analogous to the centroids from K-means. This algorithm systematically checks new medoid assignments by swapping current medoids </context>
</contexts>
<marker>[17]</marker>
<rawString>R. Salman, V. Kecman, Q. Li, R. Strack, and E. Test. Fast k-means algorithm clustering. CoRR, abs/1108.1351, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Thomadsen</author>
</authors>
<title>Hierarchical Network Design.</title>
<date>2005</date>
<tech>PhD thesis,</tech>
<institution>Technical University of Denmark,</institution>
<contexts>
<context position="23097" citStr="[18]" startWordPosition="3902" endWordPosition="3902">all edges and removes edges systematically. The crucial criterion for removing an edge is that the graph stays 2-connected after the removal. 11 Hierarchical Network Topologies Thomadsen’s phd thesis [18] focuses on hierarchical network topologies. Many properties are described as ILP formulations. A chapter about ring structures is provided. The thesis contains a formulation of the Fixed Charge Netwo</context>
</contexts>
<marker>[18]</marker>
<rawString>T. Thomadsen. Hierarchical Network Design. PhD thesis, Technical University of Denmark, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Vitter</author>
</authors>
<title>An efficient algorithm for sequential random sampling.</title>
<date>1987</date>
<journal>In ACM Transactions Mathematical Software,</journal>
<volume>13</volume>
<pages>58--67</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="40469" citStr="[19]" startWordPosition="7113" endWordPosition="7113">de (where k can also vary randomly). A naive approach would be to choose items randomly until k distinct items have been selected, see Algorithm 3.5. Nevertheless, much better approaches exist (e.g., [19], [7] and [10]), especially when a large fraction of all the items has to be chosen. In this case the naive algorithm would have to generate many random values until one element is selected, that was </context>
</contexts>
<marker>[19]</marker>
<rawString>J. S. Vitter. An efficient algorithm for sequential random sampling. In ACM Transactions Mathematical Software, volume 13, pages 58–67, New York, NY, USA, 1987. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Walshaw</author>
</authors>
<title>Multilevel refinement for combinatorial optimisation problems.</title>
<date>2004</date>
<journal>Annals of Operations Research,</journal>
<volume>131</volume>
<contexts>
<context position="39402" citStr="[20]" startWordPosition="6926" endWordPosition="6926"> compromise between runtime and optimization gained. Multilevel Heuristics The multilevel paradigm can be used for optimization problems, especially in the case of combinatorial optimization problems [20]. The approach is to coarsen the problem to get an approximate solution. Each coarsening iteration stands for a level in the multilevel algorithm. Various methods exist that make use of this paradigm.</context>
</contexts>
<marker>[20]</marker>
<rawString>C. Walshaw. Multilevel refinement for combinatorial optimisation problems. Annals of Operations Research, 131:325–372, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xie</author>
<author>S Kelley</author>
<author>B K Szymanski</author>
</authors>
<title>Overlapping community detection in networks: the state of the art and comparative study.</title>
<date>2011</date>
<location>CoRR, abs/1110.5813,</location>
<contexts>
<context position="72291" citStr="[21]" startWordPosition="12310" endWordPosition="12310">e algorithms designed for this thesis to related problems. 44 In the case of the Hierarchical Ring Network Problem more clustering heuristics could be investigated. For some clustering candidates see [21]. There is still room for improvements for finding rings with support of cluster hierarchies. One could consider centrality measures like modularity to find good subclusters. Metaheuristics like the V</context>
</contexts>
<marker>[21]</marker>
<rawString>J. Xie, S. Kelley, and B. K. Szymanski. Overlapping community detection in networks: the state of the art and comparative study. CoRR, abs/1110.5813, 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>