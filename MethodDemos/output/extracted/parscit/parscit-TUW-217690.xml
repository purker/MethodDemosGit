<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.160134">
<title confidence="0.970913">
Parallel Tracking and Mapping in Hofburg Festsaal
</title>
<author confidence="0.989297">
Georg Gerstweiler* Hannes Kaufmann Olga Kosyreva Christian Schönauer Emanuel Vonach
</author>
<affiliation confidence="0.968953">
Interactive Media Systems Group, Vienna University of Technology
</affiliation>
<figureCaption confidence="0.998284">
Figure 1: A user with the mobile setup in Festsaal (left) and a virtual theater stage placed on the real stage (right).
</figureCaption>
<sectionHeader confidence="0.911022" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.86795275">
Precise localization for mobile Augmented Reality in large indoor
environments without specific tracking infrastructure is
challenging. This is especially true for rooms with changing
properties, like lighting, seating and carpeting. With these
constraints a map for a vision based tracking approach has to be
continuously updated. The Parallel Tracking and Mapping
(PTAM) algorithm is capable of generating and extending a map
while tracking the camera pose in an unknown environment.
However, it has originally been designed for small workspace
environments and has therefore certain limitations. We have
extended and modified the original implementation in order to
ensure efficient and robust map generation and tracking in large
rooms. Furthermore, we have tested a mobile setup with the
system in Festsaal in Vienna’s Hofburg, which is close to
thousand square meters in size. The user’s position and path was
tracked while the environment was augmented with virtual objects
and the system was successfully tested for robustness and
occlusions.
Keywords: Tracking, and mapping, augmented, and virtual
realities
Index Terms: H.5.1 [Multimedia Information Systems]:
Artificial, augmented, and virtual realities—; I.4.8 [Scene
Analysis]: Object Recognition—Tracking
*gerstweiler/kaufmann/kosyreva/schoenauer/vonach@ims.tuwien.ac.at
</bodyText>
<sectionHeader confidence="0.998506" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9998995">
Vienna’s historic Hofburg accommodates many grandiose rooms,
the largest of which is Festsaal. These premises are being used as
venues for exhibitions, conferences and cultural events. In this
context our project partner, the Wiener Kongresszentrum
Hofburg, is interested in providing visitors with audio/video
information about the current room or paintings on walls and
ceilings (frescos) as well as different room configurations on a
mobile device. Furthermore, a visualization of the current position
on a map can aid navigation within Hofburg. This can be helpful
to find a certain conference booth or another person during an
event (e.g. a ball).
The rooms of the Hofburg are rich of features on walls and
ceilings, which makes them well-suited for a computer vision-
based approach. However, changes in lighting, seating, carpeting
and other changing elements like conference booths pose certain
restrictions. Therefore, we favor PTAM [1], which doesn’t require
any fiducial markers or models for pose estimation, over model-
based approaches.
</bodyText>
<sectionHeader confidence="0.959026" genericHeader="method">
2 SYSTEM &amp; WORKFLOW
</sectionHeader>
<bodyText confidence="0.999990631578947">
Our test platform was a dual-core laptop with an off-the-shelf
webcam. The software was based on Castle and Klein’s PTAM
implementation [2] with several extensions and modifications to
ensure efficient and robust map generation and tracking in large
rooms.
In a first stage the system was used to create a detailed map in a
semi-automatic procedure, while walking through the room. Our
adaptations made this process more efficient by improving the
selection of features and keyframes, resulting in a more precise
map while using less keyframes. New interface methods allow
manual intervention to avoid structures, which shouldn&apos;t be
integrated in the map (e.g. falsework present in Festsaal during
first tests, or chandeliers, which are problematic due to their
complicated structure and reflective properties). In addition, we
have modified the constraints of the original algorithm to allow
tracking behind the user&apos;s original position. New constraints
maintain efficiency in the larger environment for example by
avoiding high feature density in certain areas and removal of
features outside the basic cubical shape of the room.
</bodyText>
<sectionHeader confidence="0.998589" genericHeader="evaluation">
3 RESULTS &amp; CONCLUSION
</sectionHeader>
<bodyText confidence="0.9999784375">
We have tested our mobile setup in Festsaal and created a
detailed map. Once the camera pose was available, the
environment was augmented with various virtual content. In a first
test we added a virtual theater stage and walked around it. The
position and scaling of the scene was robust, while the frame-rate
remained close to the update-rate of the camera. In addition, we
added a virtual furniture set, which showed similar results.
Another important use case was tracking of a user within the map.
Therefore, we reconstructed the path of the user frame-by-frame
in real-time.
Another test demonstrated the system&apos;s behavior despite rapid
camera movement and occlusions. During first tests in Festsaal
there was a falsework in the middle of the room. While massive
occlusions pose problems on any vision-based tracking system,
with limited occlusions the tracking remained robust in many
cases.
</bodyText>
<sectionHeader confidence="0.999502" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999873142857143">
[1] Georg Klein and David Murray, Parallel Tracking and Mapping for
Small AR Workspaces. In Proc. International Symposium on Mixed
and Augmented Reality (ISMAR&apos;07, Nara), 2007.
[2] R. O. Castle, G. Klein, and D. W. Murray. Video-rate localization in
multiple maps for wearable augmented reality. In Proc. 12th IEEE
Int. Symp. on Wearable Computing, Pittsburgh PA, pages 15–22,
2008.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.169767">
<title confidence="0.998778">Parallel Tracking and Mapping in Hofburg Festsaal</title>
<author confidence="0.993609">Georg Gerstweiler Hannes Kaufmann Olga Kosyreva Christian Schönauer Emanuel Vonach</author>
<affiliation confidence="0.971394">Interactive Media Systems Group, Vienna University of Technology</affiliation>
<abstract confidence="0.987822047619048">Figure 1: A user with the mobile setup in Festsaal (left) and a virtual theater stage placed on the real stage (right). Precise localization for mobile Augmented Reality in large indoor environments without specific tracking infrastructure is challenging. This is especially true for rooms with changing properties, like lighting, seating and carpeting. With these constraints a map for a vision based tracking approach has to be continuously updated. The Parallel Tracking and Mapping (PTAM) algorithm is capable of generating and extending a map while tracking the camera pose in an unknown environment. However, it has originally been designed for small workspace environments and has therefore certain limitations. We have extended and modified the original implementation in order to ensure efficient and robust map generation and tracking in large rooms. Furthermore, we have tested a mobile setup with the system in Festsaal in Vienna’s Hofburg, which is close to thousand square meters in size. The user’s position and path was tracked while the environment was augmented with virtual objects and the system was successfully tested for robustness and occlusions. Tracking, and mapping, augmented, and virtual realities</abstract>
<note confidence="0.712837">H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities—; I.4.8 [Scene</note>
<title confidence="0.587697">Analysis]: Object Recognition—Tracking</title>
<abstract confidence="0.999375132075472">Vienna’s historic Hofburg accommodates many grandiose rooms, the largest of which is Festsaal. These premises are being used as venues for exhibitions, conferences and cultural events. In this context our project partner, the Wiener Kongresszentrum Hofburg, is interested in providing visitors with audio/video information about the current room or paintings on walls and ceilings (frescos) as well as different room configurations on a mobile device. Furthermore, a visualization of the current position on a map can aid navigation within Hofburg. This can be helpful to find a certain conference booth or another person during an event (e.g. a ball). The rooms of the Hofburg are rich of features on walls and ceilings, which makes them well-suited for a computer visionbased approach. However, changes in lighting, seating, carpeting and other changing elements like conference booths pose certain restrictions. Therefore, we favor PTAM [1], which doesn’t require any fiducial markers or models for pose estimation, over modelbased approaches. Our test platform was a dual-core laptop with an off-the-shelf webcam. The software was based on Castle and Klein’s PTAM implementation [2] with several extensions and modifications to ensure efficient and robust map generation and tracking in large rooms. In a first stage the system was used to create a detailed map in a semi-automatic procedure, while walking through the room. Our adaptations made this process more efficient by improving the selection of features and keyframes, resulting in a more precise map while using less keyframes. New interface methods allow manual intervention to avoid structures, which shouldn&apos;t be integrated in the map (e.g. falsework present in Festsaal during first tests, or chandeliers, which are problematic due to their complicated structure and reflective properties). In addition, we have modified the constraints of the original algorithm to allow tracking behind the user&apos;s original position. New constraints maintain efficiency in the larger environment for example by avoiding high feature density in certain areas and removal of features outside the basic cubical shape of the room. We have tested our mobile setup in Festsaal and created a detailed map. Once the camera pose was available, the environment was augmented with various virtual content. In a first test we added a virtual theater stage and walked around it. The position and scaling of the scene was robust, while the frame-rate remained close to the update-rate of the camera. In addition, we added a virtual furniture set, which showed similar results. Another important use case was tracking of a user within the map. Therefore, we reconstructed the path of the user frame-by-frame in real-time. Another test demonstrated the system&apos;s behavior despite rapid camera movement and occlusions. During first tests in Festsaal there was a falsework in the middle of the room. While massive occlusions pose problems on any vision-based tracking system, with limited occlusions the tracking remained robust in many cases.</abstract>
<note confidence="0.852361428571428">[1] Georg Klein and David Murray, Parallel Tracking and Mapping for Small AR Workspaces. In Proc. International Symposium on Mixed and Augmented Reality (ISMAR&apos;07, Nara), 2007. [2] R. O. Castle, G. Klein, and D. W. Murray. Video-rate localization in multiple maps for wearable augmented reality. In Proc. 12th IEEE Int. Symp. on Wearable Computing, Pittsburgh PA, pages 15–22, 2008.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Georg Klein</author>
<author>David Murray</author>
</authors>
<title>Parallel Tracking and Mapping for Small AR Workspaces.</title>
<date>2007</date>
<booktitle>In Proc. International Symposium on Mixed and Augmented Reality (ISMAR&apos;07,</booktitle>
<location>Nara),</location>
<contexts>
<context position="2621" citStr="[1]" startWordPosition="373" endWordPosition="373">ell-suited for a computer visionbased approach. However, changes in lighting, seating, carpeting and other changing elements like conference booths pose certain restrictions. Therefore, we favor PTAM [1], which doesn’t require any fiducial markers or models for pose estimation, over modelbased approaches. 2 SYSTEM &amp; WORKFLOW Our test platform was a dual-core laptop with an off-the-shelf webcam. The s</context>
</contexts>
<marker>[1]</marker>
<rawString>Georg Klein and David Murray, Parallel Tracking and Mapping for Small AR Workspaces. In Proc. International Symposium on Mixed and Augmented Reality (ISMAR&apos;07, Nara), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Castle</author>
<author>G Klein</author>
<author>D W Murray</author>
</authors>
<title>Video-rate localization in multiple maps for wearable augmented reality.</title>
<date>2008</date>
<booktitle>In Proc. 12th IEEE Int. Symp. on Wearable Computing,</booktitle>
<pages>15--22</pages>
<location>Pittsburgh PA,</location>
<contexts>
<context position="2884" citStr="[2]" startWordPosition="414" endWordPosition="414">ose estimation, over modelbased approaches. 2 SYSTEM &amp; WORKFLOW Our test platform was a dual-core laptop with an off-the-shelf webcam. The software was based on Castle and Klein’s PTAM implementation [2] with several extensions and modifications to ensure efficient and robust map generation and tracking in large rooms. In a first stage the system was used to create a detailed map in a semi-automatic </context>
</contexts>
<marker>[2]</marker>
<rawString>R. O. Castle, G. Klein, and D. W. Murray. Video-rate localization in multiple maps for wearable augmented reality. In Proc. 12th IEEE Int. Symp. on Wearable Computing, Pittsburgh PA, pages 15–22, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>