<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta>
      <journal-title-group>
        <journal-title>Proceedings of IJCAI</journal-title>
      </journal-title-group>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>An Extended Local Branching Framework and its Application to the Multidimensional Knapsack Problem</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>unter der Anleitung von</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>a.o. Univ.-Prof. Dipl.-Ing. Dr. G u ̈nther Raidl Univ.-Ass. Dipl.-Ing. Jakob Puchinger</institution>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>durch Daniel Lichtenberger Matr. Nr.</institution>
          <addr-line>9825754 Ma ̈rzstrasse 80/12 1150 Wien</addr-line>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2004</year>
      </pub-date>
      <volume>01</volume>
      <issue>2001</issue>
      <fpage>0</fpage>
      <lpage>1</lpage>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>Datum</title>
    </sec>
    <sec id="sec-2">
      <title>Unterschrift</title>
      <p>This thesis deals with local branching, a local search algorithm applied on top of a Branch
and Cut algorithm for mixed integer programming problems. Local branching defines custom
sized neighborhoods around given feasible solutions and solves them partially or completely
before exploring the rest of the search space. Its goal is to improve the heuristic behavior of
a given exact integer programming solver, i.e. to focus on finding good solutions early in the
computation.</p>
      <p>Local branching is implemented as an extension to the open source Branch and Cut
solver COIN/BCP. The framework’s main goal is to provide a generic implementation of local
branching for integer programming problems. IP problems are optimization problems where
some or all variables are integer values and must satisfy one or more (linear) constraints.
Several extensions to the standard local branching algorithm were added to the framework.
Pseudo-concurrent exploration of multiple local trees, aborting local trees and a variable
fixing heuristic allow the user to implement sophisticated search metaheuristics that adjust the
local branching parameters adaptively during the computation. A major design goal was to
provide a clean encapsulation of the local branching algorithm to facilitate embedding of the
framework in other, higher-level search algorithms, for example in evolutionary algorithms.</p>
      <p>As an example application, a solver for the multidimensional knapsack problem is
implemented. A custom local branching metaheuristic imposes node limits on local subtrees and
adaptively tightens the search space by fixing variables and reducing the size of the
neighborhood. Test results show that local branching can offer significant advantages to standard
Branch and Cut algorithms and eventually proves optimality in shorter time. Especially for
large, complex test instances exploring the local neighborhood of a good feasible solution
often yields better short-term results than the unguided standard Branch and Cut algorithm.
Improving the solutions found early in the computation also helps to remove additional parts
of the search tree, potentially leading to better solutions in longer runs.</p>
      <sec id="sec-2-1">
        <title>Zusammenfassung</title>
        <p>Diese Diplomarbeit bescha¨ftigt sich mit Local Branching, einem lokalen Suchalgorithmus,
der auf einem Branch and Cut Algorithmus fu¨r ganzzahlige Optimierungsprobleme aufsetzt.
Local Branching definiert beliebig große Nachbarschaften um gegebene gu¨ltige Lo¨sungen
und lo¨st diese teilweise oder komplett, bevor der Rest des Lo¨sungsraums durchsucht wird.
Das Ziel ist eine Verbesserung des heuristischen Verhaltens des gegebenen Solvers fu¨r
ganzzahlige Optimierungsprobleme, d.h. sich auf das mo¨glichst fru¨he Finden guter Lo¨sungen
zu konzentrieren.</p>
        <p>Local Branching ist als Erweiterung des Open Source Branch and Cut Solvers COIN/BCP
implementiert. Das Hauptziel des Frameworks ist eine generische Implementierung von
Local Branching fu¨r ganzzahlige Optimierungsprobleme, also Probleme, bei denen alle
oder einige Variablen ganzzahlig sein mu¨ssen, und zusa¨tzlich eine oder mehrere (lineare)
Bedingungen in Form von Ungleichungen erfu¨llen mu¨ssen. Es wurden mehrere Erweiterungen
zum Framework hinzugefu¨gt: die pseudo-parallele Abarbeitung mehrerer lokaler Suchba¨ume,
das vorzeitige Terminieren lokaler Suchba¨ume sowie eine unabha¨ngige
Variablen-FixingHeuristik. Durch diese Erweiterungen ko¨nnen die Parameter fu¨r Local Branching im Laufe der
Berechnung beliebig vera¨ndert werden. Ein wesentliches Ziel beim Entwurf des Frameworks
war eine klare Kapselung des Local Branching Algorithmus, um die Einbettung in andere,
ho¨here Suchalgorithmen zu ermo¨glichen, etwa in evolutiona¨re Algorithmen.</p>
        <p>Als Beispielapplikation wurde ein Solver fu¨r das mehrdimensionale Rucksackproblem
implementiert. Eine eigene Local Branching Metaheuristik beschra¨nkt die Gro¨ße lokaler
Ba¨ume durch Knotenlimits und kann den Suchraum durch Anwendung der
VariablenFixing-Heuristik weiter einschra¨nken. Die Testergebnisse zeigen signifikante Vorteile fu¨r
Local Branching im Vergleich zum normalen Branch and Cut Algorithmus. Vor allem fu¨r
große, komplexe Testinstanzen liefert die Suche in lokalen Ba¨umen oft bessere Resultate
am Anfang der Berechnung. Dadurch wird auch die Zeit zum Finden (und Beweisen) der
optimalen Lo¨sung potentiell verringert, da dadurch fru¨her zusa¨tzliche Teile des Suchbaums
weggeschnitten werden ko¨nnen.
5.4.1 The LP Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
5.4.2 Managing the LP Relaxation . . . . . . . . . . . . . . . . . . . . . . 24
5.4.3 Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
5.5 Parallelizing COIN/BCP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
5.5.1 Inter-Process Communication . . . . . . . . . . . . . . . . . . . . . 25
5.5.2 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.6 Developing Applications with COIN/BCP . . . . . . . . . . . . . . . . . . . 26
5.6.1 The BCP tm user Class . . . . . . . . . . . . . . . . . . . . . . . . 27
5.6.2 The BCP lp user Class . . . . . . . . . . . . . . . . . . . . . . . . 27
6 Implementation of the Framework 29
6.1 Integrating Local Branching into COIN/BCP . . . . . . . . . . . . . . . . . 31
6.1.1 Identifying Local Tree Nodes . . . . . . . . . . . . . . . . . . . . . 31
6.1.2 The LB tm Module . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6.1.3 The LB lp Module . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.2 Managing Local Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
6.2.1 The LocalTreeIndex . . . . . . . . . . . . . . . . . . . . . . . . . . 36
6.2.2 The LocalTreeManager . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.3 Controlling Local Branching . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.3.1 Implementing a Basic Local Branching Algorithm . . . . . . . . . . 38
8.4.5 Handling Terminated Local Trees . . . . . . . . . . . . . . . . . . . 55
8.5 Finishing Touches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
9 Test Results 57
9.1 Test Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
9.2 Test Results Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
9.2.1 Final Objective Comparison . . . . . . . . . . . . . . . . . . . . . . 58
9.2.2 Online Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 58
9.3 Local Branching Configurations . . . . . . . . . . . . . . . . . . . . . . . . 59
9.4 Short-Time Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
9.4.1 Local Branching and Node Limits . . . . . . . . . . . . . . . . . . . 60
9.4.2 Cut Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
9.4.3 Multiple Initial Solutions . . . . . . . . . . . . . . . . . . . . . . . . 67
9.5 Long Runs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
10 Summary and Outlook</p>
        <sec id="sec-2-1-1">
          <title>Chapter 1</title>
          <p>Introduction
Integer programming problems (IPs) are optimization problems that restrict some or all
variables to integer values. In contrast to linear programming problems (LPs) without integrality
constraints, IPs are NP-hard. Much research has gone into effective search algorithms for
integer programs, leading to exact algorithms like Branch and Bound [25], cutting plane
algorithms [30], and a large variety of heuristical algorithms that trade optimality for quickly
getting “good enough” solutions.</p>
          <p>This thesis considers the modification of standard Branch and Cut to follow ideas from
local search based heuristics, the so-called local branching [13]. Branch and Bound is a generic
algorithm for solving integer programming problems by partitioning the search space into
smaller subproblems (branching), calculating bounds on the best solution that can be found
in a subproblem (bounding), and removing those subproblems that are proven to contain only
solutions inferior to the best known solution (pruning). The bounding operation is commonly
executed by solving the LP relaxation (i.e. the IP problem without the integral constraints).
Branch and Cut tries to delay the branching operation by adding constraints (cuts) that are
violated by the current LP result, leading to a reduction of the search tree size.</p>
          <p>Local branching defines subproblems through additional local branching cuts that isolate
a neighborhood of a certain size around a given feasible solution. By exploring this smaller
subproblem before the rest of the search tree, the intention is to improve good feasible solutions
before continuing Branch and Cut in a standard way. Several extensions have been added to
local branching: pseudo-concurrent tree exploration, the possibility to abort local trees, and a
variable fixing heuristic have been added. Due to its general design, local branching can be
used with any IP solver.</p>
          <p>A large part of integer programming is concerned with combinatorial problems. These
include for example the subset sum equality problem, various graph theory problems, and the
well-known family of knapsack problems. In this thesis, the multidimensional knapsack
problem is used to demonstrate the use and the benefits of local branching. Although all types of
knapsack problems are NP-hard, some problems can be efficiently solved by enumerative
techniques like dynamic programming. For others, like the multidimensional knapsack problem,
no such methods are known. These problems supply well suited testcases for fully fledged
Branch and Cut solvers, and are often too complex to be solved to optimality in reasonable
time.
1.1</p>
          <p>Thesis Overview
In chapter 2, an overview of integer programming problems, cutting plane techniques and
Branch and Bound algorithms is given to summarize the building blocks of Branch and Cut.
Chapter 3 provides an introduction to local branching as proposed by Fischetti and Lodi [13].
Chapter 4 introduces the framework implemented for this thesis, including extensions to the
local branching algorithm, and describes the overall design of the interface to the framework.
In chapter 5, an overview of the open source COIN/BCP framework used for implementing
local branching is given. Chapter 6 contains the implementation details of the local branching
framework. An overview of knapsack problems in general and multidimensional knapsack
problems in particular is given in chapter 7. The implementation of a sample local branching
application for the multidimensional knapsack problem is described in chapter 8. Test results
exploring the benefits and drawbacks of local branching based on the sample application are
given in chapter 9. Chapter 10 summarizes the results and provides a brief outlook on possible
future work. In appendix A, the patches necessary for the COIN/BCP source code are
described. Appendix B provides a brief overview of the test scripts used for analyzing the local
branching test runs.</p>
        </sec>
        <sec id="sec-2-1-2">
          <title>Chapter 2</title>
          <p>Branch and Cut
Branch and Cut is an exact algorithm for solving integer programming problems. It combines
cutting plane methods with Branch and Bound. The following introduction is based on Lee and
Mitchell’s Branch and Bound tutorial [26], Mitchell’s introduction to Branch and Cut [29], the
COIN/BCP User’s Manual by Ralphs and Ladanyi [36], and the book on integer programming
by Laurence Wolsey [42].
2.1</p>
          <p>Integer Programming Problems
An integer programming problem (IP) is an optimization problem in which some or all
variables are restricted to integer values. A given objective function has to be maximized or
minimized in a solution space constrained by inequalities. A mixed integer programming problem
(MIP) contains both integer and continuous variables, a pure integer programming problem
restricts all variables to be integer. Mixed or pure 0-1 integer programming problems restrict
all integer variables to be 0 or 1, thus they are also called binary integer programming
problems. In this thesis we will concentrate on linear 0-1 integer programming problems where
all variables are binary and all terms of the objective function and constraints are linear. The
objective function should be maximized. A linear 0-1 IP can then be stated as:
(2.1)
(2.2)
maximize
subject to</p>
          <p>cT x</p>
          <p>Ax ≤ b
x ∈ {0, 1}n</p>
          <p>S = {x ∈ {0, 1}n : Ax ≤ b}.</p>
          <p>with A ∈ Rm×n, b ∈ Rm and c ∈ Rn. We can define the solution space S of a problem as
2.1.1</p>
          <p>Convex Hull of an Integer Program
In algebraic topology, Ax ≤ b defines a convex polyhedron which contains all feasible
solutions of the integer program. H. Weyl proved in 1935 that a convex polyhedron can be defined
as the intersection of a finite number of half-spaces or as the convex hull combined with the
conical hull of a finite number of vectors or points. If the problem is formulated in rational
numbers, Weyl’s theorem implies the existence of a finite system of linear inequalities whose
solution set coincides with the convex hull of our solution space S, also written as conv(S).
This directly leads to cutting plane algorithms for solving integer programming problems that
will be described in section 2.3.
2.1.2</p>
          <p>Relaxations
A key concept of integer programming is that of problem relaxation. A relaxation of an
optimization problem as stated in equation (2.1) is an optimization problem
max{cTRx : x ∈ SR},
(2.3)
where S ⊆ SR and cT x ≤ cT x for all x ∈ S. The relaxed solution space is a superset of</p>
          <p>R
the problem solution space, and the relaxed objective function is equal to or greater than the
original function for all feasible solutions of the given problem.</p>
          <p>A common relaxation for linear integer programming problems is the linear programming
relaxation (LP relaxation). The integer constraints on all variables are removed and the
problem can then be solved with linear programming methods. The most common algorithm for
solving linear programs is the simplex method invented by George Bernard Dantzig in 1947.
There are instances where the simplex method requires an exponential number of steps, but
those problems seem to be highly unlikely in practical applications where the simplex method
achieves very good performance.</p>
          <p>Khachian’s ellipsoid algorithm [22] proved that linear programming was polynomial in
1979. Karmarkar’s interior-point method [20] was both a practical and theoretical
improvement over the ellipsoid algorithm.
2.2</p>
          <p>Branch and Bound
Branch and Bound is a class of exact algorithms for various optimization problems, especially
integer programming problems and combinatorial optimization problems (COP). It partitions
the solution space into smaller subproblems that can be solved independently (branching).
Bounding discards subproblems that cannot contain the optimal solution, thus decreasing the
size of the solution space. Branch and Bound was first proposed by Land and Doig in 1960 [25]
for solving integer programs.</p>
          <p>Given a maximization problem as described in equations (2.1) and (2.2), a Branch and
Bound algorithm iteratively partitions the solution space S, for example by branching on
binary variables - fixing one of them to 0 in one branch and to 1 in the other branch. For each
subproblem an upper bound on the objective value is calculated. The upper bound is
guaranteed to be equal to or greater than the optimal solution for this subproblem. When a feasible
solution (i.e., no fractional variables remaining) is found, all subproblems whose upper bounds
are lower than this solution’s objective value can be discarded. The best known feasible
solution represents a lower bound for all subproblems, and only subproblems with an upper
bound greater than the global lower bound have to be considered. Discarding a subproblem is
called fathoming or pruning. Upper bounds for a subproblem can be obtained by relaxing the
subproblem, thus they are often obtained by optimizing the subproblem’s LP relaxation.</p>
          <p>Figure 2.1 summarizes the above steps using a pseudo-code notation. The sequence of
subproblems created by branching can be organized as a rooted directed graph. The original
1. Initialize list of all subproblems C = {S}
2. Generate a feasible solution and store it in sˆ. It is not necessary to generate a feasible
solution (e.g. by heuristics), but it can help to reduce the search tree size. When no
initial solution is provided, the objective value for sˆ is set to −∞.
3. Repeat while C 6= ∅:
(a) Take a subproblem S0 from C
(b) Relax S0 and solve the relaxed problem
(c) Decide to branch or prune as explained in figure 2.2.
4. Return sˆ
problem is the root node with edges going to each of its children. This graph is called the
search tree and its nodes represent all generated subproblems.
2.3</p>
          <p>Cutting Plane Algorithms
As in section 2.1, we will consider a binary integer programming problem, its mathematical
formulation is stated in equations (2.1) and (2.2). The fundamental concept used for cutting
plane algorithms is that of a valid inequality. An inequality
πx ≤ π0
(2.4)
is valid if πx ≤ π0 for all x ∈ S, where S contains all feasible solutions of the IP.</p>
          <p>The basic idea of cutting planes is to describe the convex hull conv(S) of the original
problem by adding valid inequalities to the LP relaxation until the LP solution becomes feasible
for the original problem.</p>
          <p>Mitchell [30] outlines the following structure of a cutting plane algorithm:
1. Solve the LP relaxation using linear programming methods such as the simplex
algorithm.
2. If the LP solution is feasible for the integral problem, return the optimal solution.
3. Otherwise add cutting planes to the relaxation that separate the LP solution from the
convex hull of feasible integral points.
4. Go to first step.</p>
          <p>Cutting planes can be generated with or without problem specific knowledge. One method
of obtaining cutting planes is by combining inequalities from the current LP relaxation. This
is known as integer rounding, and the resulting cutting planes are called Chva´tal-Gomory
cutting planes [15, 16, 7]. The following example is taken from [30]. Consider the integer
programming problem
Depending on the solution of the relaxed problem, do one of the following:
1. No solution was found, the relaxed problem is infeasible. Then there is also no feasible
solution in S0, thus the subproblem is pruned.
2. The optimal solution is not better than sˆ. The subproblem can be pruned because its
upper bound is lower than the global lower bound.
3. The optimal solution is better than sˆ and it is in S0 (the integer constraints are satisfied).</p>
          <p>Replace sˆ with the new optimal solution. The subtree can be pruned because no better
solution can be found.
4. The optimal solution is better than sˆ but it is not in S0 (at least one integer constraint is
violated). In this case S0 is partitioned into n smaller subproblems such that: Sin=1 Si0 =
S0. Each of these children of S0 is added to C. This is the common case and is usually
called branching.
A cutting plane is obtained by a weighted combination of inequalities, e.g.</p>
          <p>minimize − 2x1 − x2
x1 + 2x2 ≤ 7
2x1 − x2 ≤ 3
x1, x2 ∈ N0.</p>
          <p>0.2(x1 + 2x2 ≤ 7) + 0.4(2x1 − x2 ≤ 3)
gives the valid inequality</p>
          <p>This inequality is valid for all LP relaxations, but in a feasible solution, the left hand side
must be an integer value. This leads to the inequality</p>
          <p>Gomory’s cutting plane algorithm will find the optimal solution by iterating the steps as
described above. However, the number of steps to describe the convex hull (called the Chva´tal
rank) is typically very high, leading to very slow convergence [10, 11].</p>
          <p>It can be enhanced using techniques like adding many Chva´tal-Gomory cuts at once, as
shown in [1] and [5]. Another approach is to combine cutting plane methods with Branch and
Bound, which leads to a method called Branch and Cut.</p>
          <p>x1 ≤ 2.6.
x1 ≤ 2.
(2.5)
(2.6)
(2.7)
(2.8)
1. Initialize candidate list C = {S}
2. Generate a feasible solution and store it in sˆ
(b) Relax S0 and solve the relaxed problem, store LP result in s˜
(c) Repeat:
(1) Try to add cuts to the relaxed problem that are violated by s˜
(2) Exit loop when no new cuts were generated in step 1
(3) Solve relaxed problem again, store LP result in s˜. Note that the objective
value of s˜ is monotonically decreasing since the added cuts render infeasible
the previous LP results.</p>
          <p>(d) Depending on s˜, decide to branch or prune the node as shown in figure 2.2.
4. Return sˆ</p>
          <p>
            Branch and Cut
Branch and Cut methods use Branch and Bound to partition the solution space into smaller
subproblems, but also utilize cutting plane methods to tighten the relaxation and thus to reduce
the size of the search tree. Branch and Cut was first proposed by Padberg and Rinaldi [
            <xref ref-type="bibr" rid="ref1">31</xref>
            ] as
a framework for solving traveling salesman problems.
          </p>
          <p>The purpose of cutting planes or cuts is to reduce the upper bound derived from the optimal
solution of the LP relaxation. A smaller upper bound makes pruning the subproblem more
likely, thus reducing the search tree size. When the algorithm failed to generate new cuts that
are violated by the current LP solution, the subproblem is branched as in Branch and Bound.</p>
          <p>As cut generation can be very expensive, it is common to generate cuts only for some nodes
in the search tree. For example, it might be reasonable to generate cuts for every eighth node or
for all nodes at a depth of a multiple of eight. The cut-and-branch variant adds cutting planes
only at the root node. A pseudo-code formulation of Branch and Cut is given in figure 2.3.</p>
        </sec>
        <sec id="sec-2-1-3">
          <title>Chapter 3</title>
          <p>Local Branching
While there exist sophisticated solvers for integer programming problems, for many hard
problems the optimal solution is often hard to find within a reasonable time. Therefore, it becomes
increasingly important to find reasonably good solutions early in the computation process.</p>
          <p>Local Branching is a local search meta-heuristic for integer programs proposed by Fischetti
and Lodi in 2002 [13] that is entirely embedded in a Branch and Cut framework. Its goal is to
improve the heuristic behavior of a given MIP solver without losing optimality, that is, to find
good feasible solutions as soon as possible while still being able to find the global optimum
and prove its optimality.</p>
          <p>Local Branching works by partitioning the search tree through so-called local branching
cuts. Since those local cuts are just specific constraints for integer programming problems,
they can be expressed like normal IP constraints using any generic MIP solver.
3.1</p>
          <p>Soft vs. Hard Variable Fixing
A common technique for IP heuristics is hard variable fixing. For example, a heuristic might
use a LP solver to compute a continuous optimal solution, heuristically fix some variables to
integer values (e.g. by rounding the variable with the least fractional value), and then repeating
these steps for the resulting subproblem without the fixed variables. This way, relatively good
(but probably not optimal) solutions may be found in reasonable time even for hard problems.</p>
          <p>The major downside of this approach is that it may be nearly impossible during the early
stages to decide which variable should be fixed. This inevitably leads to bad fixings which may
not be detected until much later, requiring some kind of backtracking to undo bad choices.</p>
          <p>To overcome this limitation of variable fixing, Fischetti and Lodi proposed soft variable
fixing. It does not select a single variable for fixing, but only specifies that a certain percentage
of all variables of a given feasible solution should be fixed. This approach is best illustrated
using the binary integer programming problem described in section 2.1. Supposing there is a
feasible solution and 90% of its nonzero variables should be fixed to 1, Fischetti and Lodi add
a soft fixing constraint
to the current formulation. x¯j represents the feasible solution around which a local
neighn n
X x¯j xj ≥ d0.9 X x¯j e
j=1 j=1
(3.1)
borhood is isolated, i.e. in any feasible solution xj only 10% of those variables set to 1 in x¯j
may be flipped to 0. The idea is that fixing 90% of the variables helps the solver to find good
solutions as effectively as when fixing a large number of variables, but with a much larger
degree of freedom.
3.2</p>
          <p>A Basic Local Branching Framework
Given a binary integer programming problem as stated in section 2.1 and a feasible solution
x¯, the binary support S¯ is defined as S¯ := {j ∈ [1, n] : x¯j = 1}, i.e. the indices of those
variables that are set to 1. A soft fixing constraint in terms of the previous section can then be
formulated as
(3.2)
(3.3)
(3.5)
Δ(x, x¯) := X (1 − xj) + X xj ≤ k.</p>
          <p>j∈S¯ j∈/S¯</p>
          <p>Fischetti and Lodi call this a local branching constraint that counts all binary variables
that flipped their value from zero to one or from one to zero compared to x¯. Δ(x, x¯) actually
represents the Hamming distance between x and x¯, thus the constraint is also called Hamming
distance constraint. When the cardinality of S¯ is fixed, this constraint is equivalent to
j∈S¯
because for every variable xj with j ∈ S¯ that flips from one to zero another variable must
flip from zero to one. This definition is consistent with the classical k’-opt neighborhood for
the Traveling Salesman Problem, where at most k0 edges may be replaced.</p>
          <p>A local branching constraint partitions the search tree in two disjunct branches
Δ(x, x¯) ≤ k (local branch)
and
Δ(x, x¯) &gt; k (normal branch).</p>
          <p>(3.4)</p>
          <p>The local branch is completely solved before continuing with the normal branch. When a
new global optimum x¯2 was found in the local branch, local branching can continue with the
new solution by adding a new constraint to the remaining “normal” branch, again partitioning
the search tree in two disjunct branches
Δ(x, x¯) &gt; k, Δ(x, x¯2) ≤ k
Δ(x, x¯) &gt; k, Δ(x, x¯2) &gt; k
(local branch),
(normal branch).</p>
          <p>This scheme works as long as the local branching trees yield new global optima and is
illustrated in figure 3.2. The numbers indicate the sequence in which the subproblems are
generated and processed. The actual optimization problems are solved by a generic MIP solver.</p>
          <p>The size of the local subtrees at positions 2, 4 and 6 depend on the choice of the k
parameter. Small values of k define a relatively small neighborhood that is easier to solve, but may
not contain solutions that are significantly better than the current one. Larger values of k offer
higher degrees of freedom during the tree search, but drastically increase the size of the local
branching trees.
¡
º¡ ·</p>
          <p>2
¹¸
MIP
solver
improved solution x¯2
¡
º¡ ·</p>
          <p>4
¹¸
MIP
solver
improved solution x¯3
¡
º¡ ·</p>
          <p>6
¹¸
MIP
solver
no improved solution
3.3</p>
          <p>Local Branching Extensions
Fischetti and Lodi [13] proposed several extensions to the standard local branching algorithm
described in the previous section.</p>
          <p>• Imposing a time limit on local branching trees allows to use large values of k without
having to explore a local tree completely. When time runs out and a better solution
was found in the local tree, the algorithm creates a new local tree at the original root
node using the new solution. However, since the previous local tree was not explored
completely, this may lead to a duplication of effort as the optimal solution might still
be in the first local tree, and its search space can therefore not be excluded. If the time
limit is reached without finding a new better solution, k is decreased to speed up the
exploration of the local tree.
• Diversification may be used when a local tree did not improve the best known solution.</p>
          <p>Fischetti and Lodi suggest to start with a soft diversification by enlarging the
neighborhood, e.g. by dk/2e. When no better solution is found in this larger local tree, they apply
a strong diversification by taking another (worse) solution and restarting local branching
with this solution.
• Embedding local branching in heuristic frameworks like Tabu Search, Variable
Neighborhood Search, Simulated Annealing or Evolutionary Algorithms can be easily done,
since local branching naturally defines a custom sized neighborhood around a given
solution. Additional constraints imposed by the heuristic framework can be described as
linear cuts which makes them easy to join with local branching constraints.
• Working with infeasible solutions is necessary for problems where finding an initial
feasible solution is hard, e.g. for hard set partitioning models. In order to use an infeasible
solution as initial solution for local branching, one may define additional slack variables
for some of the constraints while penalizing them in the objective function.
• General integer variables require a new definition of the local branching constraint.</p>
          <p>Some general integer problems still have a relevant subset of 0-1 variables that can be
used for local branching. In case there are no relevant binary variables, introducing
weights leads to a viable local branching constraint. In a MIP model that involves the
bounds lj ≤ xj ≤ uj for j = 1 . . . n, a local branching constraint can be defined as
Δ(x, x¯) :=</p>
          <p>X μj (xj − lj) +
j:x¯j=lj</p>
          <p>X
j:x¯j=uj
μj (uj − xj) +</p>
          <p>X
j:lj&lt;x¯j&lt;uj
μj (xj+ + xj−) ≤ k.</p>
          <p>(3.6)
The weights are defined as μj = 1/(uj − lj), while xj+ and xj− define additional slack
variables that satisfy the equation
xj = x¯j + xj+ − xj−,
xj+ ≥ 0, xj− ≥ 0.</p>
          <p>Of course, there are other possibilities to improve the standard local branching algorithm
not proposed by Fischetti and Lodi. The following enhancements have been integrated in our
local branching framework as described in chapter 4.
• Fixing variables allows to tighten the neighborhood when the original local tree was too
large to be explored completely. Variables that share the same value in the incumbent
solution and in the solution of the LP relaxation are less likely to change in the global
optimum. By fixing some of those variables in the local tree and adding a corresponding
cut to the remaining tree local branching can avoid calculating parts of the tree that will
probably yield no better results. This approach was proposed by Danna et al. [8] and is
known as RINS (Relaxation Induced Neighborhood Search).
• Concurrent exploration of different local trees provides diversification by creating
several local trees from different feasible solutions and exploring them simultaneously.</p>
        </sec>
        <sec id="sec-2-1-4">
          <title>Chapter 4</title>
          <p>An Advanced Local Branching
Framework
In this chapter a generic framework for local branching is described. Standard local branching
is implemented as described in chapter 3, and several extensions are introduced to improve
its performance. The main goal of this framework is to provide a local search algorithm for
higher-level metaheuristics, for example evolutionary algorithms. These metaheuristics can
use local branching for exploring the neighborhood of certain solutions, and use the generated
feasible solutions as input for their own improvement algorithms. The actual implementation
of the framework will be described in chapter 6.
4.1</p>
          <p>Basic Functionality
A sequential version of the standard local branching algorithm provides the basis for the
framework. It is capable of using local branching to completely solve a problem without further user
intervention. The main phases of the local branching algorithm are:
1. Generate an initial solution for the first tree.
2. Initialize the first local tree using the previously generated solution.
3. Repeat:
(a) Completely solve the local tree.
(b) When the local search terminates:
• A better feasible solution was found in this local tree: create a new local tree
using the improved solution from this local tree as initial solution.</p>
          <p>• No better feasible solution was found: abort local branching.
4. Solve the rest of the search tree.
5. Return optimal solution.</p>
          <p>The initial solution can be created by a custom heuristic, or it is derived from the optimum
of the root node’s LP relaxation (e.g. by rounding or truncating the LP solution). The
default implementation uses local branching constraints as described in section 3, i.e. Hamming
distance constraints defining a neighborhood around the solution according to the distance
parameter k. Other constraints for branching might be implemented by the user as well.
The standard local branching algorithm works well for some instances, but has several
limitations:
1. Depending on the number of variables and the value of k, the Hamming distance
constraint possibly defines a very large neighborhood. Given a binary IP with n variables, a
feasible solution has ¡ n ¢ = (n−k)!k! neighbors with a Hamming distance of k (the local
n!
k
tree includes all neighbors with a Hamming distance not larger than k, so the actual
search tree is even larger).
2. Depending on the specific problem, there might be more than one reasonable initial
solution for local branching. When a given heuristic returns several promising solutions
that would create (partially) disjunct local trees (i.e. their Hamming distance is greater
than k), only one neighborhood can be explored.
3. While a local branching constraint defines a neighborhood around a feasible solution,
it provides no further guidance for exploring this neighborhood besides the standard
branch and cut strategies (e.g. best bound first search). Other local search heuristics
might help to tighten the search tree.</p>
          <p>Preliminary testing with multidimensional knapsack problems confirmed these
shortcomings. The test instances contain integer programming problems with 100 to 500 variables and
5 to 30 constraints. Detailed results will be discussed in chapter 9.</p>
          <p>The larger test instances (n ≥ 250, d ≥ 10) contain too many variables for using k
values larger than approximately 5. This allows at most five variables to flip their values,
and likely prohibits significant improvements to the initial solution of a local tree. For any
value k &gt; 5 even the local tree defines a subproblem that is often too complex to be solved
completely within the given time limit. Additionally, the first initial solution is either derived
by a first fit heuristic or by rounding the first LP solution, and both are unlikely to be in a small
neighborhood of the optimal solution.
4.2</p>
          <p>Extending the Basic Algorithm
In order to address the shortcomings described in the previous section, three extensions have
been added to the standard local branching algorithm. The first one eliminates the
restriction of sequential execution by allowing to create new local trees before the previous one(s)
are finished. On a related issue, the second extension allows to abort local trees before they
are completely solved. The last extension tries to reduce subproblem complexity by fixing
variables that are less likely to change in the optimal result than others.
4.2.1</p>
          <p>Using Multiple Local Trees
The first major extension to standard local branching is the support for pseudo-concurrent
exploration of several local trees. It removes the burden of relying on one local tree at a time,
95220
95210
lue 95200
vea 95190
tcv 95180
i
e
jlbo 95170
ifna 95160
95150
95140
[1]
[2]
[3]
[4]
75
(2) k = 5, variable fixing: 0.05, 0.2, 0.5, maximum nodes per tree: 250
(3) k = 20, no variable fixing, no node limit.
(4) k = 10, no variable fixing, maximum number of nodes: 1000</p>
          <p>All local branching configurations both showed faster convergence and better final
objective values. The plot of the objective value is given in figure 9.4.
9.4.2</p>
          <p>Cut Generation
The idea of cut generation is to find valid inequalities that are violated by the current LP
optimum as described in section 2.3, thus decreasing the LP optimum and increasing the chances
to prune a subproblem. The size of the search tree can be reduced significantly at the expense
of computationally expensive cut generation.</p>
          <p>In our test results, cut generation did not lead to a significant advantage for local branching
in comparison to the standard Branch and Cut algorithm, instead it compensated the advantage
of local branching. We used COIN/CGL’s generic cut generators, the most efficient proved to
be the knapsack cover cut generator. Others, like the Gomory cut generator, did not improve
the results but slowed down the computation.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec-3">
      <title>The following parameter configurations have been tested:</title>
      <p>(1) Standard Branch and Cut.
(3) k = 13, variable fixing: 0.1, 0.1, 0.8, maximum nodes per tree: 1000.</p>
      <p>Without cut generation, configuration (2) beat standard Branch and Cut by 27 : 10. With
cut generation, the standard algorithm reaches a tie result of 18 : 18 both against (2) and (3).
Regarding the online performance rating, local branching gains a slight advantage of 18 : 16
for configuration (2) and 19 : 15 for (3).
Creating multiple local trees in the beginning of the computation can help to improve the initial
performance of the local branching algorithm. The processor time is distributed over several
local trees, preferring those with better nodes (according to the tree search strategy, i.e. those
with better bounds.)</p>
      <p>As described in chapter 8, three different initial solutions were used:
• The feasible solution generated heuristically from the first LP result.
• A solution returned by a greedy heuristic using the relative weight as an efficiency
measure.
• A solution returned by the same heuristic including the weight distribution as an
efficiency measure.</p>
      <p>Compared to local branching with a single initial solution the results improved
considerably. For the mknapcb7 test instances, the three initial local trees contributed equally to the
best found solution, that is, the initial trees were of roughly the same size. For the mknapcb8
and mknapcb9 instances, the tree based on the first LP result was most often superior to the
trees based on efficiency measures, meaning that the latter two trees were often completed
after a few nodes. Apparently the greedy heuristics with efficiency values worked better for the
smaller mknapcb7 instances than for the more complex mknapcb8 and mknapcb9 instances.</p>
      <sec id="sec-3-1">
        <title>Mknapcb7: 100 variables, 30 constraints</title>
      </sec>
    </sec>
    <sec id="sec-4">
      <title>The following configurations have been tested:</title>
      <p>(1) Standard Branch and Cut.
(2) k = 13, variable fixing: 0.1, no node limit.
(3) k = 13, variable fixing: 0.1, 0.1, 0.8, maximum nodes per tree: 10000.</p>
      <p>Regarding the final objective values, both local branching configurations showed an 24 :
10 advantage to standard Branch and Cut. While this result is similar to what local branching
with a single initial solution achieved, the pseudo-concurrent tree exploration shows more
benefits when looking at the online performance rating. Both local branching configurations
achieved a clear advantage of 25 : 9 compared to local branching, which is considerably better
than the results using a single initial solution.</p>
      <sec id="sec-4-1">
        <title>Mknapcb8: 250 variables, 30 constraints</title>
        <p>As in section 9.4.1, the following configurations have been tested for mknapcb8:
(1) Standard Branch and Cut.
(2) k = 13, variable fixing: 0.1, no node limit
(3) k = 13, variable fixing: 0.1, 0.1, 0.5, maximum nodes per tree: 5000
Instance
mknapcb7
mknapcb8
mknapcb9
online performance
25 : 9
24 : 6
25 : 5</p>
      </sec>
    </sec>
    <sec id="sec-5">
      <title>Wilcoxon</title>
      <p>0.02%
0.02%
0.02%</p>
      <p>Comparing the final objective values, configuration (2) showed an advantage of 22 : 8
against standard Branch and Cut, (3) had an advantage of 21 : 9. Regarding online
performance, (2) showed a clear advantage of 24 : 6 and (3) an advantage of 26 : 4 compared to
standard Branch and Cut.</p>
      <sec id="sec-5-1">
        <title>Mknapcb9: 500 variables, 30 constraints</title>
      </sec>
    </sec>
    <sec id="sec-6">
      <title>The following configurations were tested:</title>
      <p>(1) Standard Branch and Cut.
(2) k = 10, variable fixing: 0.1, 0.1, 0.8, maximum nodes per tree: 5000.
(3) k = 13, variable fixing: 0.1, 0.1, 0.8, maximum nodes per tree: 1000.</p>
      <p>Both (2) and (3) showed clearly superior results to (1), with (2) showing a slight advantage
of 17 : 13 and (3) beating standard Branch and Cut by 22 : 8. The online performance ratings
clearly favor the local branching configurations: (2) beats (1) by 24 : 6, (3) beats (1) by
25 : 5. Compared with the same configuration without multiple initial solutions, (2) exhibited
an advantage of 19 : 11 for the final objective value and 27 : 3 for the online performance
rating.</p>
      <p>Table 9.5 summarizes the results for these test runs. The detailed results for all test
instances of mknapcb7, mknapcb8 and mknapcb9 are given in tables 9.6, 9.7, and 9.8. Bold
values indicate the best result for a single instance. Note that it is possible for more than one
configuration to achieve the “best” result.
9.5</p>
      <p>Long Runs
The test runs described in the last section are useful for testing the short-time heuristical
behavior a large variety of local branching configurations. Testing the instances of the OR library [2]
with longer running times (up to one hour) did not reveal significantly different behavior.
However, the very large eleventh instance of the second set of test instances [14] with 2500 variables
and 100 constraints was an interesting target for examining long-run behavior. The huge core
matrix dramatically slows down the LP solver, affecting the significance of the results of a 10
minute test run. Increasing the CPU time to 2 hours showed interesting results: local branching
extended its lead (with unmodified parameters), standard branch and cut was clearly inferior
to all tested configurations.</p>
      <p>Figure 9.5 shows the final objective plots for the following configurations:
0
757
(1) Standard Branch and Cut.
(2) k = 5, variable fixing: 0.05, 0.2, 0.5, maximum nodes per tree: 250
(3) k = 5, variable fixing: 0.05, 0.2, 0.5, maximum nodes per tree: 500
(4) k = 10, variable fixing: 0.05, 0.2, 0.5, maximum nodes per tree: 500
(5) k = 20, variable fixing: 0.05, 0.2, 0.5, maximum nodes per tree: 1000</p>
      <sec id="sec-6-1">
        <title>Chapter 10</title>
        <p>Summary and Outlook
This thesis described the implementation of a generic local branching framework based on
the open source COIN/BCP Branch, Cut and Price library. Local branching is a local search
heuristic that is well suited for integration in existing integer programming solvers. The
framework provides the possibility to augment COIN/BCP programs with local branching search
capabilities. Several extensions to the standard local branching algorithm were implemented:
pseudo-concurrent exploration of multiple local trees, aborting local trees, and search space
tightening through variable fixing.</p>
        <p>An encapsulated metaheuristic class offers means for a clean implementation of local
branching metaheuristics without touching COIN/BCP’s internals. Rich statistical data about
the current state of the local branching algorithm is provided by the framework. Methods for
creating new trees, terminating existing trees or modifying the local branching search
parameters are also provided.</p>
        <p>As a sample application, a Branch and Cut solver for the multidimensional knapsack
problem was used to demonstrate the application of the local branching framework and to research
the effects of local branching.</p>
        <p>The results for the multidimensional knapsack problem were promising: local branching
showed better convergence, especially in the early stages of the computation, and showed
significant benefits for large, complex test instances. By guiding the Branch and Cut solver
through neighborhood search and fixing of variables, local branching allows to find better
results earlier in the computation, which also leads to a reduction of the search tree complexity
in the later stages. However, the benefit for relatively small test instances was less clear.</p>
        <p>The local branching framework was designed in a way that facilitates embedding local
branching as a local search metaheuristic in another, higher-level search algorithm. This is the
main area where future work could be expected, to use the heuristical characteristics of local
branching to improve other search algorithms not based on Branch and Cut. In general, any
algorithm that involves some kind of local neighborhood search lends itself to the integration of
local branching. For example, an evolutionary algorithm could use the framework to generate
new, better solutions based on especially promising candidates.</p>
      </sec>
      <sec id="sec-6-2">
        <title>Appendix A</title>
        <p>The local branching framework requires some small patches to the COIN/BCP source. The
patches add the following functionality to COIN/BCP:
• Support for user-defined messages between LP and TM modules has been added.
• A special slot for the normal tree root node is provided in the candidate queue. This
is necessary because the normal root node must be used whenever a new local tree is
started.
• There is no way for the COIN/BCP user classes to catch all pruned nodes. Pruned nodes
are now added to a list that is available to the framework’s tree manager.</p>
        <p>All filenames in this section are relative to the COIN/BCP root directory.</p>
        <p>A.1</p>
        <p>Adding User-Defined Messages
In order to support user-defined messages between LP and TM modules, we have to add
an unique message tag for user messages and stubs for packing and unpacking routines.
We start by adding two new message tags to the BCP message tag enumeration in
include/BCP message tag.hpp (written in bold face):
(. . .)
/** The message contains the description of a variable. */
BCP Msg VarDescription, // VG / VP -&gt; LP
/** No more (improving) variables could be found. (Message body is</p>
        <p>empty.) */
BCP Msg NoMoreVars, // VG / VP -&gt; LP
BCP Msg UserMessageToLp,</p>
        <p>BCP Msg UserMessageToTm
};</p>
        <p>Then we add virtual method declarations of unpack user message() to BCP tm user and
BCP lp user by adding the following lines to the corresponding class definitions in
include/BCP lp user.hpp and include/BCP tm user.hpp:
virtual void
unpack user message(BCP lp prob&amp; prob, BCP buffer&amp; buf);</p>
        <p>We also provide a default implementation that throws an exception when called in
LP/BCP lp user.cpp and TM/BCP tm user.cpp:
void
BCP tm user::unpack user message(BCP tm prob&amp; prob, BCP buffer&amp; buf) {</p>
        <p>throw BCP fatal error(
”BCP tm user::unpack user message() invoked but not overridden!\n”);
}</p>
        <p>The implementation for BCP lp user is identical except for the class name. The last step
to be taken is to call these handlers from the tree manager and LP module message
processing functions. For the tree manager, we add the following block to the switch statement of
BCP tm prob::process message() in TM/BCP tm msgproc.cpp:
case BCP Msg UserMessageToTm:
user−&gt;unpack user message(*this, msg buf);
msg buf.clear();
break;
case BCP Msg UserMessageToLp:
user−&gt;unpack user message(*this, msg buf);
msg buf.clear();
break;</p>
        <p>Similarly, we add the following code to
BCP lp prob::process message() in LP/BCP lp msgproc.cpp:
the
switch
statement
of
A.2</p>
        <p>Extending the Candidate List
When local trees can be spawned before the previous tree terminated, the normal root node
(the sibling of the local tree root) has to be extracted from the candidate list. The easiest and
fastest way to achieve this goal is to store the normal root node in an extra variable and modify
the methods to insert and retrieve items.</p>
        <p>A.2.1</p>
        <p>include/BCP tm node.hpp
We start with modifying include/BCP tm node.hpp. We have to add two member variables to
BCP node queue which is used to store the candidate list.</p>
        <p>/** root node of the “normal” tree, to be used when all local trees</p>
        <p>are processed or a new tree should be opened */
BCP tm node* normal root node;
/** use normal root node instead of a candidate from the queue the</p>
        <p>next time top() is called */
bool use normal root node;</p>
        <p>Then we add a new parameter to BCP node queue::insert that permits to insert a normal
root node without using the extra slot. This is used when the normal root node is the last
remaining node and should be returned to the candidate list. By setting a default value, existing
calls to this function do not need to be modified.</p>
        <p>/** Insert a new node into the queue. */
void insert(BCP tm node* node, bool replace normal root node = true);</p>
        <p>We also slightly modify the inline functions empty() and top() to account for the normal
root variable.</p>
        <p>/** Return whether the queue is empty or not */
inline bool empty() const { return !normal root node &amp;&amp; pq.size() == 1; }
/** Return the top member of the queue */
BCP tm node* top() const {
return ((normal root node &amp;&amp; use normal root node) | |
(normal root node &amp;&amp; pq.size() == 1))
? normal root node : pq[1];
The last modification correctly initializes the new variables in the constructor.</p>
        <p>BCP node queue(BCP tm prob&amp; p): p(q), pq(),</p>
        <p>normal root node(0), use normal root node(false) { pq.push back(NULL); }</p>
        <sec id="sec-6-2-1">
          <title>A.2.2 include/BCP tm node.cpp</title>
          <p>We also have to change the implementations of the insert and and pop methods of the
BCP node queue class. Since we have to access user data objects, we have to include the
framework’s user data header. By using a compile-time flag for applications that use the local
branching framework, the COIN source remains usable for other applications.
#ifdef COIN LB</p>
          <p>#include ”LB user data.hpp”
void
BCP node queue::pop()
{</p>
          <p>The pop() method removes a node from the head of the priority queue. When the queue is
has only one element left, the normal root node is re-inserted to the list. If the normal root node
has been used (by setting use normal root node to true), it is deleted when pop() is called.
if (use normal root node &amp;&amp; normal root node) {
normal root node = 0;
return;
}
if (normal root node &amp;&amp; pq.size() &lt;= 2) {
// reinsert normal root node when the last element is popped
insert(normal root node, false);
normal root node = 0;
use normal root node = false;</p>
          <p>In the insert() method, we have to detect normal root nodes and store them in the extra
variable instead of the normal candidate list. By using the COIN LB flag again, the LB user data
cast does not conflict with other COIN/BCP applications.</p>
          <p>void
BCP node queue::insert(BCP tm node* node, bool replace normal root node)
{
#ifdef COIN LB
if (node−&gt;user data() &amp;&amp; replace normal root node) {
const LB user data* ud =</p>
          <p>dynamic cast&lt;const LB user data*&gt; (node−&gt;user data());
if (ud &amp;&amp; LB user data::UD NormalRoot == ud−&gt;type) {
normal root node = node;
return;
}
#endif</p>
          <p>TM/BCP tm functions.cpp
Another small modification is necessary in the static helper function BCP tm start one node().
When the normal root node should be returned, it is returned without further checking (e.g.
if it should be pruned). This way the tree manager can recognize when the normal root node
is pruned without further modifications (in this case, the node would be pruned by a LP
process). Also, when a node was pruned because of the global upper bound, it is added to the
pruned nodes list that is described in the next section.
p.ub() * (1 − p.param(BCP tm par::TerminationGap Relative)))
process this = false;
if (p.candidates.use normal root node) {
process this = true;
p.candidates.use normal root node = 0;
p.pruned nodes.push back(next node);
A.3
We start by adding a new public member to the BCP tm prob class. It is used to store nodes
that have been pruned. Since even pruned nodes are never deleted from memory, the tree
manager can access this list without further restrictions. The tree manager can also empty the
list when the nodes have been processed.</p>
          <p>/** Pruned nodes are stored in this list - may be cleared when no longer needed */
BCP vec&lt;BCP tm node*&gt; pruned nodes;</p>
          <p>There are two more places where nodes may be pruned inside the tree manager.
A.3.1</p>
          <p>TM/BCP tm msg node rec.cpp
Among other things, the method BCP tm unpack branching info() prunes child nodes
generated from a branching object when necessary. We add those nodes to our pruned nodes()
list.</p>
          <p>(. . .)
case BCP FathomChild:
child−&gt;status = BCP PrunedNode Discarded;
p.pruned nodes.push back(child);
break;</p>
          <p>TM/BCP tm msgproc.cpp
The tree manager also receives pruned nodes from LP processes. These nodes are also added
to pruned nodes.</p>
          <p>(. . .)
case BCP Msg NodeDescription Discarded:
case BCP Msg NodeDescription OverUB Pruned:
case BCP Msg NodeDescription Infeas Pruned:
node = BCP tm unpack node no branching info(*this, msg buf);
pruned nodes.push back(node);</p>
          <p>With these modifications, the tree manager is able to track the number of active nodes for
all local trees. It uses the local tree identification number stored in the user data of the pruned
nodes to update the node numbers of the corresponding local tree.
For aborting local trees, we store a set of local tree identification numbers in the candidate list.
In the tree manager method responsible for finding a new subproblem for a LP process, we
simply discard nodes that are in this set of terminated trees.</p>
        </sec>
        <sec id="sec-6-2-2">
          <title>A.4.1 include/BCP tm node.hpp</title>
          <p>We have to include two additional headers, again wrapped in a precompiler conditional.
#ifdef COIN LB
#include ”localtreeid.hpp”
#include &lt;set&gt;</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec-7">
      <title>Then we add a new public member to BCP node queue:</title>
      <p>#ifdef COIN LB
/** LocalTreeId values of trees to be terminated
(= to be pruned by BCP node queue::pop and BCP node queue::top) */
std::set&lt;LocalTreeId&gt; terminate ids;
#endif
A.4.2</p>
      <p>TM/BCP tm functions
In BCP tm start one, we modify the head of the main loop, the updates marked with bold face.
(. . .)
while (true){
if (p.candidates.empty()) return BCP NodeStart NoNode;
next node = p.candidates.top();
p.candidates.pop();
desc = next node−&gt; desc;
else
// if no UB yet or lb is lower than UB then go ahead</p>
      <sec id="sec-7-1">
        <title>Appendix B</title>
        <p>The results of chapter 9 were retrieved using test scripts written in Bash and Python code. The
printstats.py script expects a file containing the output of a set of test runs, usually covering
more than one instance and testing several configurations. The results are grouped by filename
and configuration, and miscellaneous statistical data can be extracted. For example, tables
containing the final objective values or the online performance rating. Additionally, plots of
the final objective value can be created. The gnuplot program is used to generate these cuts
which can be viewed on screen or written to a postscript file. Since the test logs are usually
rather large and take some seconds for processing, a simple interactive command line interface
was implemented to shorten user response times.</p>
        <p>B.1</p>
        <p>Generating Log Files
To simplify testing different configurations on many different files, a short Bash script is
available. The configurations to be tested are entered as an array, which is then applied to every
file supplied. Since the instances of the OR Library [2] contain 30 test instances per file, the
instance numbers to be tested can be specified in an array.</p>
        <p>#!/bin/bash
outfile=testall.log
rm $outfile
instances=”‘seq 0 29‘”
testcases=(”LB K 0” ”LB K 10 LB MaxNodes 5000” ”LB K 20”)
for file in $*
do
echo −e ”Processing” $file ”. . .\n”
for inst in $instances
do
date &gt;&gt; $outfile
echo −e ”Processing” $file ”, instance” $inst ”, params =” $opts ”. . .\n” &gt;&gt; $outfile
nice Linux−O/bcps $opts ${file}:${inst} &gt;&gt; $outfile
echo &gt;&gt; $outfile
done
done
done</p>
        <p>The script has to be executed from the main knapsack application directory and stores the
results in the file specified by outfile. The instance numbers are stored in instances (in this
case {0 . . . 29}), the configurations are stored in testcases. The test files are supplied on the
command line, possibly using wildcards.</p>
        <p>B.2</p>
        <p>Analyzing Log Files
The printstats.py script parses the log file given on the command line and offers a simple
line-based interactive interface to query the results. The most important commands are:
• help returns a list of all commands.
• help [command] returns a short description and possible parameters of the given
command.
• table [configuration]* prints a table containing all tested instances as rows and the given
configurations (or all, if none are supplied) as columns. The index numbers of the
configurations correspond with the log file and are also displayed below the table.
• columns [parameter] sets the displayed values. Possible parameters are:
– finalobjective: the final objective value.
– finalobjective delta: the final objective value, and the number of processed nodes
relative to the best configuration in a row (when two configurations found the same
result.)
– onlineperformance: the online performance rating.
– localtrees: the number of (created) local trees.
– localtime: time spent in local branching relative to the total computation time.
– finalbinary: a binary comparison function for the final objective value, useful for
executing Wilcoxon rank sum tests.
• showfilename [true/false] enables or disables the file name column in the table view.
• plot [filename] [configurations]* executes gnuplot to plot the final objective values of
the given configurations (or all if none are given).
• outputformat [screen/postscript] sets the output format of the plots generated by the plot
commands. screen uses gnuplot to display the diagram on the screen, postscript writes
the output to a postscript file.
• outputdir [directory] sets the directory where the postscript files are stored (default:
current working directory.)
[1] E. Balas, S. Ceria, G. Cornue´jols, and N. Natraj. Gomory cuts revisited. Operations</p>
        <p>Research Letters, 19:1–9, 1996.
[2] J. E. Beasley. Operation research library.</p>
        <p>http://www.brunel.ac.uk/depts/ma/research/jeb/info.html.
[3] D. Bertsimas and R. Demir. An approximate dynamic programming approach to
multidimensional knapsack problems. Management Science, 48(4):550–565, 2002.
[4] A. Caprara, H. Kellerer, U. Pferschy, and D. Pisinger. Approximation algorithms for
knapsack problems with cardinality constraints. European Journal of Operational
Research, 123:333–345, 2000.
[5] S. Ceria, G. Cornuejols, and M. Dawande. Combining and strengthening gomory cuts.</p>
        <p>In E. Balas and J. Clausen, editors, Integer Programming and Combinatorial
Optimization: Proc. of the 4th International IPCO Conference, pages 438–451. Springer, Berlin,
Heidelberg, 1995.
[6] P. C. Chu and J. E. Beasley. A genetic algorithm for the multidimensional knapsack
problem. Journal of Heuristics, 4(1):63–86, 1998.
[7] V. Chva´tal. Edmonds polytopes and a hierarchy of combinatorial problems. Discrete</p>
        <p>Mathematics, (4):305–337, 1973.
[8] E. Danna, E. Rothberg, and C. L. Pape. Exploring relaxation induced neighborhoods to
improve mip solutions. Mathematical Programming, 2004.
[9] L. Davis. A genetic algorithm tutorial. In Handbook of Genetic Algorithms, pages 1–101,</p>
        <p>New York, 1991.
[10] F. Eisenbrand. On the chva´tal rank of polytopes in the 0/1 cube. Discrete Applied
Mathematics, 98:21–27, 1999.
[11] F. Eisenbrand. Gomory-Chva´tal cutting planes and the elementary closure of polyhedra.</p>
        <p>PhD thesis, 2000.
[12] M. Eso¨, L. Lada´nyi, T. K. Ralphs, and L. Trotter. Fully parallel generic
branch-andcut. Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientific
Computing, 1997.
[13] M. Fischetti and A. Lodi. Local branching. Mathematical Programming, 98:23–47,
2002.
[14] H. C. for Enterprise Science. Benchmarks for the multiple knapsack problem.</p>
        <p>http://hces.bus.olemiss.edu/tools.html.
[15] R. E. Gomory. Outline of an algorithm for integer solutions to linear programs. Bulleting
of the American Mathematical Society, (64):275–278, 1958.
[16] R. E. Gomory. An algorithm for integer solutions to linear programs. Recent Advances
in Mathematical Programming, pages 269–302, 1963.
[17] J. Gottlieb. Permutation-based evolutionary algorithms for multidimensional knapsack
problems. Proceedings of 2000 ACM Symposium on Applied Computing, 2000.
[18] R. Hinterding. Mapping, order-independent genes and the knapsack problem.
Proceedings of the 1st IEEE International Conference on Evolutionary Computation, pages 13–
17, 1994.
[19] O. H. Ibarra and C. E. Kim. Fast approximation algorithms for the knapsack and sum of
subset problems. J. ACM, 22(4):463–468, 1975.
[20] N. Karmarkar. A new polynomial-time algorithm for linear programming.
Combinatorica, 4:373–395, 1984.
[21] H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems. Springer, 2004.
[22] L. G. Khachian. A polynomial algorithm for linear programming. Doklady Akad. Nauk</p>
        <p>USSR, 224:1093–1096, 1979.
[23] P. Kolesar. A branch and bound algorithm for the knapsack problem. Management</p>
        <p>Science, 13:723–735, 1967.
[24] B. Korte and R. Schrader. On the existence of fast approximation schemes. In O. L.</p>
        <p>Mangasarian, R. R. Meyer, and S. Robinson, editors, Nonlinear Programming 4, pages
415–437. Academic Press, 1981.
[25] A. H. Land and A. G. Doig. An automatic method for solving discrete programming
problems. Econometrica, 28:497–520, 1960.
[26] E. K. Lee and J. E. Mitchell. Branch-and-bound methods for integer programming. In
Encyclopedia of Optimization, volume 2, pages 509–519. Kluwer Academic Publishers,
2001.
[27] J. S. Lee and M. Guignard. An approximate algorithm for multidimensional zero-one
knapsack problems. Management Science, 34(3):402–410, 1988.
[28] R. Lougee-Heimer. The common optimization interface for operations research. IBM</p>
        <p>Journal of Research and Development, 47:57–66, 2003.
[29] J. E. Mitchell. Branch-and-cut algorithms for integer programming. In Encyclopedia of</p>
        <p>Optimization, volume 2, pages 519–525. Kluwer Academic Publishers, 2001.
[30] J. E. Mitchell. Cutting plane algorithms for integer programming. In Encyclopedia of
Optimization, volume 2, pages 525–533. Kluwer Academic Publishers, 2001.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          [31]
          <string-name>
            <given-names>M.</given-names>
            <surname>Padberg</surname>
          </string-name>
          and
          <string-name>
            <given-names>G.</given-names>
            <surname>Rinaldi</surname>
          </string-name>
          .
          <article-title>A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems</article-title>
          .
          <source>SIAM Rev</source>
          .,
          <volume>33</volume>
          (
          <issue>1</issue>
          ):
          <fpage>60</fpage>
          -
          <lpage>100</lpage>
          ,
          <year>1991</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          [32]
          <string-name>
            <given-names>D.</given-names>
            <surname>Pisinger</surname>
          </string-name>
          .
          <article-title>Algorithms for Knapsack Problems</article-title>
          .
          <source>PhD thesis</source>
          , University of Copenhagen, Dept. of Computer Science, Feb.
          <year>1995</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          [33]
          <string-name>
            <given-names>G.</given-names>
            <surname>Raidl</surname>
          </string-name>
          .
          <article-title>An improved genetic algorithm for the multiconstrainted 0-1 knapsack problem</article-title>
          .
          <source>Proceedings of the 5th IEEE International Conference on Evolutionary Computation</source>
          , pages
          <fpage>207</fpage>
          -
          <lpage>211</lpage>
          ,
          <year>1998</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          [34]
          <string-name>
            <given-names>G. R.</given-names>
            <surname>Raidl</surname>
          </string-name>
          .
          <article-title>Weight-codings in a genetic algorithm for the multiconstraint knapsack problem</article-title>
          .
          <source>Proceedings of the 1999 IEEE Congress on Evolutionary Computation</source>
          , pages
          <fpage>596</fpage>
          -
          <lpage>603</lpage>
          ,
          <year>1999</year>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>