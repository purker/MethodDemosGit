<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Coloring of the Self-Organising Maps based on class labels</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>unter Anleitung von</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Taha Abdel Aziz</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Ao. Univ. Prof. Andreas Rauber</institution>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>Quellenstrasse 24B/13/13 A 1100 Wien</institution>
        </aff>
      </contrib-group>
      <pub-date>
        <year>2003</year>
      </pub-date>
      <fpage>32</fpage>
      <lpage>73</lpage>
    </article-meta>
  </front>
  <body>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_1_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_16_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_20_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_28_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_29_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_30_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_31_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_32_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_33_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_34_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_35_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_36_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-247743.images\img_37_1.png" />
    </fig>
    <sec id="sec-1">
      <title>Unterschrift i</title>
    </sec>
    <sec id="sec-2">
      <title>Kurzfassung</title>
      <p>Die Selbstorganisierende Karte ( SOM) ist ein n tzliches und starkes Werkzeug
f r die Datenanalyse, besonders f r gro e Datens tze oder Datens tze von hoher
Dimensionalit t. SOM Visualisierungen bilden die Dimensionen des
Datenmodells auf graphische Dimensionen wie Farbe und Position ab, so helfen sie der
Navigation und dem Erforschen von dem SOM. SOM Visualisierungen k nnen
auch die Daten selbst einbeziehen, so dass der Zugri auf Informationen m glich
wird, die in einem reinen SOM nicht verf gbar sind. Dadurch wird ein tieferer
Einblick in die Daten m glich. Wenn die Daten mit klassen gekennzeichnet sind,
k nnen diese Klassen auch in der Visualisierung einbezogen werden, so dass, eine
klarere Idee ber die Klassinformation gewonnen wird. In dieser Arbeit schlagen
wir eine neuartige SOM Visualisierungsmethode, n mlich die SOM Klassenf
rbung vor, welche auf den Datenklassen beruht. Diese Methode ndet eine farbige
Partition des SOM-Gitters, die die Klassenstruktur widerspiegelt. Diese
Visualisierung erm glicht das entdecken von Klassinformation wie Klassenstruktur,
Klassenverteilung und Klassenclusters. Au erdem k nnen neue Daten Klassen
zugeordnet werden und zwar indem der Punkt auf dem SOM-Gitter ermittelt
wird, welcher das neue Datum (Messwert) am besten repr sentiert; das neue
Datum wird dann jener Klasse zugeordnet, die die Partition repr sentiert, auf der
sich der Punkt be ndet.</p>
      <p>The Self-Organizing Map (SOM) is a useful and strong tool for data analysis,
especially for large data sets or data sets of high dimensionality. SOM
visualizations map the data model dimensions to visual dimensions like color and position,
thus they help exploring the SOM. Visualization can also involve the data itself
so that it helps accessing information that are not available in the trained SOM,
thereby enabling a deeper look inside the data. If the data comes with supervised
class labels, these labels can be also involved in the visualization, thus enabling
the user to have a clearer idea about the data and the structures learned by the
SOM. In this work we propose a novel SOM visualization method, namely the
SOM class coloring, which is based on the data class labels. This method nds a
colored partitioning of the SOM lattice, that re ects the class distribution. SOM
class coloring helps discovering class information such as class topology, class
clusters, and class distribution. Furthermore class labels can be assigned to new
data items by estimating the point on the lattice, that best represents the data
item and then assigning the class of the partition that includes this point to the
data item.
1 Introduction
1
3.4.4 Smooth partitioning of Voronoi cells . . . . . . . . . . . . 39
3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4 Experements and evaluation 51
4.1 Iris data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.2 Text data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.3 Audio data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5 Conclusions
List of Figures
2.1
2.2
2.3
2.4
2.5
2.6</p>
      <p>Adapting the best-matching unit and its neighbors weights The input vector
coordinates are marked with a cross, coordinates of the map nodes before
modication are shown as full circles and after modication as empty circles.</p>
      <p>Agglomerative algorithm: (A) steps, (B) corresponding dendrogram . . . . .
U-Matrix, PMatrix, and U*Matrix [Ult05]. (A)Dataset consisting of a mixture
of two Gaussians with 2048 points each. (B)The U-Matrix of the dataset,
darker colors correspond to large distances. (C)The P-Matrix of the dataset,
darker colors correspond to larger dinsities. (D)The U*-Matrix of the dataset,
which shows clearly the two Gaussians. . . . . . . . . . . . . . . . . . .
Hit Histogram visualization methods [Ves99] (a)The size of the black hexagon
is proportional to the value of the histogram in the corresponding unit. (b)The
height of the bar tells the same thing as in (a). . . . . . . . . . . . . . .
SDH visualization with dierent values of the smoothing parameter s[PRM02].
The Data set consists of 5000 samples, that are randomly drawn from a
probability distribution, that is a mixture of 5 Gaussians. The SOM is of 10X10
units arranged on a rectangular grid. . . . . . . . . . . . . . . . . . . .
Gradient visualization with parameters:(a) = 1, (b) = 5, (c) = 15 [PDR]
2.7
2.8
3.1
3.2
3.3
3.4
3.5</p>
      <p>Gabriel Graph Visualization [Aup03] (a)Notation for the three data
qualities dened in this method, from top to bottom: border, isolated, normal.
(b)Voronoi diagram (thin lines) and Delaunay triangulation (bold lines) of
the data (circles). (c)Gabriel graph (bold lines) of the data. (G2-G5)Number
of nodes x ( edges y) in (between) corresponding connected components of G2
and G4, are indicatd beside nodes (edges) of G3 and G5. The class graph G3
shows the topology of the classes and the way they are connected and the
density of the connections between the dierent components, where two classes
are drawn with their densities and topology; the class and quality graph G5
provides the same of information as in G3 in addition to the visualization of
border and isolated components. . . . . . . . . . . . . . . . . . . . . .
Pie chart visualization . . . . . . . . . . . . . . . . . . . . . . . . . .
SOM Toolbox overview An illustration of some features of the SOM Toolbox
with banksearch data (A)The SOM lattice with units represented as pie charts
(B)Zoomed details view with labels and pie charts (C)Pie chart representing
the unit position and the class contribution (D)Smoothed data histogram
visualization as an example visualization (E)DMatrix visualization as another
example (F)There are other visualizations that can be selected from the
visualization menu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Base conguration of the SOM Class Coloring (A)The SOM units located on
the grid with their class contributions, that is given by function f. (B)Representation
of A as it is in SOM Toolbox, where f is represented with pie chart. . . . . .
Coloring by Simple Flooding assuming one class per unit. (A)Colored points
representing the dominant classes of the SOM units, (B to D)The process of
color ooding animated in 3 steps. . . . . . . . . . . . . . . . . . . . .
Unit substitution.(A) Multi class units represented by pie charts,(B)Substitution
points that can be used by color ooding. . . . . . . . . . . . . . . . . .
Instability eect in the color ooding. (A)While growing, the two blue points
close the way and prevent the red point from growing to the left side. (B)After
making a slight change in the position of one of the blue points, the red point
can grow to the left side, which changes the end result dramatically. . . . .
27
29
30
30
3.6 Areas in Voronoi diagram (A) are similar to these produced by color ooding
(B). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7 Relation between Voronoi diagram and Delaunay triangulation(DT). (A)a set
of point (sites) in a 2-D space(B). (B)Voronoi faces (thin lines) and Delaunay
triangulation (thick lines) of the sites in A. (C)The circumsphere of every
triangle in DT contains no sites inside. . . . . . . . . . . . . . . . . . .
3.8 Sweepline algorithm. The slides 1-9 animate the algorithm while nding the
Voronoi diagram of the three sites in slide 1. In slide 2 a parabola for the
upper site arises because the sweepline passes this site. In the next two slides
the parabola grows while the sweepline moves. In slide 5 two parabolas for
the two lower points arise because the sweepline passes these points. In the
rest slides all parabolas grow while the line sweeping. The intersections of the
parabolas form the edges of the Voronoi diagram as in slide 9 (red lines). . .
3.9 Bowyer-Watson algorithm when adding new site. (A)The faces to be deleted
are all faces whose circumsphere or itself contains the new added site. (B)Faces
to delete are colored, and new faces are marked in dashed lines. (C)The
resulting Delaunay triangulation after insertion and update. . . . . . . . .
3.10 Region substitution. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.11 Sector partitioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.12 (A)Notations used int this section(Note that we use the colors as class names
for simplifying). (B)Connection lines between regions that have same classes.
3.13 The work of attractor function: In region r1, at rst the 34 grids, that are
most close to L1, are colored in red then the next nearest 45 grids are colored
in yellow. The 41 grids most nearest to L5 are colored in blue, also 14 grids
from r4 that are nearest to L5 from the other side, are colored in blue. that
gives the eect of an area extending over two regions In regions r2 and r3
the same thing is done with L2 and L3, because L2 and L3 have a common
point the resulting colored area appears as a thin area, that extends over two
regions In r4 the line L4 has the length 0 (is equivalent to a point), thus the
resulting colored area has a circular shape. . . . . . . . . . . . . . . . .
3.14 Distance between a point and a line segment[Sof]. . . . . . . . . . . . . .
vii
32
33
35
36
38
39
40</p>
      <p>Class coloring of the iris data. The areas marked with circles are examples of
regions, where it is dicult to assign new data without using the coloring. . .
viii
46
48
49
50
ix
4.2 Class visualization overview of the banksearch data set. (A)Pie chart
visualization. (B)Smoothed class coloring of the same data set. For the categories
and themes see Table 4.1. . . . . . . . . . . . . . . . . . . . . . . . . . 60
4.3 Class Coloring with dierent values of the parameter minimum visible class.
(A)the parameter is set to 100%. (B)the parameter is set to 50%. (C)the
parameter is set to 0%. . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.4 Class coloring with Voronoi cell border. (A) to (F) are signicant samples
showing some strengths and drawbacks of the visualization. . . . . . . . . . 62
4.5 Chessboard visualization of the banksearch dataset. . . . . . . . . . . . . 63
4.6 Radio station data set. (A) Pie chart visualization. (B) Class coloring of the
data set. The text labels describe the main categories of the genres. See [LR06]. 64
4.7 Class coloring of the radio search data set. The parameter minimum
visible class is set to dierent values (A) 0% (B) 30% (C) 60% (D)100%. The
dominant classes in (C) do not agree with these seen by the human eye in (A). 65
4.8 Class coloring of the radio search data set after the correction of the algorithm.</p>
      <p>The parameter minimum visible class is set to dierent values (A) 0% (B) 30%
(C) 60% (D)100%. The dominant classes in (C) agree with these seen by the
human eye in (A). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.9 Class coloring of the banksearch data set after the correction of the algorithm.</p>
      <p>Some of the drawbacks that was found in the least section have disappeared
after the correction. For example the area marked with a circle is to be
compared with Figure 4.4-(C). . . . . . . . . . . . . . . . . . . . . . . 67
4.10 Chessboard visualization of the radio search data. . . . . . . . . . . . . . 68</p>
      <sec id="sec-2-1">
        <title>Chapter 1</title>
      </sec>
      <sec id="sec-2-2">
        <title>Introduction</title>
        <p>Analyzing data usually needs more than getting pure statistical properties of the
data set. There are many methods for quickly producing overall summaries of a
data set. For example, ve-number summary consisting of some statistical values
(greatest, median, lower quartile and upper quartile) provides a help
understanding simple data sets of low dimensionality and limited volume [Kas97]. Data
structure and topology such as nding the clusters and class distribution of the
data is very essential. The more data there is available the more di cult it is to
understand this data set. Also, the dimensionality of the feature space
representing the data is a factor. For this problem there is an essential need of methods
for data exploration and especially the methods, that can discover and illustrate
e ectively the structure of the data. Data mining is one of the elds where such
exploratory methods are needed in the form of tools in Knowledge discovery in
database (KDD), whose purpose is to nd new knowledge from databases where
dimensionality, complexity, or amount of data is too large or complex for pure
human observation to be studied. Data mining is now an important area of
research, responding to the presence of large databases in commerce, industry, and
research. Methods of data exploration vary depending on the nature of the data
and the goal of study: clustering methods are the more conventional ones; they
aim to organize the data patterns into groups, so that patterns in one group are
more similar to each other than to those in another group. In other words
clustering tries to reduce the amount of data items by grouping them. Clustering is
useful in several areas such as pattern analysis, grouping, and machine-learning,
document retrieval, image segmentation, and many other applications [AJ99].
Projection methods try to reduce the dimensionality of the data: they map the
multidimensional data features to low dimensional space (usually 2D or 3D) that
represents the data and can be easier visualized and studied. Of course this
mapping should preserve the original data topology as much as possible. If the goal of
projection is to visualize the data, then low output dimensionality should be
chosen in order to achieve meaningful visualizations. Visualization methods aim to
map the dimensions of the data to visual dimensions such as position, color, etc.
in a way that the observer can get a deeper insight in the structure of the data.
Of course the visual dimensions are limited, so if the data is of high
dimensionality, direct visualization will be di cult, not helpful or impossible [Ves99]. In such
a case visualization should make use of the two previous methods (Clustering
and projection) as a pre-processing step to prepare the data in a form that it
can be e ciently visualized. Self-Organizing Maps (SOMs) [Koh97] are e cient
tools that implicitly combine all of the previous methods. A SOM is a neural
network consisting of low dimensional grid (mostly 2D) of nodes that are trained
with high-dimensional data. The nodes on the trained map represent the original
data in a manner that similar data points in the feature space are mapped to
nodes which are close to each other on the map. SOMs are strong tools used
in the data exploration. Some properties that distinguish SOM from the other
data mining tools are that it is numerical instead of symbolic, nonparametric, and
capable of learning without supervision [Kas97]; Many variants and extensions
of the standard SOM were proposed which try to cover some drawbacks of the
standard SOM. In practical applications the process of deciding which method
to use is essential: this depends on the nature of the data and the goal of
exploration; the following questions are in any case to be resolved: What kind of
structure the method can extract?; How does it illustrate this structure?; Does
the method reduce the data dimensionality and/or the number of data items?;
Which and if so which type of data pre-processing is necessary?. The focus of
this work is visualization of the SOM, in particular SOMs that are trained with
labeled data. It is meaningful here to distinguish between two things that are of
great importance in this work: SOM is based on an unsupervised learning process
in the meaning that no prior class information is needed to perform the training,
because the learning is data-driven. Regardless of this fact, class information
can be used in the form of labels which can be then used for exploration and
visualization purposes; such labels can be collected in a pre-processing step, or
even as a post-processing step. These labels do not a ect the training process
and the resulting structure or topology, but they help in generating visualizations
displayed on top of the nished map. In particular we propose in this work a
visualization method of SOM which is based on such class labels namely a SOM
coloring that depends on these labels.</p>
        <p>The remainder of this thesis is structured as follows: In Chapter 2 we will
discuss some related concepts and methods such as the concept of the Self-Organizing
Map, some clustering and classi cation methods and methods for projection and
visualization of the trained SOM. In Chapter 3 we present our novel method for
coloring the SOM based on class labels. In Chapter 4 we test and evaluate our
method with some collected data sets. Finally we present our conclusions in
Chapter 5.</p>
      </sec>
      <sec id="sec-2-3">
        <title>Chapter 2</title>
      </sec>
      <sec id="sec-2-4">
        <title>Related Work</title>
        <p>In this chapter we will introduce some of the concepts and worksljub me that
have been proposed which are closely related with the underlying issue, coloring
of the SOM. In Section 2.1 the general issue of the SOM will be introduced.
In Section 2.2 and 2.3 we will give some idea about the two processes, which
are closely related to visualization: these are clustering and projection. In
Section 2.4 visualization will be discussed brie y especially in respect to the SOM,
Some SOM-based clustering, projection and visualization methods will be also
discussed.
2.1</p>
        <sec id="sec-2-4-1">
          <title>Self organizing map</title>
          <p>The Self-Organizing Map (SOM) in the basic form is an arti cial neural network
model. The nodes of this network are trained to various input patterns from
the feature space. SOMs use an unsupervised learning process: no a priori
classi cation for the input is needed. The result of the learning process is a map
lattice (in this work we assume a 2 dimensional lattice, which is usually the case
), that map the high-dimensional feature space of the input in a way, so that
similar inputs in the feature space have associated nodes that are close to each
other on the map lattice, thus reducing the high-dimensionality of the feature
space. This projection is capable (a) to help clustering the data and (b) to
approximately preserve the original data topology of the input. SOMs are therefore
especially useful for data visualization and exploration purposes. I a classic SOM,
each neuron represents an n-dimensional column vector , where n depends on the
data space dimensionality (input vectors dimensionality). The reason for using
one- and two-dimensional grids is that space structures of higher dimensionality
cause problems with data display. Neurons are usually located in the nodes of
the two-dimensional grid with rectangular or hexagonal cells. The number of
neurons in the lattice determines the resulting resolution and the granularity of
data presentation.</p>
          <p>During the training not only the winning neuron is modi ed but its neighbors
as well, although the strength of the adaption may depend on their distance from
the winner neuron. This allows considering the SOM to be a method of projecting
the data nonlinearly onto a lower-dimensional display. When using this algorithm
the vectors similar in the initial space get close to each other on the nal map
SOMs [Koh97].
2.1.1</p>
          <p>The map initialization
If the map contains tens of thousands of neurons then it usually takes too much
time to train the system for the practical tasks. Thus making choice of the nodes
quantity requires a reasonable trade-o . Before training the map it is necessary
to initialize the weight coe cients of the neurons. The learning rate factor can
be signi cantly increased by an appropriate initialization method, thus bringing
better results. In general, there are three methods of weights initialization:
Initialization with random values. All the weights are assigned small
random values.</p>
          <p>Initialization with patterns. Initial values are set to randomly chosen
patterns of the training sample.</p>
          <p>PCA-based initialization. The weights are initialized by values of the
vectors ordered along a two-dimensional subspace spanned by the two principal
eigenvectors of the input data vectors.
The learning process consists of sequential corrections of the vectors representing
neurons. On every step of the learning process a random vector is chosen from
the initial data set and then the best-matching neuron coe cient vector (up now
BMU or best-matching unit ) is identi ed. This is the most similar unit to the
input vector. The word ’similarity’ in this task means the distance between the
vectors. There are various distance metrics that can be used for this purpose.
The most common one is the Euclidean metric, where the the winner unit must
meet the following relation:
kX</p>
          <p>Wck = minikX</p>
          <p>Wik
(2.1)
Where Wc is the winner vector at index c, X is the input vector and Wi is any
Unit vector in the SOM. After the BMU is found the neural network weights are
adapted. The winning unit and its neighbors adapt to represent the input by
modifying their reference vectors towards the current input. Figure 2.1 shows
how this works with a 2D vector. The weight modi cation is de ned by the
following formula:</p>
          <p>Wi(t + 1) = Wi(t) + hci(t) [X(t)
where t is the discrete time index. Vector X(t) is a randomly selected sample at
iteration t. The function h(t) is the neighborhood function:
h(t) = h(krc
rik; t)
(t)
(2.3)
It represents a decreasing function of time and distance between the winning
neuron and its neighbors on the grid. The function consists of two parts: the
distance function and the learning rate function of time. where r determines
the position of the neuron on the 2D grid. A simple version for h is the
boxneighborhood. Usually one of two functions of distance is used, simple constant:
or Gaussian function:
Where (t) is a diminishing function of time. This value is often called the radius
of the neighborhood. At the beginning of the learning procedure it is fairly large,
but it is made to gradually shrink during learning. Towards the end a single
winning neuron is trained. Most frequently the linear decreasing function of time
is used.</p>
          <p>Let us proceed to the learning rate function (t). This is also a decreasing
function of time. A variant of this function are linear and inversely proportional
of time with the formula:</p>
          <p>(t) = t +AB (2.6)
where A and B are constants. There are two main phases in the learning
process. At the beginning the learning rate and the neighborhood radius are fairly
large, which allows ordering the neurons vectors according to the sample patterns
distribution. Then the weights are accurately adjusted with the learning rate
parameters much smaller than they initially were. In case of using PCA-based
initialization the rst step of rough adjustment can be skipped.</p>
          <p>The previous training algorithm, which was described formally is now
summarized in the following steps:
1. Initialize the weights of each node .
(2.4)
(2.5)
2. Chose a sample vector randomly from the set of training data.
3. Every node is examined to nd the best-matching unit .
4. The radius of the neighborhood of the BMU is now calculated. This is
a value that starts large, typically set to the ’radius’ of the lattice, but
diminishes each time-step. Any nodes found within this radius are deemed
to be inside the BMU’s neighborhood.
5. Each of neighboring node’s weights are adjusted to make them more like
the input vector. The closer a node is to the BMU, the more its weights
get altered.</p>
          <p>6. Iterate from step 2 for N iterations.
2.2</p>
        </sec>
        <sec id="sec-2-4-2">
          <title>Clustering and classication</title>
          <p>Cluster analysis is the organization of a collection of patterns into clusters based
on similarity among these patterns. That is patterns belonging to a cluster
are (more) similar to each other to than those in another clusters. There are
many clustering methods, however it is useful at this point to distinguish between
supervised and unsupervised methods: in the case of supervised classi cation
(discriminant analysis) a collection of already classi ed patterns is provided and
the problem is to classify a new pattern. In the case of unsupervised classi cation
(clustering) a collection of unclassi ed pattern is provided and the problem is to
group this collection into meaningful clusters, but this process of labeling is data
driven and not as according to a given class structure as in the case of supervised
classi cation. The clustering process typically consist of the following steps:
Pattern representation, pattern measure de nition, grouping, data abstraction.
1. Pattern representation: this step involves preparing the input data and
nding important information such as the number of available patterns
and the number, type, and scale of the features involved in the clustering.
Furthermore a subset of features may be selected to be the most important
subset of the original feature set. This process is called feature selection.
If the features are in form that is not suitable for clustering, one or more
transformation could be used to put the features in an appropriate form to
start clustering.
2. Pattern proximity measure de nition: this is the distance function, that
is applied on pairs of patterns to measure the similarity between the two
patterns. There is a variety of distance measure functions. The simplest
one may be the Euclidean distance.
3. Grouping: this is the process of partitioning of the date into groups. The
partitions can be hard, where a pattern can belong to only one partition
and it can be fuzzy, where a pattern can have membership degrees in many
clusters. Furthermore there are two conceptual clustering algorithms:
hierarchical algorithms, that produce many nested partitions based on
similarity according to a criterion for merging or splitting clusters and partitive
clustering algorithms, that identify the partition that optimizes a clustering
criterion. Clustering algorithms will be discussed in the next sub sections
in more details.
4. Data abstraction: is the process of extracting a simple and compact
representation of a data set. In the clustering context, a typical data abstraction
is a compact description of each cluster, usually in terms of cluster
prototypes.</p>
          <p>To have optimal clustering, there are many parameters to be tuned. This depends
also on the purpose of the clustering. We note, that the purpose is usually not
to nd an optimal clustering of the data, but to have a clustering that enables
an optimal and e cient exploring the data considering speed, robustness and the
ability of visualization [AJ99].
2.2.1</p>
          <p>Hierarchical algorithms
Hierarchical clustering approaches can be divided into agglomerative and
divisive algorithms, corresponding to bottom-up and top-down strategies, to build a
hierarchical clustering tree. Agglomerative algorithms are more commonly used
than the divisive methods and usually have the following steps:</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec-3">
      <title>1. Initialize: Assign each vector to its own cluster.</title>
      <p>2. Compute distances between all clusters.
3. Merge the two clusters that are closest to each other.</p>
      <p>4. Return to step 2 until there is only one cluster left.</p>
      <p>That means, that the algorithm begins with each element in its own cluster and
the clusters are merged together iteratively according to the similarity between
them. The process continue until all elements are in one cluster, see Figure 2.2-A.
A tree structure is provided as a result of this algorithm, which can represented
with a tree diagram called dendrogram, depicted in Figure 2.2 -B. The
dendrodiagram can be cut in any level providing a di erent clustering [VA00].
2.2.2</p>
      <p>Partitive algorithms
In this approach the algorithm identi es the partition by minimizing some error
function. The number of clusters is either already prede ned or determined by the
algorithm by trying various numbers of clusters. In general partitive algorithms
consist of the following steps:
SOM coloring we can directly see the class topology of the data with sharp class
cluster boundaries. In fact the main class clusters can also be seen in the pie
chart visualization, but the absence of exact borders make it di cult to decide
for data points that are close to the cluster borders and those points that lie near
outliers or noise nodes. See the regions marked with circles in Figure 4.1.</p>
      <p>The main bene t of the coloring in case of such simple data is the ability to
label new data: after we obtain such a coloring from a labeled data set, we can
assign a new unlabeled data by nding the best matching unit for this data and
then nd in which colored area this unit is located. Note that the best matching
unit can be eventually a new node, if so, we can see the bene t in comparison
with the pure pie chart visualization, where we must estimate and choose one of
the nodes in the neighborhood to assign the class.
4.2</p>
      <sec id="sec-3-1">
        <title>Text data</title>
        <p>In this section we will use the Banksearch data set described in [SC02] to test
and evaluate our visualization method. Banksearch data set was designed as a
standard data set for general use in web document clustering and similar
experiments and in particular for research in the eld of unsupervised clustering of web
documents, so that clustering methods can be tested with a common data set,
and thus be easily and objectively compared. It was aimed to use a relatively
large number of categories, but with su cient of each category to allow a wide
variety of sensible experiments, each using clearly de ned subsets of the data set.
The banksearch data set consists of 4 main categories (Banking &amp; Finance,
Programming languages, Science and Sport), which are further divided into a total
of 11 subcategories (themes). These Categories, their themes and identi cation
Ids are listed in Table 4.1. For each one of the 11 themes, the data set contains
1000 documents,which means a total number of 11000 documents.</p>
        <p>A SOM with the following con guration was trained with this data set: 35
X 25 SOM grid, 10000 iterations, 0.7 learn rate, random seed of the value 7, no
normalization was performed on the data. Figure 4.2 shows an overview of the
class visualization of the trained SOM: (A) is the pie chart visualization and (B)
is the smoothed class coloring of the trained SOM.</p>
        <p>by comparing the pie chart visualization with the smoothed coloring in
Figure 4.2 we see that the class clusters are easier to recognize in the smooth coloring,
especially those clusters formed by nodes with multiple classes. With pure pie
chart visualization, it is di cult to recognize such class clusters. Smooth coloring
provides sharp boundaries between class clusters. These boundaries are helpful
in the process of assigning new data.</p>
        <p>Figure 4.3shows the class coloring of the same data with di erent values of the
parameter minimum visible class. In Figure 4.3 -(A) only the dominant classes
are visualized; This is achieved by sitting the parameter minimum visible class
to 100%. Recall that the coloring algorithm nd the dominant class for each
Voronoi cell and paint the class color as a background for the corresponding cell.
Then the algorithm paints the rest of the classes. By setting a large value for
the parameter minimum visible class, we prevent the algorithm from painting
classes other than the dominant class. The dominant class visualization shows
the general class topology by visualizing the main classes and ignoring the details
and outliers. In Figure 4.3 -(B) the parameter is set to 50%, only few details are
seen. In Figure 4.3-(C) the parameter is set to 0% and this causes the no classes
are ltered and thus all details are visualized.</p>
        <p>In Figure 4.4 the the border of the voronoi cells are visible to help analyze the
coloring. Furthermore 6 samples from di erent areas were zoomed. The pie chart
visualization of each sample is placed beside it to help comparing and analyzing
the sample. Samples of signi cant meaning were selected; some of them show
strength and other shows drawbacks of the class coloring.</p>
        <p>The sample in Figure 4.4 -(A) shows a drawback in the coloring algorithm,
namely nding the dominant class: The Voronoi cell marked with a circle was not
optimal partitioned: we expect that the blue area in the marked cell is connected
to the blue cluster in the neighborhood. Actually there is an isolated blue area
in this cell. after analyzing the sample we found that this happens if there are
two classes that are equally dominant. In our case they are blue and brown.
The randomly decision to one of the two classes results in such sub-optimal
partitioning.</p>
        <p>The sample in Figure 4.4 -(B) shows an area with a heavy interleaved class
distribution: As it is shown in the pie chart visualization, the nodes in the sample
have up to 7 classes assigned. Partitioning such areas is not meaningful especially
if the cells are small and close to each other. The resulting coloring looks like
chessboard visualization.</p>
        <p>The sample in Figure 4.4 -(C) also shows a drawback concerning the areas
marked with circles and colored in light gray. Three neighboring Voronoi cells
have the same class, namely the class colored in light gray; it is expected to have
a continuous gray area extending over the three cells. Actually we see four gray
areas, two in the median cell and one in each of the two other cells. We analyze
the case and found that this is caused by the class colored in green, which has two
special properties: isolated and has a large fraction in the cell. The algorithm
always assigns isolated classes rst of all. That is the most of the area is colored
in green which means there is no enough place to paint a gray stripe, which is
drawn through the cell and connect the two other cell. If we change the algorithm
to paint the isolated classes at the end, then we will face the problem, isolated
cells could be split int two or more parts, which is also not optimal.</p>
        <p>The sample in Figure 4.4 -(D) shows an optimal partitioning in our opinion:
The class colored in brown form a continuous cluster that extends over many
Voronoi cells. The cluster borders are smoothed without zigzag boundaries.
Isolated classes are also painted as expected. If there are more than one isolated
class in one Voronoi cell, they are painted as rings in the middle of the Voronoi
cell. In fact this good quality holds for most of the cases.</p>
        <p>The sample in Figure 4.4 -(E) shows a typical example of isolated classes.
Isolated classes are painted as a circle in the middle of the voronoi cell. If there
are more than one isolated class, then the they are painted as rings about the
circle. The order of painting (which class to which rings) is randomly selected.</p>
        <p>The sample in Figure 4.4 -(F) shows the e ect of the weighting to produce
smoothed borders without zigzag boundaries. The yellow area marked with a
circle has smoothed boundaries, i.e. at the points, that join the yellow areas in
the three cells cells.</p>
        <p>Figure 4.5 shows the chessboard class coloring of the banksearch data set.
This visualization is meaningful especially in the areas, where the classes are
interleaved, because in this case the smoothed partitioning makes no sense.
4.3</p>
      </sec>
      <sec id="sec-3-2">
        <title>Audio data</title>
        <p>In this subsection we will test and evaluate our visualization method by using
the data set presented in [LR06], where a method for pro ling radio stations is
described. The aim of this method is to use SOM to give listeners the possibility
to easy select the radio stations they like from the overwhelming number of radio
stations, that exist online and over the air. Radio station maps are created,
which visualize the pro les of radio stations so that it is possible to directly pick
a speci c program type instead of having to search for a suitable radio station.</p>
        <p>In this data set, audio output of 8 radio stations was analyzed: features from
the audio signal received from each radio station were extracted, then a
twodimensional SOM was trained with this data. Rhythm Histograms and Statistical
Spectrum Descriptors were used for pro ling radio stations. Data was sampled
with a rate of one feature vector for every 6 seconds received from a radio station
broadcast. For more information see [LR06].</p>
        <p>Figure 4.6 shows the class coloring of the the data set. There are 8 classes,
which represent the radio stations. The coloring partitions the SOM-lattice into
8 partitions, which are painted on top the SOM lattice. The nodes are organized
according the similarity between feature vectors, which represent the genre of
the broadcast signals; that means that near areas on the Map represents similar
genres regardless of the color(radio station).</p>
        <p>To help testing and analyzing the coloring, we manually divided the lattice
into regions labeled with the most important categories of genres, such as speech,
classic, pop, schlager, etc. These labels was taken from [LR06]. Each region
represents a main genre and can be further divided into subregions that represents
di erent genres that are similar; for example speech can be divided into news,
reports, advertisement, etc.</p>
        <p>At rst sight, we can see that there are some large areas painted in one
(main) color forming class clusters; also, there are other areas that are painted
in many colors and in some cases they are heavy interweaved. Class clusters can
be interpreted as radio stations that are specialized or have the focus on audio
broadcast of certain genres, an example for this type could be the radio station
Oe1 and Bgld: For some one who is familiar with these radio stations, this is
meaningful, because radio station Oe1 is a radio station which classic music as
one of the main broadcas categories, whereas the other analyzed stations seldom
broadcast classic music. The same thing holds also for the genre schlager and
the radio station Bgld: Bgld Radio station broadcast very often schlager music.</p>
        <p>Interweaved classes can be interpreted as radio stations, that broadcast similar
or almost the same genres; an example could be the radio stations OE3 and
TIROL, which are mostly represented by the area top/left in Figure 4.6 -B. Also
it can interpreted as radio stations that broadcast genres in a wide spectrum and
has no focus, such as the radio station ONE.</p>
        <p>The area represents the genre Pop music (upper part of the gure) is very
interleaved. This is meaningful because because Pop music is very popular and is
broadcast by almost all radio stations. The same thing holds exactly for the area
Speech especially those regions representing news genre, which is also expected,
because all radio stations broadcast news.</p>
        <p>In the areas with heavily interleaved classes we face the problem of
ambiguousness: if the classes are uniformly (or nearly uniformly) distributed, the coloring
algorithm fails to nd a de nite order to color the areas; namely to nd the
dominant class, which is painted as a background for the voronoi region, on which
the other classes are painted. The selection of the dominant class depends on
the class distribution in the neighborhood. In case of heavily interleaved areas
and, the dominant class may be selected randomly. This means that more than
one coloring are possible. Figure 4.6 -E shows such a case. On the contrary
Figure 4.6 -F shows areas with classes, that are not very interleaved. These areas
have a clear class structure, because the dominant classes and coloring order is
algorithmically clear de ned. In fact the ambiguousness in the coloring of heavy
interleaved classes can be ignored: we can not assume that the trained SOM
holds and re ects the data exactly, but only the general topology and details up
to a certain grad are hold; this means that the coloring algorithm deals with a
class distribution that meet the reality with a certain accuracy level, which varies
depending on many factors; in such a situation we can ignore the error and the
ambiguousness that result from the heavy interleaved class distribution.</p>
        <p>Figure 4.7 shows the class coloring of the data set with di erent values of
the parameter minimum visible class. As we see in the gure the visible details
decrease as the parameter value increase. After comparing the visualizations
Figure 4.7-(A) an Figure 4.7-(D) we can see a serious problem: if we look at the
areas marked with circles in (A), we see a dominant colors which are brown in
the upper area and red in lower one. After setting the parameter to 100% the
elementary classes should be eliminated and only the dominant class should be
visible. If we look at the visualization in (D) we see that the corresponding areas
(areas marked in circles) have a di erent classes than these dominant classes we
see with our eyes; namely blue/cyan in the upper area and green in the lower
one. This means that there is an error in the coloring algorithm. We analyzed
the problem and found the error; it was namely in the algorithm, that nd the
dominant class: the algorithm nds the dominant class as follows: for each class in
the actual unit, it counts the regions in the neighborhood which contain this class,
then it selects the class with the maximum number of such regions. The algorithm
does not take into account the contributions for these classes. This means that if
most of the surrounding regions have a class then it is considered the dominant
class regardless the contribution fraction of this class. So the marked area in (A)
right/buttom have the class green in most of the cells but in a low contribution,
nevertheless it is considered as a dominant class and this is the error.</p>
        <p>We corrected the algorithm to consider the weighted occurrence instead of only
the occurrence of the class in the surrounding regions in nding the dominant
class; That is, the dominant class is the class for which the sum of the fractions
in the surrounding neighborhood is the maximum.</p>
        <p>The same test after the correction produced a satisfying result, which matches
the expectation and agree with the dominant class seen by the human eye. The
result after correction is illustrated in Figure 4.8.</p>
        <p>Also, we tested the banksearch data set after the correction and we found
that some e ects, which we considered as sub-optimal, disappeared. For example
the e ect in Figure 4.4-(C). The class coloring of the banksearch data set after
the correction of the error is illustrated in Figure 4.9</p>
        <p>Figure 4.10 shows the chessboard visualization of the radio search data set. In
the visualization we can see the main clusters and the class topology and clouds
of color mixture in the areas with interleaved class distribution. This coloring is
meaningful because if you concentrates only at the full painted areas you see the
dominant classes and thus the general class topology. Whereas if you concentrate
at the color mixture you see the detailed class distribution.
Figure 4.3: Class Coloring with dierent values of the parameter minimum visible class.
(A)the parameter is set to 100%. (B)the parameter is set to 50%. (C)the parameter is set to
0%.</p>
        <sec id="sec-3-2-1">
          <title>Conclusions</title>
          <p>Self-Organizing Map is a strong and useful tool for analyzing large and/or complex
data sets especially those of high dimensionality. If the SOM is designed to be a
tool for human use, then a SOM visualization is useful or sometimes necessary.
SOM Visualizations are divided into two main categories: Visualization methods
for unlabeled data, which are either visualizations that show the map in relation
to the data set such as hit histograms or visualizations that are derived from the
model vectors which aim at showing cluster structure and boundaries such as
UMatrix. The second category includes visualizations that assume the availability
of labeled data and uses these (class) labels to produce a visualization, which
shows the class topology of the data such as the pie chart visualization.</p>
          <p>In this work a novel visualization method for visualization of labeled data is
proposed namely the SOM class coloring method. This method aims to produce
a colored partitioning of the SOM lattice depending on the class distribution in
the units. The coloring process has two main steps: The rst step is partitioning
the SOM lattice by nding the Voronoi diagram having the unit positions as
sites. The second step is partitioning each Voronoi cell separately depending on
the classes in the corresponding unit and the classes in the neighboring cells. To
achieve the second step, attractor functions are used: The attractor function is
an algorithm, that attracts the colored pixels to a line segment. For partitioning
a Voronoi cell with the attractor function, line segments with suitable lengths,
directions, start and end points are selected, taking into account the classes in
the unit and the classes in the neighboring units; nally the attractor function is
CHAPTER 5. CONCLUSIONS
applied to these line segments and the corresponding classes.</p>
          <p>An alternative method for coloring the cells is the chess board coloring, in
which the Voronoi cell is divided into squires, which are colored according a
uniform distribution function taking into account the contribution fractions for
each class.</p>
          <p>The class coloring method was tested with three di erent data sets, namely
the iris data set, banksearch data set and an audio data set consisting of the
broadcast of eight radio stations. The most of test showed satisfying results.
Some drawbacks were found such as the problem of ambiguousness; that is more
than one coloring is possible. The problem occurs if the class distribution is very
interleaved.</p>
          <p>
            P.J. Flynn Anil Jain, M.N. Murty. Data clustering: a review. ACM
Computing Surveys , 31(
            <xref ref-type="bibr" rid="ref2">3</xref>
            ):264 323, September 1999.
          </p>
          <p>Franz Aurenhammer and Otfried Schwarzkopf. A simple on-line
randomized incremental algorithm for computing higher order voronoi
diagrams. In Annual Symposium on Computational Geometry, Proceedings
of the seventh annual symposium on Computational geometry , pages
363 381. ACM Press, New York, NY, USA, 1992.
[Aup03] Michael Aupetit. High-dimensional labeled data analysis with gabriel
graphs. In Proceedings Intl. European Symp. on Articial Neural
Networks (ESANN’03) , Bruges, Belgium, 2003. Dside publications.</p>
          <p>Olivier Devillers. Improved incremental randomized delaunay
triangulation. In Proceedings 14th Annu. ACM Sympos. Computer Geometry ,
pages 106 115, 1998.</p>
          <p>Steven Fortune. A sweepline algorithm for voronoi diagrams. In SCG
86: Proceedings of the second annual symposium on Computational
geometry, pages 313 322, New York, NY, USA, 1986. ACM Press, New
York, NY, USA.</p>
          <p>Steven Fortune. Voronoi diagrams and delaunay triangulations. pages
377 388, 1997.</p>
          <p>Samuel Kaski. Data exploration using self-organizing maps. Acta
Polytechnica Scandinavica, Mathematics, Computing and Management in
Engineering Series No. 82 , March 1997.
[Koh97] Teuvo Kohonen. Exploration of very large databases by self-organizing
maps. In Proceedings of ICNN’97, International Conference on Neural
Networks, pages PL1 PL6. IEEE Service Center, Piscataway, NJ, 1997.</p>
          <p>Thomas Lidy and Andreas Rauber. Visually pro ling radio stations. In
Proceedings of the 7th International Conference on Music Information
Retrieval (ISMIR 2006) , 10 2006.</p>
          <p>
            Georg P lzlbauer, Michael Dittenbach, and Andreas Rauber. Advanced
visualization of self-organizingmaps with vector elds.
[PRM02] Elias Pampalk, Andreas Rauber, and Dieter Merkl. Using smoothed
data histograms for cluster visualization in self-organizing maps. In
Proceedings of the International Conference on Artical Neural
Networks (ICANN’02) . Springer Lecture Notes in Computer Science,
Madrid, Spain, 2002.
[RPM03] Andreas Rauber, Elias Pampalk, and Dieter Merkl. The som-enhanced
jukebox organization and visualization of music collections based on
perceptual models. New Music Research , 32(
            <xref ref-type="bibr" rid="ref1">2</xref>
            ):193 210, June 2003.
[Sam69] John Sammon. A nonlinear mapping for data structure analysis. In
          </p>
          <p>IEEE Trans. Computer , volume c-18, pages 401 409, May 1969.
[SC02]</p>
          <p>Mark Sinka and David Corne. A large benchmark dataset for web
document clustering. In Soft Computing Systems: Design, Management
and Applications, Volume 87 of Frontiers in Articial Intelligence and
Applications , pages 881 890, 2002.</p>
          <p>Softsurfer.com. Geometry algorithms. about lines
and distance of a point to a line (2d 3d). See
http://softsurfer.com/Archive/algorithm_0102/algorithm_0102.htm.
[Ult03a] Alfred Ultsch. Maps for the visualization of high dimensional data
spaces. In Proceedings Workshop on Self organizing Maps(WSOM’03),
Kyushu, Japan, 2003.</p>
          <p>Alfred Ultsch. Esom-maps: tools for clustering, visualization, and
classi cation with emergent som. In Technical Report 46, Dept. of
Mathematics and Computer Science , D-35032 Marburg, Germany, March
2005.</p>
          <p>
            Juha Vesanto and E. Alhoniemi. Clustering of the self-organizing map.
IEEE-NN, 11(
            <xref ref-type="bibr" rid="ref2">3</xref>
            ):586, May 2000.
          </p>
          <p>Juha Vesanto. SOM-based data visualization methods.
IntelligentData-Analysis , 3:111 26, 1999.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          <source>2 Related Work 4 2</source>
          .1 Self organizing
          <string-name>
            <surname>map . . . . . . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>4 2.1.1 The map initialization</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>5 2.1.2 The training process</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>6 2.2 Clustering and classi cation</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . . . . 8</surname>
          </string-name>
          <year>2</year>
          .
          <issue>2</issue>
          .1 Hierarchical
          <string-name>
            <surname>algorithms . . . . . . . . . . . . . . . . . . . . 9</surname>
          </string-name>
          <year>2</year>
          .
          <issue>2</issue>
          .2 Partitive
          <string-name>
            <surname>algorithms . . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>10 2.2.3 Two-level clustering approach / Clustering of the SOM</article-title>
          . .
          <volume>11</volume>
          2.3
          <string-name>
            <surname>Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>12 2.3.1 Sammon's</article-title>
          <string-name>
            <surname>Mapping . . . . . . . . . . . . . . . . . . . . . . 13</surname>
          </string-name>
          <year>2</year>
          .4
          <string-name>
            <surname>Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>14 2.4.1 Visualization of SOM with Unlabeled data</article-title>
          <string-name>
            <surname>. . . . . . . . .</surname>
          </string-name>
          <article-title>14 2.4.2 Visualization of SOM with Labeled data</article-title>
          <string-name>
            <surname>. . . . . . . . . . 20</surname>
          </string-name>
          <year>2</year>
          .5
          <string-name>
            <surname>Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21</surname>
          </string-name>
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          <source>3 Methods 25 3</source>
          .1 SOM
          <string-name>
            <surname>Toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26</surname>
          </string-name>
          <year>3</year>
          .2 Problem
          <string-name>
            <surname>formulation . . . . . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>28 3.3 SOM coloring by color ooding</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>28 3.4 Using graphs to color the SOM</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>32 3.4.1 Voronoi diagrams</article-title>
          and
          <source>Delaunay triangulations . . . . . . . 32 3</source>
          .
          <issue>4</issue>
          .
          <issue>2 Voronoi cell</issue>
          <string-name>
            <surname>coloring . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>36 3.4.3 Angular segmentation of Voronoi cells</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . 38</surname>
          </string-name>
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>