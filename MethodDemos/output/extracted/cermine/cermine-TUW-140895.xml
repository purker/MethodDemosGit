<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>An Ant Colony Optimisation Algorithm for the Bounded Diameter Minimum Spanning Tree Problem</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>unter der Anleitung von</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
          <xref ref-type="aff" rid="aff1">1</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Univ.-Prof. Dipl.-Ing. Dr.techn. Gu ̈nther Raidl Univ.Ass. Dipl.-Ing. Martin Gruber</institution>
        </aff>
        <aff id="aff1">
          <label>1</label>
          <institution>durch Boris Kopinitsch</institution>
          ,
          <addr-line>Bakk.techn. R ̈omerweg 31 A-2801 Katzelsdorf</addr-line>
        </aff>
      </contrib-group>
      <pub-date>
        <year>1997</year>
      </pub-date>
      <fpage>309</fpage>
      <lpage>314</lpage>
    </article-meta>
  </front>
  <body>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-140895.images\img_26_1.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-140895.images\img_26_2.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-140895.images\img_26_3.png" />
    </fig>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-140895.images\img_26_4.png" />
    </fig>
    <sec id="sec-1">
      <title>-</title>
      <p>Im Rahmen dieser Magisterarbeit wurde ein Ant Colony Optimisation Algorithmus fu¨r
das durchmesserbeschr¨ankte minimale Spannbaum Problem erarbeitet. Bei diesem
Problem handelt es sich um ein N P-schweres kombinatorisches Optimierungsproblem mit
zahlreichen praktischen Anwendungsgebieten, zum Beispiel im Netzwerkdesign. Der
Algorithmus wurde mit einer lokalen Optimierungsheuristik, n¨amlich einem Variable
Neighbourhood Descent, erweitert. Diese lokale Optimierungsheuristik arbeitet auf vier verschiedenen
Nachbarschaftsstrukturen, bei deren Entwicklung besonders auf eine effiziente Evaluierung
der Nachbarschaft einer L¨osung Wert gelegt wurde. Vergleiche mit verschiedenen
evolution¨aren Algorithmen und einer variablen Nachbarschaftssuche auf euklidischen Instanzen
bis zu 1000 Knoten hinsichtlich L¨osungsqualit¨at als auch Berechnungszeit zeigen, dass
der Ant Colony Optimisation Algorithmus bei ausreichend Zeit die bisher besten
bekannten Ergebnisse zum Teil deutlich u¨bertreffen kann, hingegen bei Testl¨aufen mit starker
Zeitbeschr¨ankung nicht die L¨osungsqualit¨at der variablen Nachbarschaftssuche erreichen
kann.
This master thesis presents an ant colony optimisation algorithm for the bounded diameter
minimum spanning tree problem, a N P-hard combinatorial optimisation problem with
various application fields, e.g. when considering certain aspects of quality in communication
network design. The algorithm is extended with local optimisation in terms of a variable
neighbourhood descent algorithm based on four different neighbourhood structures. These
neighbourhood structures have been designed in a way to enable a fast identification of
the best neighbouring solution. The proposed algorithm is empirically compared to various
evolutionary algorithms and a variable neighbourhood search implementation on Euclidean
instances based on complete graphs with up to 1000 nodes considering either solution
quality as well as computation time. It turns out that the ant colony optimisation algorithm
performs best among these heuristics with respect to quality of solution, but cannot reach
the results of the variable neighbourhood search implementation concerning computation
time.
2. Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
8. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57</p>
      <p>LIST OF FIGURES
6.1 Class diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46</p>
    </sec>
    <sec id="sec-2">
      <title>LIST OF TABLES</title>
      <p>7.1 Long-term runs on Euclidean instances . . . . . . . . . . . . . . . . . . . . . 53
7.2 Short-term runs on Euclidean instances . . . . . . . . . . . . . . . . . . . . 55</p>
    </sec>
    <sec id="sec-3">
      <title>1. INTRODUCTION</title>
      <p>
        The bounded diameter minimum spanning tree (BDMST) problem is a combinatorial
optimisation problem. Combinatorial optimisation problems belong to the group of
optimisation problems, that in turn are divided into two groups. One, encoding solutions with
real-valued variables and one, encoding solutions with discrete variables. Combinatorial
optimisation problems belong to the latter one. The definition of a combinatorial optimisation
problem given here, follows that by Blum and Roli in [
        <xref ref-type="bibr" rid="ref4">6</xref>
        ].
      </p>
      <p>Definition 1: A combinatorial optimisation (CO) problem P = (f, S) has a set of
variables X = {x1, . . . , xn}, variable domains D1, . . . , Dn, constraints among variables, and an
objective function to be minimised or maximised, where f : D1 × . . . × Dn → R+.
The set of all feasible assignments is S = {s = {(x1, v1), . . . , (xn, vn)} | vi ∈ Di, s satisfies
all constraints}. S is also called search or solution space.</p>
      <p>To solve a combinatorial optimisation problem as defined above, a solution T 0 ∈ S with
either minimum objective value function, f (T 0) ≤ f (T ) ∀ T ∈ S, or maximum objective
value function, f (T 0) ≥ f (T ) ∀ T ∈ S, has to be found. T 0 is called a global optimal
solution. Representative combinatorial optimisation problems are the travelling salesman
problem, the quadratic assignment problem, timetabling and scheduling problems.
After having introduced the definition of the combinatorial optimisation problem that of
the bounded diameter minimum spanning tree problem can be given.</p>
      <p>Definition 2: Given an undirected, connected graph G = (V, E) of n = |V | nodes and
m = |E| edges, where each edge e has associated costs ce ≥ 0, the bounded diameter
minimum spanning tree problem is defined as the spanning tree T = (V, ET ), with edge
set ET ⊆ E, of minimum weight Pe∈ET ce, where the diameter D is bounded above by a
constant ≥ 2.</p>
      <p>The eccentricity of a node v, with v ∈ V , is defined as the maximum number of edges on
a path from v to any other node in the minimum spanning tree T . The diameter bound D
is the maximum eccentricity a node is allowed to have.
From this definition follows that the centre of T is either, in case of an even diameter
bound, the single node or, in case of an odd diameter bound, the pair of adjacent nodes of
minimum eccentricity. Thus the bounded diameter minimum spanning tree problem can
also be interpreted as the search for a minimum spanning tree rooted at an unknown centre
(having depth 0) and whose maximal depth is restricted to bD/2c.</p>
      <p>The BDMST problem is known to be N P-hard for 4 ≤ D ≤ n − 1 [15]. Within the context
of this master thesis, simply BDMST problem instances based on complete graphs are
considered, since incomplete graphs can be anytime transformed into complete graphs, by
setting the edge costs for not really available edges extremely high, so that these edges do
not surface in solutions.</p>
      <p>The BDMST problem is not just of scientific interest, there are various real world
applications, for example in communication network design. When considering certain aspects of
quality of service, e.g. a maximum communication delay or minimum noise-ratio, BDMSTs
become of great importance.</p>
      <p>
        A further practical application can be found in [
        <xref ref-type="bibr" rid="ref3">5</xref>
        ]: When planning a Linear Light-wave
Network (LLN) an undirected multi-graph G = (V, E) is used, representing the network.
This multi-graph G has to be decomposed into edge disjoint trees forming at least one
spanning tree. Nevertheless, the aim of this decomposition process is to gain many spanning
trees with a small diameter.
      </p>
      <p>
        Another application field of the BDMST problem is data compression. Bookstein et al.
have introduced a way of transferring the problem of compressing correlated bit-vectors
into the problem of finding a minimum spanning tree [
        <xref ref-type="bibr" rid="ref5">7</xref>
        ]. The decompression time of a
given bitmap vector is proportional to its depth within the tree. Thus the whole running
time depends on the height of the built tree. So a MST with a bounded diameter is
preferable.
      </p>
      <p>
        The BDMST problem is also met in the field of mutual exclusion algorithms. When
considering costs of the distributed mutual exclusion algorithm proposed in [
        <xref ref-type="bibr" rid="ref7">32</xref>
        ], the number
of messages required to enter a critical section has a major effect on these costs. As the
upper bound for these messages is 2D, where D is the diameter of the underlying tree, it is
quite obvious, that the network topology the algorithm is based on is of major importance.
Furthermore, the BDMST problem can also be found as a subproblem in other
combinatorial optimisation problems such as vehicle routing [2]. In the vehicle routing problem
a set of vehicles has to serve a set of costumers minimising a cost function like the path
covered by the vehicles or the number of vehicles to be used. In general some additional
constraints have to be met, for example each costumer has to be visited exactly once by
exactly one vehicle. Another one could be, that the demand each vehicle has to satisfy
is not allowed to exceed the vehicle’s capacity D. Or that each vehicle has to start and
end its tour in the depot. A further constraint could be that the total tour length of each
vehicle is not allowed to exceed a bound L. From these example constraints follows that
the vehicle routing problem (VRP) is closely related to the travelling salesman problem
(TSP), as a solution for the VRP consists of several TSP solutions with common start and
end cities. Famous heuristics for solving TSP instances, e.g. the Christofides heuristic, use
minimum spanning trees. Thus, in case an additional constraint requires that the lengths
of the vehicle tours are limited to size L, for applying these heuristics to discover a tour
for a vehicle they have to be extended in terms of using BDMSTs instead of MSTs.
This master thesis will present an ant colony optimisation algorithm for the bounded
diameter minimum spanning tree problem. The ant colony optimisation is extended with a
local search strategy, namely a variable neighbourhood descent, to improve overall
solution quality. The following chapter will give an overview of already evolved exact as well
as heuristic methods for solving a BDMST instance. In chapter 3 the basic concepts of
two metaheuristics, namely ant colony optimisation and variable neighbourhood search,
are introduced. Chapter 4 presents four different neighbourhood structures the variable
neighbourhood descent algorithm is based on. In chapter 5 the ant colony optimisation
algorithm for the BDMST problem, developed within the context of this master thesis, is
described in full details. Chapter 6 gives an overview of the implementation of the ant
colony optimisation algorithm. And finally in Chapter 7 the performance of the ant colony
optimisation algorithm compared to that of the variable neighbourhood search
implementation for the BDMST, proposed by Gruber and Raidl [21], and some other state-of-the-art
metaheuristics is discussed.
      </p>
    </sec>
    <sec id="sec-4">
      <title>2. PREVIOUS WORK</title>
      <p>
        A couple of exact algorithms for the bounded diameter minimum spanning tree problem has
been designed. The majority of them relies on network flow-based multi-commodity mixed
integer linear programming (MIP) or Miller-Tucker-Zemlin (MTZ) formulations. By
introducing multi-source multi-destination network flow models [27] Magnanti and Wong
performed fundamental work of using MIPs in the field of network design. A multi-commodity
formulation for the BDMST problem has been proposed by Achuthan and Caccetta [
        <xref ref-type="bibr" rid="ref1">3</xref>
        ].
Achuthan et al. suggested an improvement of this formulation in [
        <xref ref-type="bibr" rid="ref2">4</xref>
        ].
      </p>
      <p>A couple of different advanced multi-commodity flow (MCF) formulations for the BDMST
problem, counting and limiting the hops on paths from a virtual root node to any other
node, has been introduced by Gouveia and Magnanti [18]. For the sake of achieving tight
LP-relaxation bounds, they had to accept a quite large number of variables in their models.
In [19] Gouveia et al. proposed an extended flow formulation for the BDMST problem in
case of an odd diameter D.</p>
      <p>Santos et al. [14] used modified and lifted Miller-Tucker-Zemlin subtour elimination
inequalities, ensuring that the diameter bound is not violated. This MTZ-based model is
claimed to work well on BDMST problem instances having a diameter bound D close to
the diameter of the unconstrained minimum spanning tree.</p>
      <p>A compact 0-1 integer linear programming (ILP) model embedded into a branch and cut
environment has been published by Gruber and Raidl [22]. Computational results turned
out that BDMST instances, having an underlying dense graph, with small to moderate
diameter bounds, are solved significantly faster than by the MTZ-based model from Santos
et al. [14]. However the looser the diameter bound the faster the MTZ-based model
becomes. Therefore it cannot be said, that one approach dominates the other. When
comparing the ILP with various MCF formulations from Gouveia and Magnanti [18], similar
results were yielded.</p>
      <p>Experiments turned out that all these exact approaches for solving a BDMST problem
instance can only be applied to relatively small problem instances, not more than 100</p>
      <sec id="sec-4-1">
        <title>2. Previous Work 10</title>
        <p>nodes when considering fully connected graphs. As these exact algorithms are not able to
yield the optimal solution for large problem instances in practicable time, several heuristics
have been evolved to obtain solutions for large instances, too.</p>
        <p>One of these heuristics is the so called one time tree construction (OTTC) algorithm by
Abdalla et al. [1]. This greedy construction heuristic, based on the minimum spanning
tree algorithm of Prim, starts by selecting a single node at random and then repeatedly
extends the tree by adding the cheapest available edge, that connects a new node to the
so far built tree. To guarantee that the diameter bound is not violated it must keep track
of the eccentricities of all already appended nodes and update this information every time
a new node is connected to the tree, a time consuming procedure. It turned out that the
choice of the first node has crucial effects on the quality of the solution.</p>
        <p>Julstrom [26] modified the OTTC approach by starting from a predetermined centre node.
This idea simplifies the OTTC algorithm significantly, as the diameter constraint can be
displaced by a height restriction of bD/2c for the spawned MST tree. This centre-based tree
construction (CBTC) provides relatively good solutions on random instances. Nevertheless,
on Euclidean instances the randomised centre-based tree construction (RTC) [31], that uses
a permutation of all nodes and connects them with the cheapest possible edge to the tree in
the order given by this permutation, whereas the centre is build by the first (two) node(s),
yields much better results.</p>
        <p>Clementi et al. [8] describe other construction heuristics, e.g. a modification of Kruskal’s
MST algorithm, for the related height-constrained MST problem.</p>
        <p>As the solutions obtained by these construction heuristics were still not satisfactory,
different evolutionary algorithms (EAs) [31, 24, 25] have been developed, to further improve
the quality of these solutions. The initial population for these evolutionary algorithms is
provided by one of the greedy construction heuristics. On instances up to 1000 nodes the
evolutionary algorithms are in a position to significantly improve these initial solutions.
The best metaheuristic published so far for Euclidean instances, outperforming the EAs
mentioned above in solution quality as well as running time, is a variable neighbourhood
search by Gruber and Raidl [21]. Unfortunately it was not tested on random instances yet.</p>
      </sec>
    </sec>
    <sec id="sec-5">
      <title>3. METAHEURISTICS</title>
      <p>
        Since combinatorial optimisation (CO) problems are of major interest for the scientific
as well as for the industrial world, several algorithms have been evolved to meet them.
These algorithms can be classified into exact algorithms and heuristics. Exact algorithms
guarantee to find an optimal solution for finite size CO problems in bounded time [30].
As long as not proofed that P = N P, no polynomial time algorithms exist for N P-hard
CO problems [15]. Due to their heigh computational time complexity exact algorithms
are in general only applicable to instances of relatively small or moderate size. To attack
even bigger instances heuristics have been evolved. Heuristics abandon the guarantee of
finding an optimal solution for the sake of returning a valid solution in useable time. For
some heuristics, referenced as approximate algorithms, it has been proven, that the yielded
solution is not more (when the objective function has to be minimised) or less (when the
objective function has to be maximised) than a factor φ times the optimal solution.
Among heuristics two basic types, constructive and local search methods, are differentiated.
Constructive methods, as the name already presumes, create a solution for a CO problem
from scratch. They append step by step to an initially empty solution components until
a valid solution is reached. Local search methods start with an initial solution. They try
to replace this initial solution T by a better solution from a predefined neighbourhood of
T . Both definitions given here, either that of a neighbourhood of a solution T as well as
that of a local optimal (minimum) solution with respect to a predefined neighbourhood
structure N , follow those by Blum and Roli in [
        <xref ref-type="bibr" rid="ref4">6</xref>
        ].
      </p>
      <p>Definition 3: The neighbourhood structure is a function N : S → 2S 1 that assigns each
T ∈ S a set of neighbours N (T ) ⊆ S, where S is the search or solution space. N (T ) is
called neighbourhood of T .</p>
      <p>Definition 4: A solution T 0 is called a local optimal (minimum) solution with respect
to a neighbourhood structure N , if ∀ T ∈ N (T 0): f (T 0) ≤ f (T ), where f is the objective
1 2S is the power set of S
function to be minimised. A solution T 0 is called a strict local optimal (minimum) solution,
if f (T 0) &lt; f (T ) ∀ T ∈ N (T 0).</p>
      <p>Local search algorithms are also called iterative improvement algorithms, as moves are
only performed if they result in an improvement of the existing solution. To identify an
improvement move usually one of the two main strategies, namely first improvement and
best improvement, is used. First improvement means, that the local search algorithm stops
searching the neighbourhood N (T ) of an existing solution T as soon as an improvement
move has been identified. By contrast the best improvement strategy enumerates the
complete neighbourhood N (T ) to identify the best improvement move. Local search algorithms
terminate as soon as a local optimal (minimum) solution has been reached.
Beside these basic heuristics so-called metaheuristics have been developed. The aim of this
concept is to build high level frameworks based on several heuristics, allowing an effective
and efficient exploration of the whole solution space. Until now no universal valid definition
of the term metaheuristic has prevailed, therefore a multitude of definitions from various
scientists and organisations exists. In my opinion, the most appropriate one is the following
from Voß et al.:
“A metaheuristic is an iterative master process that guides and modifies the operations of
subordinate heuristics to efficiently produce high-quality solutions. It may manipulate a
complete (or incomplete) single solution or a collection of solutions at each iteration. The
subordinate heuristics may be high (or low) level procedures, or a simple local search, or
just a construction method.”[35]
The term metaheuristics, that was first introduced in 1986 by Glover [16], also reflects the
aim of this concept, since it is composed of two Greek words, heuriskein, meaning “to
find” and meta, meaning “beyond, in an upper level”. Nowadays, several metaheuristics
have been proposed. Ant colony optimisation, variable neighbourhood search, evolutionary
algorithms, simulated annealing, iterated local search and tabu search, to name just a
selection.</p>
      <p>The following two sections present the basic ideas of the variable neighbourhood search
and ant colony optimisation metaheuristic.
3.1</p>
      <sec id="sec-5-1">
        <title>Variable Neighbourhood Search</title>
        <p>The variable neighbourhood search (VNS) metaheuristic was first proposed by Hansen and
Mladenovi´c [28, 29]. The basic principle of VNS is to improve a given solution T by a
systematic change of different neighbourhood structures. VNS is based on following three
simple facts.</p>
        <p>Fact 1: A local minimum T with respect to a single neighbourhood structure does not
imply that T is a local minimum to any other neighbourhood structure.</p>
        <p>Fact 2: The optimal solution has to be a local minimum with respect to all neighbourhood
structures.</p>
        <p>Fact 3: For many problems local minima with respect to one or more neighbourhood
structures are relatively close to each other.</p>
        <p>It can be gathered from these three facts that in general a local optimal solution with
respect to at least one neighbourhood structure has something in common with the global
optimal solution. It is only for sure that the local minimum provides some information,
that can also be found in the global optimal solution. Unfortunately, this information
cannot be extracted from the local minimum. So an organised search in the predefined
neighbourhood structures of this local minimum is performed in order to improve it. There
are three possible ways to guide this neighbourhood search: deterministic, stochastic or
both combined.</p>
        <p>The deterministic approach, called variable neighbourhood descent (VND), manages in
a deterministic way the change of neighbourhoods. The VND concept exploits fact one,
namely that a local optimal solution with respect to one neighbourhood structure is not
necessarily one to another neighbourhood structure. Algorithm 1 gives an overview of
the VND concept. Before executing Algorithm 1 the different neighbourhood structures
N1, . . . , Nk have to be defined. The idea of the VND concept is to compute iteratively a
local optimum with respect to all different neighbourhood structures. Every time a new best
solution is discovered, assume by exploiting neighbourhood structure Ni with i ≤ k, the
algorithm jumps back to the first neighbourhood structure as it cannot be guaranteed that
this new best solution is also a local minimum with respect to all preceding neighbourhood
structures Nj with 1 ≤ j &lt; i. So each time the algorithm starts searching for better solution
Algorithm 1: Variable Neighbourhood Descent
1 create initial solution T ;
2 i = 1;
3 while i ≤ k do
4 find best neighbouring solution T 0 ∈ Ni(T );
5 if T 0 better than best solution found so far then
6 save T 0 as new best solution T ;
7 i = 1;
8</p>
        <p>else i = i + 1;
by exploiting neighbourhood structure Ni, it is ensured that the actual best solution is a
local optimal solution with respect to all preceding neighbourhood structures Nj with j &lt; i.
At the end the algorithm terminates in a solution T that is a local minimum with respect
to all neighbourhood structures defined for the VND.</p>
        <p>The stochastic approach is referenced as reduced variable neighbourhood search (RVNS).
Algorithm 2 presents the basic idea of the RVNS concept. After defining the different
neighbourhood structures N1, . . . , Nk the algorithm starts by computing an initial solution
T . In the RVNS concept each neighbourhood structure is no longer explored completely,
identifying the best move. Instead, in each iteration (lines 4 to 9), including all
neighbourhood structures Ni with 1 ≤ i ≤ k, a solution T 0 ∈ Ni(T ) is arbitrarily chosen and
compared if it is better than the so far best solution T . This process is called shaking.
Again, as already shown in Algorithm 1, an improvement of the best solution T terminates
the current iteration and starts a new one, beginning with neighbourhood N1.</p>
        <p>Algorithm 2: Reduced Variable Neighbourhood Search
1 create initial solution T ;
2 while termination condition not met do
3 i = 1;
4 while i ≤ k do
5 select completely random a solution T 0 ∈ Ni(T );
6 if T 0 better than best solution found so far then
7 save T 0 as new best solution T ;
8 i = 1;
9</p>
        <p>else i = i + 1;
However, this shaking makes it impossible to guarantee that at any time the best solution
found so far is a local minimum with respect to all neighbourhood structures. Therefore a
termination condition has to be introduced, e.g. a maximum number of iterations without
further improvement or a maximum CPU time. The RVNS method seems to be practicable
if the enumeration of a whole neighbourhood is too cost-intensive, e.g. in case of a large
neighbourhood with an exponential number of neighbouring solutions.</p>
        <p>And finally, the approach including a deterministic as well as a stochastic component is
known as basic variable neighbourhood search. In principle the basic VNS is an extension
of the RVNS concept. Algorithm 3 presents the idea of the basic VNS. Assuming the
different neighbourhood structures N1, . . . , Nk have been defined, the algorithm starts by
creating an initial solution, that is simultaneously the best solution T found so far. After
shaking T , that is always performed within the context of the actual neighbourhood under
consideration Ni(T ) with i ≤ k, the resulting solution T 0 ∈ Ni(T ) is tried to be further
improved by local search methods. This complete random selection of a solution T 0 ∈
Ni(T ), where T represents the best solution found so far, avoids any possible cycling.
A drawback of this behaviour is, as already mentioned for the RVNS concept, that it
cannot be assured at any time that a solution T is locally optimal with respect to all
neighbourhood structures. Therefore the use of a termination condition becomes essential.
Again if the local search methods yield an improvement of the overall best solution T found
so far, the current iteration is aborted and a new one is started, beginning by shaking T
within N1(T ). This shaking yields a solution T 0 ∈ N1(T ) as input for the local search
methods. As proposed in [23], these local search methods can be replaced by a complete
VND. However, as a consequence of using a VNS/VND combination the shaking process
has to be extended, since VND always yields a local optimal solution T with respect to
all neighbourhood structures. Therefore, within a shaking process m ≥ 2 moves based on
a single neighbourhood have to be performed to facilitate this VNS/VND combination an
escape from local optimal solutions with respect to all neighbourhood structures.</p>
        <p>Algorithm 3: Basic Variable Neighbourhood Search
1 create initial solution T ;
2 while termination condition not met do
3 i = 1;
4 while i ≤ k do
5 select completely random a solution T 0 ∈ Ni(T );
6 try to improve T 0 by using local search methods;
7 if T 0 better than best solution found so far then
8 save T 0 as new best solution T ;
9 i = 1;
10 else i = i + 1;
3.2</p>
      </sec>
      <sec id="sec-5-2">
        <title>Ant Colony Optimisation</title>
        <p>The ant colony optimisation (ACO) metaheuristic belongs to the class of ant algorithms.
Ant algorithms were first proposed by Dorigo et el. [10, 13] as a multi agent approach for
various difficult combinatorial optimisation problems, e.g. the travelling salesman problem
(TSP) or the quadratic assignment problem (QAP). Ant algorithms are based on the
behaviour of real ants. Of main interest is their foraging behaviour and, in particular, their
ability to find the shortest paths between their nest and food sources.</p>
        <p>The key component of this ability is a chemical substance, called pheromone. While
walking ants deposit pheromone on the ground, building a pheromone trail. Pheromone
can be sensed by ants. Therefore these pheromone trails enable ants to find their way back
as well as other ants to find food sources, discovered by nest-mates. Furthermore, when
ants have to choose between several trails, they tend to pick, in probability, those with high
pheromone concentration. It has been experimentally shown that in case of different trails
to the food source, this pheromone trail following behaviour is responsible for finding the
shortest path. One of these conducted experiments to study this behaviour in controlled
conditions is the binary bride experiment by Deneubourg et al. [9]. In this experiment
the nest was separated by two branches of same length from the food source. Initially the
two branches were pheromone free. Due to the fact of random fluctuations a few more
ants randomly selected one branch, in the experiment the upper one. As ants deposit
pheromone, the greater number of ants on the upper branch laid more pheromone on it,
that in turn stimulated more ants to choose the upper branch, and so on. Finally nearly
all ants chose the upper branch. A modification of this experiment, where the branches are
of different length, can be found in [17].</p>
        <p>Remarkable is the fact that a single ant has only the capability of finding a path to the
food source. Only the coaction of the whole ant colony enables finding the shortest path
between the nest and the food source. Therefore the foraging behaviour of ant colonies
can bee seen as a distributed optimisation process, using only indirect communication,
accomplished through pheromone trails. This indirect communication between nest-mates
is known as stigmergy [20].</p>
        <p>The key design component of the ACO metaheuristic is to portion computational resources
to a set of agents (artificial ants), that in turn provide through cooperative interaction good
solutions. As artificial ants are a reproduction of real ants, we can distinguish between
qualities adopted from real ants and those added to make them more efficient and effective:
• Ant colony: Ant algorithms consist of a finite size colony of artificial ants. These
artificial ants, forming the colony, act as real ants independently and concurrently.
Despite this two properties artificial ants show – as their counterparts in the real
world – a cooperative behaviour.
• Stigmercy: As real ants, artificial ants deposit pheromone on the paths used for
constructing a solution. This pheromone laying behaviour modifies the problem
representation, that in turn is the basis for the indirect communication among artificial
ants. This way of indirect communication is, as already mentioned, called stigmercy.
• Local moves: Real ants are not able to jump to any position they want to. So
do artificial ants, since they only move through adjacent problem states. Therefore
artificial ants accomplish as their natural counterparts only local moves.
• State transition policy: As real ants, artificial ants apply a stochastic local
decision policy while moving through adjacent problem states. This decision policy is
exclusively based on local information, consisting of the pheromone trails and
sometimes some a priori problem-specific information. Furthermore, local means that this
information can only be accessed from the state in which it was released. Sometimes
this condition is relaxed so that local information can also be accessed from
neighbouring states. So as real ants, artificial ants do not make use of lookahead concepts
to predict future states.
• Memory: Memory is a characteristic of artificial ants, that is not found in their
natural counterparts. Each artificial ant has some memory capacity to store past
activities, that in turn can be used to prevent ants from entering invalid problem
states. Another practical application of using memory capacity is to compute the
quality of the generated solution.
• Amount of pheromone: In contrast to real ants, artificial ants are able to evaluate
the fitness of the solution found. Depending on this value they are able to bias
the amount of pheromone laid on the paths forming the solution. In general the
amount of pheromone deposited by an ant is proportional to the fitness of the solution
constructed by this ant.
• Time of pheromone laying: Artificial ants are not only capable of regulating
the amount of pheromone they deposit, they are also able to influence the time
of pheromone laying. Real ants deposit pheromone while walking. By contrast ant
algorithms adapt the time when pheromone is laid to the problem. For many problems
the deposition of pheromone after generating the solution turned out to be more
practical.
• Additional capabilities: The basic capabilities can be extended to improve overall
performance. Lookahead, backtracking or local optimisation are examples of these
extra capabilities, to name just a few.</p>
        <p>On principle the activity of the ant colony optimisation metaheuristic can be described as
follows. In each iteration of an ACO algorithm an ant colony, consisting of a finite size of
artificial ants, equipped with the above listed characteristics, tries to find good solutions to
a difficult combinatorial optimisation problem. A solution is a minimum cost path through
the different problem states. It should be clear that this path has to satisfy all problem
constraints. The basic ACO concept envisions two tasks for each artificial ant. The first
one is constructing either whole solutions or parts of a solution. Important is that while
accomplishing this task ants only move through adjacent problem states. Each time an
artificial ant moves from one state to the next, it uses a stochastic local decision policy,
based solely on local information, that is information that can be only accessed from the
actual state. To avoid ants to enter invalid problem states this local decision policy can
be extended with private information of the artificial ant. The second task each artificial
ant has to achieve is depositing pheromone. This can be arranged in several ways. The
one extreme is that while moving through adjacent problem states ants deposit pheromone
on the paths used. The other is that after finishing task one, thus the whole solution
or a part of it has been constructed, ants evaluate the fitness of the solution and deposit
pheromone depending on its quality. Combinations and variations of these two extremes are
also possible pheromone depositing strategies. This pheromone laying behaviour is the key
component of the ACO metaheuristic, since it changes the representation of the problem,
more precisely the way how the problem is experienced by other artificial ants, and therefore
influences the further search process. After accomplishing these two tasks, creating or
improving a solution and depositing pheromone, the artificial ant “dies”, respectively is
deleted from the colony.</p>
        <p>
          Beside these two basic tasks of artificial ants, two further procedures can be used to
enhance the performance of the ACO metaheuristic, pheromone trail evaporation and daemon
activities. The process of decreasing intensity of pheromone trails, assuming no further
pheromone is deposited on them, is called pheromone trail evaporation. Pheromone trail
evaporation becomes essential when prohibiting a too fast convergence. The strength of
pheromone trail evaporation and pheromone depositing on the one hand, as well as the
stochasticity of the local decision policy on the other hand are responsible that the ant
colony optimisation metaheuristic is not trapped soon in a local optimum, but exploits
already accumulated knowledge. To find the right balance between the pheromone update
process and the stochasticity of the decision policy is essential, since only then an
exploration of many different search areas of the solution space is possible. Daemon activities
describe actions that cannot be accomplished by a single ant. Therefore they are executed
by a global guard, called a daemon. A typical activity is biasing the search process by
depositing some additional pheromone on paths forming an especially good solution.
Over the years many different ACO metaheuristics have evolved. More or less they all have
their roots in the first ACO algorithm developed, the so called ant system (AS) proposed
in 1991 by Dorigo [10, 13]. Three ant system algorithms have been introduced, that differ
only in the pheromone update process. In ant-density (constant amount of pheromone)
and ant-quantity (amount of pheromone inversely proportional to the costs of the chosen
trail) algorithms ants deposit pheromone while constructing the solution. On the contrary
to this two approaches, in ant-cycle algorithms ants lay pheromone after having finished
building a solution. Computations on a set of benchmark problems [10, 13] showed, that
the performance of ant-cycle algorithms was superior to those of ant-density and
antquantity algorithms. So research concentrated on the characteristics of ant-cycle, which is
nowadays known as ant system, whereas the two other algorithms were abandoned. Other
remarkable characteristics of ant system algorithms are that every ant deposits pheromone
and that the amount of pheromone is proportional to the fitness of the solution found. AS
algorithms do not make use of daemon activities, since the pheromone trail evaporation
process is delegated to every ant. So each artificial ant is not only responsible for depositing
pheromone on the paths used, but also for decreasing pheromone on all paths, either those
used as well as those not. The memory capability is used to prohibit ants from entering
invalid problem states. In general, before starting an AS algorithm, each path, connecting
two adjacent problem states, is initialised with a small positive amount of pheromone.
Experimental results on small TSP instances [13] showed that the AS approach reached
and sometimes even exceeded the performance of some general purpose heuristics compared
to. Unfortunately, on larger problem instances it could not keep up with other heuristics.
So several other ACO metaheuristics have evolved to improve the performance of the ant
system.
One of this refined approaches was proposed by Stu¨tzle and Hoos [
          <xref ref-type="bibr" rid="ref8">34, 33</xref>
          ] in 1997. They
presented a MAX − MIN ant system (MMAS) for the travelling salesman problem, that
is in the main an ant system with three slightly differences. First, the pheromone trails are
updated by a daemon that deposits extra pheromone on the best discovered tour. Second,
the amount of pheromone on each path connecting two adjacent problem states has to be
within a given interval [τMin, τMax]. The pheromone amount τ is not allowed to fall under
τMin, nor to exceed τMax. And finally, as a third difference, the pheromone amount on
each path is initialised to the maximum τMax.
        </p>
        <p>Another ACO metaheuristic developed to improve the poor performance of the basic AS
is the ant colony system (ACS) introduced by Dorigo and Gambardell in 1996 [12, 11].
The main difference to the ant system is the daemon activity guarding the pheromone
update. It picks within one iteration only the best solution constructed by an ant and
deposits pheromone only on the paths forming this solution. So only a single artificial ant
of the whole colony deposits its pheromone. Furthermore, the daemon is responsible for
the pheromone trail evaporation process. Another difference can be found in the details of
the local decision policy.</p>
        <p>To conclude, the basic concepts of the ant colony optimisation heuristic are quite simple,
inspired by nature. Nevertheless, for a concrete ACO implementation various problem
specific decisions have to be made, for example the number of artificial ants the colony
should consist of, the kind of pheromone update to be used, including depositing as well as
evaporation, the usage of a local decision policy and a-priori problem-specific local
information. Another problem dependent key component is the initialisation of the pheromone
amount on the paths, connecting adjacent problem states. Experience shows that the ACO
metaheuristic can only yield good results when applied to problems where each state in
the search space does not have a large number of neighbouring solutions. Otherwise the
probability that artificial ants will visit the same state is marginal due to the huge number
of possible moves. As a consequence the key component of an ACO, making local decisions
based on pheromone information, does not work anymore because of the small pheromone
differences on the various paths.
afterwards. And finally, if T is a local minimum with respect to the Edge Exchange,
Node Swap and Centre Exchange neighbourhood, Level Change tries to optimally arrange
all non-centre nodes within the level structure. When this VND algorithm terminates it
reached a local optimal solution with respect to all four different neighbourhood structures.
5.2</p>
      </sec>
      <sec id="sec-5-3">
        <title>Ant Colony Optimisation</title>
        <p>As already presented in Chapter 3.2, several different ant algorithms have been developed.
The ACO for the BDMST problem, proposed within the context of this master thesis, is
based on the ant colony system (ACS), first proposed by Dorigo et al. in 1996 [12, 11]. A
property taken from the ACS for the travelling salesman problem (TSP) is the pheromone
initialisation process. Another adapted characteristic is that every artificial ant is only
responsible for constructing a solution, since the pheromone update process is managed
by a daemon. This daemon identifies, as it is representative for ACS algorithms, the best
solution found by the whole colony in one iteration and updates the pheromone information
proportional to the fitness of this best solution found. Pheromone trail evaporation is also
performed by the daemon. A main difference between the ACS concept and the ant colony
optimisation metaheuristic presented here lies in the local decision policy. In general the
decision policy of ACS algorithms includes a heuristic component when computing a state
transition. This heuristic component for the local decision policy is completely omitted for
this ACO algorithm.</p>
        <p>For the implementation of the ACO algorithm some data structures are required, most of
them already known from the four different neighbourhoods:
• Again the successor lists succ, storing for each node its immediate successors, the
predecessor array pred, saving for each node its predecessor in the directed tree from
the centre to the leaves, the level information lev, stating the level a node is assigned
to and the level sets Vl with l = 0, . . . , bD/2c, containing all nodes at the level l, are
used.
• In addition, a two dimensional n × (bD/2c + 1) pheromone matrix is needed to keep
one pheromone entry for each combination of a node and a level. The value τi,j
reflects the desirability of assigning node i to level j.</p>
        <p>The level information lev and level sets Vl ⊂ V are of major interest, as the ant colony
optimisation algorithm developed in the context of this master thesis uses a different approach
regarding pheromone deposition in comparison to the basic ACO concept for the TSP
proposed by Dorigo [10, 13]. There they operate on a graph G = (V, E) where pheromone is
deposited – either by ants themselves or by a daemon – on edges e ∈ E contained in the
solution. The ACO algorithm presented here does not deposit pheromone on edges but
exploits the fact that once given the level information lev(v) for all nodes v ∈ V an optimal
solution for the bounded diameter spanning tree problem with respect to this level
information lev can be easily derived. Therefore simply for each non centre node v the least cost
predecessor p with lev(p) &lt; lev(v) has to be found. In case of several possible least cost
predecessors, always one with minimum level is selected. Thus ants construct a solution
not by “running” through the graph structure, but by assigning nodes to levels, building
the level information lev. So the pheromone information reflects instead of the desirability
of using an edge the desirability of assigning a node to a certain level. Algorithm 9 gives an
overview of the ant colony optimisation algorithm developed in the context of this master
thesis.</p>
        <p>To be able to construct a solution based on the information stored in the pheromone matrix,
it has to be first initialised. This initial amount of pheromone for each combination of node
and level has to be in relation to those values the pheromone update process is going to write
into the matrix. E.g., if the initial values are too big compared to those, the pheromone
update process is going to deposit, it will last very long until the ant colony will be capable
of exploiting the information stored in the pheromone matrix. To guarantee that ants are
able to use the collected information very soon the pheromone initialisation process follows
the scheme of ACS algorithm for the TSP [12, 11], where the initial amount of pheromone
deposited is proportional to the product of an initial solution value and the total number
of nodes of the instance graph. Therefore in the first line of Algorithm 9 a solution S using
the randomised tree construction (RTC) heuristic [31] is computed. The solution value
obtained from the RTC heuristic, together with the total number of nodes n is used to
initialise the pheromone matrix with τi,j = 1/(S ∗ n), i = 0, . . . , n − 1 and j = 0, . . . , bD/2c.
Until no termination condition is met (line 3), which can be either a time limit, a maximum
number of iterations without further improvement or both combined, in each iteration an
colony of a finite number of artificial ants tries to find good solutions based on the so far
collected information on the problem instance.</p>
        <p>Each artificial ant builds a solution based on the information stored in the pheromone
matrix. As already mentioned, the solution construction process exploits the fact that a
given level information lev for each node is enough to construct the local optimal tree.
Algorithm 9: ACO for the BDMST
1 create an initial solution S using the RTC heuristic;
2 initialise the pheromone matrix with 1/(S ∗ n);
3 while termination condition not met do
4 for each ant of the colony do
5 create a solution T , based on the information saved in the pheromone matrix;
6 VND(T );
7
8
9
10
determine best solution T 0 found by the whole colony;
update pheromone matrix using T 0;
if T 0 better than bestSolution then</p>
        <p>bestSolution = T 0;
So each ant simply sets each node to a certain level. The level a node is assigned to is
selected randomly with a probability proportional to the pheromone information stored for
this node, i.e. Pi,l is the probability that node i is assigned to level l.</p>
        <p>Pi,l =
τi,l</p>
        <p>D</p>
        <p>Plb0 =21c τi,l0
In general – due to the stochastic level selection process – each node can be assigned to
any level as long as no entry in the pheromone matrix is zero. This is a desirable behaviour
except for level 0, where the number of nodes is fixed in advance based on the diameter
bound. Therefore the centre node(s) is (are) selected separately based on the information
stored in the first column of the pheromone matrix (the pheromone values of level 0 for
each node). As said above this local decision policy has no heuristic component, every node
is assigned to a certain level independently from all other nodes, only the decoding step –
deriving a tree from the level information – makes use of a problem specific heuristic when
using for each non centre node its cheapest available predecessor.</p>
        <p>To improve solutions built by artificial ants the variable neighbourhood descend for the
BDMST, presented in the previous section, is applied to these solutions. Thus each ant
constructs a solution based on the information stored in the pheromone matrix. Afterwards
the VND algorithm tries to improve this solution (lines 5 to 6).</p>
        <p>The daemon is responsible for collecting all solutions found by the ant colony in the current
iteration (line 7). From this pool of solutions it selects the best one T 0 and determines
the amount of pheromone, that is proportional to the fitness of this best solution, going
to be written into the pheromone matrix for the appropriate node/level combinations.
This behaviour, that only the best ant deposits pheromone, was first proposed for ACS
algorithms. The quantity of pheromone deposited is determined as follows:
After having calculated the amount of pheromone to be deposited, the pheromone matrix is
updated (line 8). This update process combines the pheromone laying and the pheromone
evaporation process and follows the scheme introduced for ACS algorithms:
τi,j = (1 − ρ)τi,j + ρΔτi,j
The parameter ρ, with 0 ≤ ρ ≤ 1 has a crucial impact on the convergence of the ant colony.
The greater ρ the faster the ant colony commits itself to a certain solution, it converges
faster, perhaps too fast. On the contrary, the smaller ρ the later the ant colony is able
to exploit already collected knowledge of the problem instance, in terms of the pheromone
matrix, to construct good solutions, but therefore the solution space is explored in more
detail. So using this parameter the behaviour of this ant colony optimisation algorithm
can be influenced substantially and adjusted for different applications.</p>
        <p>In line 9 and 10 the daemon verifies if the best solution found by the ant colony in the
current iteration is better than the best solution found so far. If so this solution is saved
as new best solution.</p>
      </sec>
    </sec>
    <sec id="sec-6">
      <title>6. IMPLEMENTATION</title>
      <p>The ant colony optimisation metaheuristic, introduced in this master thesis, is implemented
in C++, following the C++ coding standard, e.g. representing each class through a header
file, stating the attribute and method declarations, and an implementation file, stating the
program code. The only non-standard class library used is LEDA (Library of Efficient Data
Types and Algorithms) in terms of version 5.0.1. The aim of the LEDA project, launched
in 1988, was to build a small, but extendable library of data structures and algorithms
for combinatorial computing in a form making them easy to use. Since February 1st,
2001, Algorithmic Solutions Software GmbH is responsible for maintaining and developing
LEDA1.</p>
      <p>The Instance class encapsulates all relevant information concerning a BDMST problem
instance that does not change during computations, e.g. the underlying graph or the
diameter bound. Due to this fact, each run of the ant colony optimisation algorithm has
exactly one Instance object.</p>
      <p>The InstanceParser is responsible for parsing the input file and storing the relevant
information in the corresponding data structures. The InstanceParser is able to parse
various file formats, e.g. GNUplot or EA graph instances. GNUplot files were created at
the Institute of Computer Graphics and Algorithms for testing and visualising purposes.
EA files are instances from the Beasley’s OR-Library2 which haven been originally intended
for Euclidean Steiner tree problems. As in [31, 25, 21] these instances have been used for
comparing this ACO with other state-of-the-art metaheuristics.</p>
      <p>The Solution class is used for representing solutions for a certain problem instance. To
express a valid solution the attribute successorVector, storing for each node its immediate
successors, would be sufficient, but to enable faster computations additional attributes
1 http://www.algorithmic-solutions.com/enleda.htm
2 http://people.brunel.ac.uk/∼mastjjb/jeb/orlib/esteininfo.html</p>
      <p>Instance
+graph
+diameter : int
+Instance()
1..*
InstanceParser
+InstanceParser()
creates
1
1</p>
      <p>Aco
+pheromoneMatrix
+main()
+ConstructSolution(inout Solution : Solution)
has
+currentSolution : Solution
+LocalSearch(inout currSol : Solution)
+Execute() : bool
+inst : Instance
+centreNode : int
+centreNode2 : int
+sucessorVector
+predecessor
+Solution(in inst : Instance)
+ValidateSolution() : bool
+CalcSolValue() : int</p>
      <p>EdgeExchange
+EdgeExchange(inout currSol : Solution)
+Execute() : bool</p>
      <p>LevelBased
+nodesAtLevel
+LevelBased(inout currSol : Solution)</p>
      <p>NodeSwap
+NodeSwap(inout currSol : Solution)
+Execute() : bool
CentreExchange</p>
      <p>LevelChange
+CentreExchange(inout currSol : Solution)
+Execute() : bool
+LevelChange(inout currSol : Solution)
+Execute() : bool</p>
      <p>Fig. 6.1: Class diagram
are used. The predecessor attribute stores for each node its direct predecessor and
centreNode keeps the centre node. In case of an odd diameter centreNode2 saves the
second one. To express the correlation between a Solution object and the instance it
belongs to, a pointer to the Instance object is passed to in the constructor of the Solution
class. This pointer enables the Solution object to access instance specific information,
like number of nodes, diameter bound or connection costs between two nodes.
Important methods of the Solution class are ValidateSolution() and CalcSolValue(). The
ValidateSolution() method verifies if the solution represented by this Solution object
contains all nodes of the problem instance, if the diameter bound is not violated and if
the solution is free of circles. If it turns out that the Solution object represents a valid
BDMST true is returned, otherwise false. The CalcSolValue() method calculates the
total costs of the solution represented by the object and returns this objective value.
The LocalSearch class is an abstract class and merges common attributes and
methods of the implementation of the four different neighbourhood structures, namely Edge
Exchange, Node Swap, Centre Exchange and Level Change. In the constructor of the
LocalSearch class a reference to a Solution object is passed to. This enables different
LocalSearch objects, in case they are referencing the same object, to operate on the same
Solution object in a sequential order. Additional instance specific information can be
accessed via this reference to the Solution object and its pointer to the Instance object.
The Execute() method, each subclass of LocalSearch has to implement, starts the
improvement process, stopping in a local optimal solution with respect to the neighbourhood
structure implemented by this subclass. In case the objective value of the solution passed
to the constructor of the LocalSearch class has been improved, Execute() returns true,
otherwise false.</p>
      <p>EdgeExchange is a subclass of the abstract LocalSearch class and implements the Edge
Exchange neighbourhood. All improvement moves found by the Execute() method are
executed directly on the Solution object passed to it in the constructor. This holds true
for all other subclasses of the abstract LocalSearch class, too.</p>
      <p>Node Swap, another subclass of LocalSearch, implements the second neighbourhood
structure based on the successor and predecessor relationships of the nodes, beside Edge
Exchange, namely – as the name already presumes – Node Swap.</p>
      <p>The LevelBased class is abstract and summarises attributes and methods used by its
two subclasses, CentreExchange and LevelChange, implementing neighbourhoods based
on the level representation of a solution. An example for such an additional attribute is
nodesAtLevel which stores for each level all nodes assigned to it.</p>
      <p>As already mentioned CentreExchange and LevelChange are subclasses of LevelBased.
The Execute() method of CentreExchange computes a local optimal solution with
respect to the Centre Exchange neighbourhood, whereas LevelChange implements the Level
Change neighbourhood.</p>
      <p>The Aco class is the core of this ant colony optimisation program. It is responsible for
creating all the required objects, as there are for example the sole object of the Instance
class, objects of Solution and subclasses of the abstract LocalSearch class. Furthermore,
it administrates – strictly speaking initialises and updates – the pheromone matrix. Based
on the information stored in this pheromone matrix each artificial ant builds a solution
by calling the CreateSolution() method. These solutions are then tried to be locally
improved by a variable neighbourhood descent algorithm using the four different
neighbourhood structures implemented as subclasses of LocalSearch.
The proposed ant colony optimisation metaheuristic, implemented in C++, is a simple
command line program, named Aco. The program was developed under Linux 2.4.21.
The program was compiled using gcc in version 3.3.1 under glibc in version 2.3.2. To
start the program some mandatory parameters have to be specified. If they are missing
the program will prompt an error message followed by a usage instruction. Furthermore,
Aco has some optional parameters, that can be omitted when starting the ant colony
optimisation program. In the following a synopsis of the program parameters is shown.
./Aco -t graphtype -d diameter -i instancefile [-g instancefile] [-r rho]
[-s time_limit] [-l iteration_limit] [-a ants]
Here in alphabetical order the list describing all optional as well as mandatory parameters
of the Aco program in detail:
• a is an optional parameter. It indicates the number of artificial ants forming the ant
colony. If it is omitted a default colony size of 10 ants is used.
• d is a mandatory parameter, since it specifies the diameter bound. The number
declared has to be a positive integer number greater or equal to 4, otherwise an error
message is prompted.
• i has also to be specified, stating the path to the instance file.
• g is only a mandatory parameter when a GNUplot graph instance is used, since
GNUplot instances consist of two files, a point and a line file. So in case of a GNUplot
graph instance the parameter g takes the line file, the corresponding point file has to
be specified using parameter i. In case of all other graph types this parameter has to
be omitted, otherwise an error message is prompted.
• l specifies the maximum number of iterations without an improvement of the best
solution found so far. As it is an optional parameter it can be omitted, in this case
the program sets it per default to 1000.
• r is an optional parameter, with 0 ≤ r ≤ 1. This parameter represents the ρ value
used in the pheromone update process. The greater ρ, the faster the ant colony
commits itself to a certain solution. If omitted a default value of 0.5 is used.
• s is an optional parameter indicating the time limit in seconds, otherwise no time
limit is set. Only positive integer numbers are valid values for this parameter, all
other specifications will result in a corresponding error message.
• t is a mandatory parameter and specifies the type of the instance file. The instance
parser used by this program is able to read in four different file formats:
– 0 GNUplot graph
– 1 CostMatrix graph (Gouveia and Magnanti [18])
– 2 EA graph
– 3 Santos graph (Santos et al. [14])
The output of the Aco program is sent to standard out (std::cout) per default, but it can
easily be redirected to any file with the help of the appropriate Unix operator “&gt;”. The
output starts with a summary of all instance relevant information, e.g. instance file, number
of nodes or diameter bound. Then a summary of each iteration follows, containing various
information on every ant of the colony. Each ant builds a solution exploiting the knowledge
stored in the pheromone matrix. Afterwards, this solutions is tried to be improved by a
VND algorithm using four different neighbourhood structures. This iteration summary
states for each artificial ant the total costs of the solution found after local improvement
by VND, the total costs of the best solution found so far, time needed so far and finally an
indicator if the current best solution was achieved by this ant. Finally, when a termination
condition is met – either time limit or x rounds without further improvement of the best
found solution – a complete summary of the optimisation process is printed, containing
again all instance relevant information plus total costs of the best solution found and the
time in seconds until the program was terminated. Note, that it in general the time stated
at the end of this summary is not the time required to achieve the best solution. In the
following a sample output of an Aco run is presented:
------------------------------------------------------------------------Bounded diameter minimum spanning tree problem: ACO [ver. 1.0 / 10.2005]
------------------------------------------------------------------------Reading in instance ...</p>
      <p>Successfully read in EA instance... ../../data/ea/estein500_01.eu
Creating Solution object to save best solution ... done
Creating Solution object for computations ... done
Creating EdgeExchange object ... done
Creating NodeSwap object ... done
Creating CentreExchange object ... done
Creating LevelChange object ... done
Creating starting solution value ... done
Creating Aco object ... done
build time: 0.35 sec.</p>
      <p>objective value: 2215672
Starting Ant Colony Optimisation:
objective value (starting solution): 2215672
neighbourhood order: escl
number of ants (colony size): 10
rho: 0.1000
maximum number of iterations without improvement: 1000
time limit (in seconds, 0 for no time limit): 4000
------------------------------------------------------------------------* Ant 0 found 1877464 [vnd] 1877464 [ACO] 1.75 sec.
* Ant 1 found 1858671 [vnd] 1858671 [ACO] 3.65 sec.</p>
      <p>Ant 2 found 1885506 [vnd] 1858671 [ACO] 5.30 sec.
Ant 4 found 1918045 [vnd] 1836331 [ACO] 8.92 sec.
Ant 5 found 1884828 [vnd] 1836331 [ACO] 11.30 sec.
Ant 6 found 1887286 [vnd] 1836331 [ACO] 13.10 sec.
Ant 7 found 1945281 [vnd] 1836331 [ACO] 15.31 sec.
Ant 8 found 1876021 [vnd] 1836331 [ACO] 18.13 sec.</p>
      <p>Ant 9 found 1918894 [vnd] 1836331 [ACO] 20.05 sec.
------------------------------------------------------------------------Updating pheromone matrix... 1836331 [ACO] 20.05 sec.
------------------------------------------------------------------------Ant 0 found 1844813 [vnd] 1836331 [ACO] 21.46 sec.
.
.</p>
      <p>.
* Ant 8 found
1676033 [vnd]
1676033 [ACO]</p>
      <p>2726.53 sec.
Summary of optimisation:</p>
    </sec>
    <sec id="sec-7">
      <title>7. COMPUTATIONAL RESULTS</title>
      <p>This chapter takes a closer look at the performance of the ACO algorithm for the BDMST
problem, developed within the context of this master thesis, and compares it to that of
other heuristics for the BDMST problem. Three state-of-the-art metaheuristics have been
selected for this comparison, a VNS implementation by Gruber and Raidl [21], a
permutation coded EA as well as a random-key coded EA from [25]. Of major interest is the
comparison of the ACO presented here and the VNS by Gruber and Raidl, since they use
in principle the same neighbourhood structures for their VND as it is done in the ACO,
especially because the VNS is the so far leading metaheuristic for the BDMST problem.
For comparison the same instances, diameter bounds, time limits and termination
conditions as in [21] were used. The instances were taken from the Beasley’s OR-Library1 and
have been originally proposed for the Euclidean Steiner tree problem. All nodes of a graph
are settled within the unit square and the Euclidean distance between any pair of nodes
corresponds to the edge costs between these two nodes. Furthermore, as in [21], the tests
were grouped into long-term and short-term runs. The long-term runs concentrated on
achieving as good results as possible. In the short-term runs the different algorithms had a
relatively short time with respect to the instance size for yielding good solutions. The size
of the ant colony was set to ten artificial ants for all test categories. Before presenting the
computational results it should be noted, that all test runs were performed on a Pentium R 4
2.8 GHz system using Linux 2.4.21 as operating system.</p>
      <p>First the results of the long-term runs are discussed. As in [21] the first five instances of
each size n = 100, 250, 500 available in the OR-Library were used and the diameter bound
was set to 10 for instances with 100 nodes, to 15 for instances with 250 nodes, and to 20 for
instances with 500 nodes. Two different termination conditions were introduced for these
long-term runs for the ACO, adopted from [21]: The ACO algorithm terminated either if
the last 1000 iterations yielded no improvement of the best found solution so far or if a time
limit was reached. This time limit was set to 2000 seconds for instances with 100 nodes,
to 3000 seconds for instances with 250 nodes, and to 4000 seconds for instances with 500
1 http://people.brunel.ac.uk/∼mastjjb/jeb/orlib/esteininfo.html
196.06
126.43
200.40
200.15
158.37
1159.08
1219.32
1034.92
1108.55
1152.07
2346.49
2726.73
2433.62
2384.56
2890.01
Instance permutation coded EA random-key coded EA
n D nr. best mean stddev best mean stddev best
best</p>
      <p>ACO
mean stddev time (sec.)</p>
      <p>Tab. 7.1: Long-term runs on Euclidean instances
nodes.
the number of nodes, the diameter bound, the instance number, and for each metaheuristic
the objective values of the best and mean solutions found. Furthermore, the standard
deviation of 50 (EAs) and 30 (VNS, ACO) independent runs is presented.
In addition, for the ACO the mean times for identifying the best solutions are given. As
indicated in Table 7.1 ρ varies among the different instance sizes in order to obtain better
solutions. The results for the EAs as well as those for the VNS are taken from [21].
Like the VNS implementation the ACO algorithm outperforms the two EAs concerning
either best as well as mean solution values. Comparing the ACO with the VNS it can
be seen that on instances with 100 nodes the ACO algorithm is still close to the VNS
implementation concerning best solutions, but is already better regarding mean values.
The greater the instances the larger the gap between the ACO algorithm and the VNS.
While on instances of size n = 250 the ACO performs already clearly better than the VNS
concerning best as well as mean solutions, on instances with 500 nodes the mean objective
value of the solutions found by the ACO is close to – in one case it is even better than –
the best solution identified by the VNS. In addition, the standard deviation of the ACO
algorithm is much smaller than those of the three other metaheuristics. Furthermore, it is
interesting to observe, that the time limit was never reached. For each instance size 1000
iterations without further improvement terminated the ACO algorithm, in contrast to, for
example, the VNS, which requires the whole 4000 seconds to compute the solutions listed
for the instances with 500 nodes.</p>
      <p>As in [21] for the short-term tests the permutation coded EA was replaced by an
edgeset coded EA from [31], since it makes use of operators with linear time complexity and
therefore scales much better with larger instances. The same tests should be performed
but for the 1000 node instances the time limit turned out to be really though for the ACO.
When using the time limit of 100 seconds no useful results could be achieved due to the
following reason: In the first iteration of the colony with its ten artificial ants every solution
constructed by an artificial ant is more or less random, as the pheromone matrix contains no
practical information yet (it is initialised uniformly). Therefore the variable neighbourhood
descent part of the ACO algorithm can substantially improve these solutions, but this
improvement process requires a lot of time due to the fact that the quality of the solution
constructed by an ant is very poor. As a consequence only four to six solutions could
be processed within the time limit of 100 seconds. Since not even one iteration could be
finished within this time limit, thus no update of the pheromone matrix was performed,
no relevant results for the ACO can be presented here for the 1000 nodes instances in
combination with a time limit of 100 seconds. Using a time limit of 1000 seconds enables
the ACO algorithm to finish at least five to eight complete iterations.</p>
      <p>Table 7.2 presents the results of the short-term runs. It shows the number of nodes, the
diameter bound, the instance number, and the time limit in seconds. Furthermore, for
each metaheuristic the objective values of the best solutions found, the mean values and
the standard deviations are listed. For each instance 50 (EAs) respectively 30 (VNS, ACO)
independent runs have been performed.</p>
      <p>In addition for the ACO algorithm the parameter ρ used for the experiments is given, since
varying it depending on the time limit and instance size yielded better results. Again the
results for the EAs as well as those for the VNS are taken from [21].</p>
      <p>From Table 7.2 it can be seen that the ACO algorithm consistently outperforms the two
EAs. Considering the 500 node instances, the mean values of the ACO with tighter time
limits, as well as those of the VNS, are even better than the objective values of the best
solutions found by any of the EAs. Unfortunately, the good results of the VNS cannot be
reached by the ACO. Looking at the 500 node instances it can be seen that when using
a time limit of 500 seconds the ACO algorithm comes closer to the results of the VNS
but cannot really exceed them. Just on two instances the ACO yields a better solution,
and it beats the VNS only one time concerning the mean values. The main reason for
this behaviour is – as already mentioned above – that the ACO algorithm starts with very
poor, more or less random solutions, since in the first iteration the solutions constructed by
the artificial ants are based on a uniformly initialised pheromone matrix. In the following
Instance time edge-set coded EA random-key coded EA
n D nr. limit (sec.) best mean stddev best mean stddev
best</p>
      <p>ACO
mean stddev
500 20 1
2
3
4
5
500 20 1
2
3
4
5
1000 25 1
2
3
4
5
1000 25 1
2
3
4
5</p>
      <p>Tab. 7.2: Short-term runs on Euclidean instances
these solutions can be improved significantly by the variable neighbourhood descent, but
at the expense of a high running time. Another aspect that has to be taken into account
in the short-term runs is the value chosen for the parameter ρ which controls how much
influence the best solution identified within one iteration shall have on the pheromone and
– consequently – on the probabilities calculated based on this pheromone information for
the following iterations. Remember, that based on these probabilities all artificial ants
construct solutions. The greater the ρ value the higher the probability to assign node v to
level l also in a subsequent iteration (if this combination was part of the best solution) since
the pheromone value for this combination of node and level will dominate the other ones
relatively fast. As a consequence the ACO algorithm converges to a certain solution after
only a small number of iterations. As the first solutions are in general of a poor quality
(even after the VND local improvement step), a great ρ value will not yield good solutions
at all, since a solution close to these bad solutions will be favoured very soon. On the other
hand, if the value of the parameter ρ is chosen too low the pheromone updates written into
the pheromone matrix are so small compared to the values already stored there that they
have only marginal effects on the probabilities calculated from the pheromone information.
So within the next iteration nearly the same probabilities can be met. When using a too
tough time limit with respect to the ρ value it is likely to occur that the already collected
information cannot be exploited at all. Table 7.2 indicates this effect. For instances with
500 nodes and a time limit of 50 seconds a relatively great ρ value has been used in order
to make use of the information already written to and stored in the pheromone matrix. By
decupling the time limit an obviously smaller ρ can be applied. This smaller ρ is responsible
for the behaviour that the algorithm does not favour – as fast as in the prior test category
– a certain solution, but explores a greater part of the solution space before preferring a
certain solution. This leads inevitably to much better results.</p>
      <p>Generally it can be noted, that the ACO algorithm is very slow in the beginning of the
computation process, as the solutions constructed in this phase are very poor and the
variable neighbourhood descent part can substantially improve these solutions, but this
local improvement step is very time consuming. As time proceeds the artificial ants will
construct better and better solutions, due to the information stored in the pheromone
matrix. As a consequence the neighbourhood descent part needs less and less time to
improve these solutions. So the time required for one iteration decreases over the whole
runtime of the ACO algorithm, as the time needed by an artificial ant to build a solution
is constant.</p>
    </sec>
    <sec id="sec-8">
      <title>8. CONCLUSIONS</title>
      <p>This master thesis proposed an ant colony optimisation algorithm for the bounded
diameter minimum spanning tree problem. Its main characteristic is that it makes use of a
local optimisation heuristic, namely a variable neighbourhood descent, to improve overall
solution quality. This VND approach is based on four different neighbourhood structures
for the BDMST problem, namely Edge Exchange, Node Swap, Centre Exchange and Level
Change. When developing these neighbourhood structures main focus was on an efficient
and fast exploration of a certain neighbourhood, for example computing only cost
differences when evaluating a solution. For two of them, namely Node Swap and Level Change,
an incremental enumeration, applied after having identified and executed the most
profitable move, was presented. This incremental exploration could not reduce the theoretical
worst case time complexity, but accelerated computation essential in practise.
Another important characteristic of the ant colony optimisation algorithm, developed
within the context of this master thesis, is the way artificial ants construct solutions. A tree
is not built by starting from a single node and successive connecting the remaining ones,
but ants distribute the nodes to various levels, since given this level information always a
local optimal minimum spanning tree ensuring the diameter bound can be derived in an
easy and straightforward way.</p>
      <p>To evaluate the overall performance of the ant colony optimisation algorithm it was
compared to the so-far-leading variable neighbourhood search implementation for the BDMST,
that operates on the same neighbourhood structures, as well as to three evolutionary
algorithms, namely a permutation, a random-key and a edge-set coded EA. Results on complete
Euclidean instances turned out, that when considering solution quality the ACO algorithm
is not only superior to the EAs but also to the VNS, especially on larger instances. In case
the computation time is highly restricted the solutions provided by the ACO algorithm are
in-between those of the EAs and those of the VNS.</p>
      <p>To conclude, the ant colony optimisation algorithm, proposed within the context of this
master thesis, has its strengths when operating on large complete Euclidean instances</p>
      <sec id="sec-8-1">
        <title>8. Conclusions 58</title>
        <p>
          without a time limit. A starting point for further research activities could be to adapt
this ACO approach in a way that it also works on instances where the underlying graph
is incomplete or, since the ACO algorithm was only compared to other metaheuristics on
Euclidean instances, to test its effectiveness on random instances.
[2] N. R. Achuthan and L. Caccetta. Models for vehilce routing problems. Proceedings of
the 10th National Conference of the Australian Society for Operations Research, pages
276–294, 1990.
[
          <xref ref-type="bibr" rid="ref1">3</xref>
          ] N. R. Achuthan and L. Caccetta. Minimum weight spanning trees with bounded
diameter. Australasian Journal of Combinatorics, 5:261–276, 1992.
[
          <xref ref-type="bibr" rid="ref2">4</xref>
          ] N. R. Achuthan, L. Caccetta, P. Caccetta, and J. F. Geelen. Computational
methods for the diameter restricted minimum weight spanning tree problem. Australasian
Journal of Combinatorics, 10:51–71, 1994.
[
          <xref ref-type="bibr" rid="ref3">5</xref>
          ] K. Bala, K. Petropoulos, and T. E. Stern. Multicasting in a linear lightwave network.
        </p>
        <p>
          In IEEE INFOCOM’93, pages 1350–1358, 1993.
[
          <xref ref-type="bibr" rid="ref4">6</xref>
          ] Christian Blum and Andrea Roli. Metaheuristics in combinatorial optimization:
        </p>
        <p>
          Overview and conceptual comparison. ACM Computing Surveys, 35(3):268–308, 2003.
[
          <xref ref-type="bibr" rid="ref5">7</xref>
          ] A. Bookstein and S. T. Klein. Compression of correlated bit-vectors. Information
        </p>
        <p>Systems, 16(4):387–400, 1991.
[8] A. E. F. Clementi, M. Di Ianni, A. Monti, G. Rossi, and R. Silvestri. Experimental
analysis of practically efficient algorithms for bounded-hop accumulation in ad-hoc
wireless networks. In Proceedings of the 19th IEEE International Parallel and
Distributed Processing Symposium (IPDPS’05), workshop 12, volume 13, page 247.1, 2005.
[9] J.-L. Deneubourg, S. Aron, S. Goss, and J.-M. Pasteels. The self-organizing
exploratory pattern of the argentine ant. Journal of Insect Behavior, 3:159–168, 1990.
[10] M. Dorigo. Optimization, Learning and Natural Algorithms (in Italian). PhD thesis,
Dipartimento di Elettronica, Politecnico di Milano, IT, 1992.
[11] M. Dorigo and L. M. Gambardella. Ant colonies for the traveling salesman problem.</p>
        <p>BioSystems, 43:73–81, 1997.
[12] M. Dorigo and L. M. Gambardella. Ant colony system: A coopeartive learning
approach to the traveling salesman problem. IEEE Transactions on Evolutionary
Computation, 1(1):53–66, 1997.
[13] M. Dorigo, V. Maniezzo, and A. Colorni. Positive feedback as a search strategy.</p>
        <p>Technical Report 91-016, Dipartimento die Elettronica, Politecnico di Milano, IT,
1991.
[14] A. C. dos Santos, A. Lucena, and C. C. Ribeiro. Solving diameter constrained minimum
spanning tree problems in dense graphs. In Proceedings of the International Workshop
on Experimental Algorithms, volume 3059 of LNCS, pages 458–467. Springer, 2004.
[15] M. R. Garey and D. S. Johnson. Computers and Intractability. A Guide to the Theory
of NP-Completeness. W.H. Freeman, New York, 1979.
[16] F. Glover. Future paths for integer programming and links to artificial intelligence.</p>
        <p>Comput. Oper. Res., 13:533–549.
[17] S. Goss, S. Aron, J. L. Deneubourg, and J. M. Pasteels. Self-organized shortcuts in
the argentine ant. Naturwissenschaften, 76:579–581, 1989.
[18] Luis Gouveia and Thomas L. Magnanti. Network flow models for designing
diameterconstrained minimum spanning and Steiner trees. Networks, 41(3):159–173, 2003.
[19] Luis Gouveia, Thomas L. Magnanti, and Christina Requejo. A 2-path approach for
odd-diameter-constrained minimum spanning and Steiner trees. Networks, 44(4):254–
265, 2004.
[20] P. P. Grass´e. La reconstruction du nid et les coordinations interindividuelles
chez bellicositermes natalensis et cubitermes sp. la th´eorie de la stigmergie: essai
d´interpr´etation du comportement des termites constructeurs. Insectes Sociaux, 6:41–
81, 1959.
[21] M. Gruber and Gu¨nther. R. Raidl. Variable Neighborhood Search for the Bounded
Diameter Minimum Spanning Tree Problem. PhD thesis, Institute of Computer Graphics
and Algorithms, Vienna University of Technology, 2005.
[22] Martin Gruber and Gu¨nther R. Raidl. A new 0–1 ILP approach for the bounded
diameter minimum spanning tree problem. In L. Gouveia and C. Mour˜ao, editors,
Proceedings of the 2nd International Network Optimization Conference, volume 1, pages
178–185, Lisbon, Portugal, 2005.
[23] P. Hansen and N. Mladenovi´c. An introduction to variable neighborhood search. In
S. Voss, S. Martello, I.H. Osman, and C. Roucairol, editors, Meta-heuristics, Advances
and Trends in Local Search Paradigms for Optimization, pages 433–458. Kluwer
Academic Publishers, 1999.
[24] B. A. Julstrom and G. R. Raidl. A permutation-coded evolutionary algorithm for
the bounded-diameter minimum spanning tree problem. In A. Barry, F. Rothlauf,
D. Thierens, et al., editors, in 2003 Genetic and Evolutionary Computation
Conference’s Workshops Proceedings, Workshop on Analysis and Desgn of Representations,
pages 2–7, 2003.
[25] Bryant A. Julstrom. Encoding bounded-diameter minimum spanning trees with
permutations and with random keys. In Kalyanmoy Deb et al., editors, Genetic and
Evolutionary Computation Conference – GECCO 2004, volume 3102 of LNCS, pages
1282–1281. Springer, 2004.
[26] Bryant A. Julstrom. Greedy heuristics for the bounded-diameter minimum spanning
tree problem. Technical report, St. Cloud State University, 2004. Submitted for
publication in the ACM Journal of Experimental Algorithmics.
[27] T. Magnanti and R. Wong. Network design and transportation planning: Models and
algorithms. Transportation Science, 18(1), 1984.
[28] N. Mladenovi´c. A variable neighborhood algorithm - a new metaheuristic for
combinatorial optimization. Abstracts of papers presented at Optimization Days. Montreal,
1995.
[29] N. Mladenovi´c and P. Hansen. Variable neighborhood search. Computers Opers. Res.,
24:1097–1100, 1997.
[30] C. H. Papadimitriou and K. Steiglitz. Combinatorial Optimization-Algorithms and</p>
        <p>Complexity. Dover Publications, Inc., New York, 1982.
[31] Gu¨nther R. Raidl and Bryant A. Julstrom. Greedy heuristics and an evolutionary
algorithm for the bounded-diameter minimum spanning tree problem. In Gary Lamont
[35] S. Voß, S. Martello, I. H. Osman, and C. Routacirol. Meta-Heuristics-Advances and
Trends in Local Search Paradigms for Optimization. Kluwer Academic Publishers,
Dordrecht, The Netherlands.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          3.
          <string-name>
            <surname>Metaheuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11</surname>
          </string-name>
          <year>3</year>
          .1
          <string-name>
            <given-names>Variable</given-names>
            <surname>Neighbourhood</surname>
          </string-name>
          <string-name>
            <surname>Search . . . . . . . . . . . . . . . . . . . . . . . . . 13</surname>
          </string-name>
          <year>3</year>
          .2
          <string-name>
            <given-names>Ant</given-names>
            <surname>Colony</surname>
          </string-name>
          <string-name>
            <surname>Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16</surname>
          </string-name>
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          4.
          <string-name>
            <surname>Neighbourhood</surname>
            <given-names>Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21</given-names>
          </string-name>
          <year>4</year>
          .1
          <string-name>
            <given-names>Tree</given-names>
            <surname>Based</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhoods . . . . . . . . . . . . . . . . . . . . . . . . . . . 22</surname>
          </string-name>
          <year>4</year>
          .
          <issue>1</issue>
          .1
          <string-name>
            <given-names>Edge</given-names>
            <surname>Exchange</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhood . . . . . . . . . . . . . . . . . . . . . 22</surname>
          </string-name>
          <year>4</year>
          .
          <issue>1</issue>
          .2
          <string-name>
            <given-names>Node</given-names>
            <surname>Swap</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhood . . . . . . . . . . . . . . . . . . . . . . . 24</surname>
          </string-name>
          <year>4</year>
          .2
          <string-name>
            <given-names>Level</given-names>
            <surname>Based</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhoods . . . . . . . . . . . . . . . . . . . . . . . . . . 27</surname>
          </string-name>
          <year>4</year>
          .
          <issue>2</issue>
          .1
          <string-name>
            <given-names>Centre</given-names>
            <surname>Exchange</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhood . . . . . . . . . . . . . . . . . . . . 27</surname>
          </string-name>
          <year>4</year>
          .
          <issue>2</issue>
          .2
          <string-name>
            <given-names>Level</given-names>
            <surname>Change</surname>
          </string-name>
          <string-name>
            <surname>Neighbourhood . . . . . . . . . . . . . . . . . . . . . . 30</surname>
          </string-name>
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          5.
          <string-name>
            <given-names>Ant</given-names>
            <surname>Colony</surname>
          </string-name>
          <article-title>Optimisation for the BDMST</article-title>
          <string-name>
            <surname>Problem . . . . . . . . . . . . . . . . . 39</surname>
          </string-name>
          <year>5</year>
          .1
          <string-name>
            <given-names>Variable</given-names>
            <surname>Neighbourhood</surname>
          </string-name>
          <string-name>
            <surname>Descent . . . . . . . . . . . . . . . . . . . . . . . . 39</surname>
          </string-name>
          <year>5</year>
          .2
          <string-name>
            <given-names>Ant</given-names>
            <surname>Colony</surname>
          </string-name>
          <string-name>
            <surname>Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41</surname>
          </string-name>
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          6.
          <string-name>
            <surname>Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45</surname>
          </string-name>
          <year>6</year>
          .1 User
          <string-name>
            <surname>Manual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48</surname>
          </string-name>
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          7.
          <string-name>
            <surname>Computational</surname>
            <given-names>Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52</given-names>
          </string-name>
          <year>4</year>
          .1 Edge
          <string-name>
            <surname>Exchange</surname>
            <given-names>neighbourhood . . . . . . . . . . . . . . . . . . . . . . . . . 23</given-names>
          </string-name>
          <year>4</year>
          .2 Node
          <string-name>
            <surname>Swap</surname>
            <given-names>neighbourhood. . . . . . . . . . . . . . . . . . . . . . . . . . . . 24</given-names>
          </string-name>
          <year>4</year>
          .3 Centre
          <string-name>
            <surname>Exchange</surname>
            <given-names>neighbourhood . . . . . . . . . . . . . . . . . . . . . . . . 28</given-names>
          </string-name>
          <year>4</year>
          .4 Level
          <string-name>
            <surname>Change</surname>
            <given-names>neighbourhood . . . . . . . . . . . . . . . . . . . . . . . . . .</given-names>
          </string-name>
          <article-title>30 4.5 Decrement move of node v (case 1) .</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <article-title>33 4.6 Increment move of node v (case 2) .</article-title>
          <string-name>
            <surname>. . . . . . . . . . . . . . . . . . . . . .</surname>
          </string-name>
          <volume>36</volume>
          [1]
          <string-name>
            <given-names>Ayman</given-names>
            <surname>Abdalla</surname>
          </string-name>
          , Narsingh Deo, and
          <string-name>
            <given-names>Pankaj</given-names>
            <surname>Gupta</surname>
          </string-name>
          .
          <article-title>Random-tree diameter and the diameter constrained MST</article-title>
          .
          <source>Congressus Numerantium</source>
          ,
          <volume>144</volume>
          :
          <fpage>161</fpage>
          -
          <lpage>182</lpage>
          ,
          <year>2000</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          et al., editors,
          <source>Proceedings of the 2003 ACM Symposium on Applied Computing</source>
          , pages
          <fpage>747</fpage>
          -
          <lpage>752</lpage>
          , New York,
          <year>2003</year>
          . ACM Press.
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          [32]
          <string-name>
            <given-names>Kerry</given-names>
            <surname>Raymond</surname>
          </string-name>
          .
          <article-title>A tree-based algorithm for distributed mutual exclusion</article-title>
          .
          <source>ACM Transactions on Computer Systems</source>
          ,
          <volume>7</volume>
          (
          <issue>1</issue>
          ):
          <fpage>61</fpage>
          -
          <lpage>77</lpage>
          ,
          <year>1989</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref8">
        <mixed-citation>
          [33]
          <string-name>
            <given-names>T.</given-names>
            <surname>Stu</surname>
          </string-name>
          <article-title>¨tzle and</article-title>
          <string-name>
            <given-names>H.</given-names>
            <surname>Hoos</surname>
          </string-name>
          .
          <article-title>Introducing MAX − MIN ant system</article-title>
          .
          <source>In Proceedings of the International Conference on Artifical Neural Networks and Genetic Algorithms</source>
          , pages
          <fpage>245</fpage>
          -
          <lpage>249</lpage>
          . Springer Verlag,
          <year>1997</year>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>