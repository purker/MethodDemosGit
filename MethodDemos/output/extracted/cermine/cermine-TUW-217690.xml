<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Parallel Tracking and Mapping in Hofburg Festsaal</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Georg Gerstweiler</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Christian Schönauer</string-name>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Interactive Media Systems Group, Vienna University of Technology</institution>
        </aff>
      </contrib-group>
      <abstract>
        <p />
      </abstract>
      <kwd-group>
        <kwd>Tracking</kwd>
        <kwd>and mapping</kwd>
        <kwd>augmented</kwd>
        <kwd>and virtual realities</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <fig>
      <graphic xlink:href="D:\output\methods\cermine\cermine-TUW-217690.images\img_1_1.png" />
    </fig>
    <sec id="sec-1">
      <title>-</title>
      <p>Precise localization for mobile Augmented Reality in large indoor
environments without specific tracking infrastructure is
challenging. This is especially true for rooms with changing
properties, like lighting, seating and carpeting. With these
constraints a map for a vision based tracking approach has to be
continuously updated. The Parallel Tracking and Mapping
(PTAM) algorithm is capable of generating and extending a map
while tracking the camera pose in an unknown environment.
However, it has originally been designed for small workspace
environments and has therefore certain limitations. We have
extended and modified the original implementation in order to
ensure efficient and robust map generation and tracking in large
rooms. Furthermore, we have tested a mobile setup with the
system in Festsaal in Vienna’s Hofburg, which is close to
thousand square meters in size. The user’s position and path was
tracked while the environment was augmented with virtual objects
and the system was successfully tested for robustness and
occlusions.</p>
    </sec>
    <sec id="sec-2">
      <title>1 INTRODUCTION</title>
      <p>Vienna’s historic Hofburg accommodates many grandiose rooms,
the largest of which is Festsaal. These premises are being used as
venues for exhibitions, conferences and cultural events. In this
context our project partner, the Wiener Kongresszentrum
Hofburg, is interested in providing visitors with audio/video
information about the current room or paintings on walls and
ceilings (frescos) as well as different room configurations on a
mobile device. Furthermore, a visualization of the current position
on a map can aid navigation within Hofburg. This can be helpful
to find a certain conference booth or another person during an
event (e.g. a ball).</p>
      <p>The rooms of the Hofburg are rich of features on walls and
ceilings, which makes them well-suited for a computer
visionbased approach. However, changes in lighting, seating, carpeting
and other changing elements like conference booths pose certain
restrictions. Therefore, we favor PTAM [1], which doesn’t require
any fiducial markers or models for pose estimation, over
modelbased approaches.
2</p>
    </sec>
    <sec id="sec-3">
      <title>SYSTEM &amp; WORKFLOW</title>
      <p>Our test platform was a dual-core laptop with an off-the-shelf
webcam. The software was based on Castle and Klein’s PTAM
implementation [2] with several extensions and modifications to
ensure efficient and robust map generation and tracking in large
rooms.</p>
      <p>In a first stage the system was used to create a detailed map in a
semi-automatic procedure, while walking through the room. Our
adaptations made this process more efficient by improving the
selection of features and keyframes, resulting in a more precise
map while using less keyframes. New interface methods allow
manual intervention to avoid structures, which shouldn't be
integrated in the map (e.g. falsework present in Festsaal during
first tests, or chandeliers, which are problematic due to their
complicated structure and reflective properties). In addition, we
have modified the constraints of the original algorithm to allow
tracking behind the user's original position. New constraints
maintain efficiency in the larger environment for example by
avoiding high feature density in certain areas and removal of
features outside the basic cubical shape of the room.
3</p>
    </sec>
    <sec id="sec-4">
      <title>RESULTS &amp; CONCLUSION</title>
      <p>We have tested our mobile setup in Festsaal and created a
detailed map. Once the camera pose was available, the
environment was augmented with various virtual content. In a first
test we added a virtual theater stage and walked around it. The
position and scaling of the scene was robust, while the frame-rate
remained close to the update-rate of the camera. In addition, we
added a virtual furniture set, which showed similar results.
Another important use case was tracking of a user within the map.
Therefore, we reconstructed the path of the user frame-by-frame
in real-time.</p>
      <p>Another test demonstrated the system's behavior despite rapid
camera movement and occlusions. During first tests in Festsaal
there was a falsework in the middle of the room. While massive
occlusions pose problems on any vision-based tracking system,
with limited occlusions the tracking remained robust in many
cases.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          <string-name>
            <given-names>Georg</given-names>
            <surname>Klein</surname>
          </string-name>
          and
          <article-title>David Murray, Parallel Tracking and Mapping for Small AR Workspaces</article-title>
          .
          <source>In Proc. International Symposium on Mixed and Augmented Reality (ISMAR'07</source>
          ,
          <string-name>
            <surname>Nara</surname>
            <given-names>)</given-names>
          </string-name>
          ,
          <year>2007</year>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          <string-name>
            <given-names>R. O.</given-names>
            <surname>Castle</surname>
          </string-name>
          , G. Klein, and
          <string-name>
            <given-names>D. W.</given-names>
            <surname>Murray</surname>
          </string-name>
          .
          <article-title>Video-rate localization in multiple maps for wearable augmented reality</article-title>
          .
          <source>In Proc. 12th IEEE Int. Symp. on Wearable Computing</source>
          ,
          <string-name>
            <surname>Pittsburgh</surname>
            <given-names>PA</given-names>
          </string-name>
          , pages
          <fpage>15</fpage>
          -
          <lpage>22</lpage>
          ,
          <year>2008</year>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>