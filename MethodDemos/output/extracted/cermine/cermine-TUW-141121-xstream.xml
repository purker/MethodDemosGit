<Publication>
  <id>TUW-141121</id>
  <title>Integration of Text and Audio Features for Genre Classification in Music Information Retrieval</title>
  <abstractText>Multimedia content can be described in versatile ways as its essence is not limited to one view. For music data these multiple views could be a song&apos;s audio features as well as its lyrics. Both of these modalities have their advantages as text may be easier to search in and could cover more of the &apos;content semantics&apos; of a song, while omitting other types of semantic categorisation. (Psycho)acoustic feature sets, on the other hand, provide the means to identify tracks that &apos;sound similar&apos; while less supporting other kinds of semantic categorisation. Those discerning characteristics of different feature sets meet users&apos; differing information needs. We will explain the nature of text and audio feature sets which describe the same audio tracks. Moreover, we will propose the use of textual data on top of low level audio features for music genre classification. Further, we will show the impact of different combinations of audio features and textual features based on content words.</abstractText>
  <maxSectionLayer>0</maxSectionLayer>
  <keywords/>
  <authors>
    <Author>
      <name>Robert Neumayer</name>
      <firstNames/>
      <affiliations>
        <Affiliation>
          <id>aff0</id>
          <institution>Vienna University of Technology Institute of Software Technology and Interactive Systems</institution>
        </Affiliation>
      </affiliations>
    </Author>
    <Author>
      <name>Andreas Rauber</name>
      <firstNames/>
      <affiliations>
        <Affiliation reference="/Publication[1]/authors[1]/Author[1]/affiliations[1]/Affiliation[1]"/>
      </affiliations>
    </Author>
    <Author>
      <name>neumayer</name>
      <firstNames/>
      <affiliations>
        <Affiliation reference="/Publication[1]/authors[1]/Author[1]/affiliations[1]/Affiliation[1]"/>
      </affiliations>
    </Author>
    <Author>
      <name>rauber}@ifs.tuwien.ac.at</name>
      <firstNames/>
      <affiliations>
        <Affiliation reference="/Publication[1]/authors[1]/Author[1]/affiliations[1]/Affiliation[1]"/>
      </affiliations>
    </Author>
  </authors>
  <affiliations>
    <Affiliation reference="/Publication[1]/authors[1]/Author[1]/affiliations[1]/Affiliation[1]"/>
  </affiliations>
  <sections>
    <Section>
      <id>sec-1</id>
      <title>-</title>
      <referenceIds/>
      <referenceCitations/>
    </Section>
    <Section>
      <id>sec-2</id>
      <title>Related Work</title>
      <referenceIds>
        <string>ref1</string>
        <string>ref6</string>
        <string>ref5</string>
        <string>ref2</string>
        <string>ref4</string>
        <string>ref3</string>
      </referenceIds>
      <referenceCitations/>
    </Section>
    <Section>
      <id>sec-3</id>
      <title>Experiments</title>
      <referenceIds/>
      <referenceCitations/>
    </Section>
    <Section>
      <id>sec-4</id>
      <title>Conclusions and Future Work</title>
      <referenceIds/>
      <referenceCitations/>
    </Section>
  </sections>
  <citationContexts/>
  <references>
    <Reference>
      <id>ref1</id>
      <title>An overview of audio information retrieval</title>
      <source>Multimedia Systems</source>
      <authors>
        <ReferenceAuthor>
          <firstNames>
            <string>Jonathan</string>
          </firstNames>
          <lastName>Foote</lastName>
        </ReferenceAuthor>
      </authors>
      <volume>7</volume>
      <issue>1</issue>
      <pageFrom>2</pageFrom>
      <pageTo>10</pageTo>
      <publicationYear>1999</publicationYear>
    </Reference>
    <Reference>
      <id>ref2</id>
      <title>Evaluation of feature extractors and psychoacoustic transformations for music genre classification</title>
      <source>In Proceedings of the Sixth International Conference on Music Information Retrieval (ISMIR</source>
      <authors>
        <ReferenceAuthor>
          <firstNames>
            <string>Thomas</string>
          </firstNames>
          <lastName>Lidy</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Andreas</string>
          </firstNames>
          <lastName>Rauber</lastName>
        </ReferenceAuthor>
      </authors>
      <volume>11</volume>
      <pageFrom>34</pageFrom>
      <pageTo>41</pageTo>
      <publicationYear>2005</publicationYear>
    </Reference>
    <Reference>
      <id>ref3</id>
      <title>Semantic analysis of song lyrics</title>
      <source>In Proceedings of the 2004 IEEE International Conference on Multimedia and Expo</source>
      <authors>
        <ReferenceAuthor>
          <firstNames>
            <string>Beth</string>
          </firstNames>
          <lastName>Logan</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Andrew</string>
          </firstNames>
          <lastName>Kositsky</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Pedro</string>
          </firstNames>
          <lastName>Moreno</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames/>
          <lastName>ICME</lastName>
        </ReferenceAuthor>
      </authors>
      <volume>27</volume>
      <pageFrom>30</pageFrom>
      <publicationYear>2004</publicationYear>
    </Reference>
    <Reference>
      <id>ref4</id>
      <title>Natural language processing of lyrics</title>
      <source>In MULTIMEDIA &apos;05: Proceedings of the 13th annual ACM international conference on Multimedia</source>
      <authors>
        <ReferenceAuthor>
          <firstNames/>
          <lastName>A</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Fabien</string>
          </firstNames>
          <lastName>Gouyon</lastName>
        </ReferenceAuthor>
      </authors>
      <pageFrom>475</pageFrom>
      <pageTo>478</pageTo>
      <publicationYear>2005</publicationYear>
    </Reference>
    <Reference>
      <id>ref5</id>
      <title>Using psycho-acoustic models and self-organizing maps to create a hierarchical structuring of music by musical styles</title>
      <source>In Proceedings of the 3rd International Symposium on Music Information Retrieval</source>
      <authors>
        <ReferenceAuthor>
          <firstNames>
            <string>Andreas</string>
          </firstNames>
          <lastName>Rauber</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Dieter</string>
          </firstNames>
          <lastName>Merkl</lastName>
        </ReferenceAuthor>
      </authors>
      <volume>13</volume>
      <pageFrom>71</pageFrom>
      <pageTo>80</pageTo>
      <publicationYear>2002</publicationYear>
    </Reference>
    <Reference>
      <id>ref6</id>
      <title>Marsyas: A framework for audio analysis</title>
      <source>Organized Sound</source>
      <authors>
        <ReferenceAuthor>
          <firstNames>
            <string>George</string>
          </firstNames>
          <lastName>Tzanetakis</lastName>
        </ReferenceAuthor>
        <ReferenceAuthor>
          <firstNames>
            <string>Perry</string>
          </firstNames>
          <lastName>Cook</lastName>
        </ReferenceAuthor>
      </authors>
      <volume>4</volume>
      <issue>30</issue>
      <publicationYear>2000</publicationYear>
    </Reference>
  </references>
</Publication>