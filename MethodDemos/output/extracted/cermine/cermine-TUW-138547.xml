<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink">
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Gigabit Ethernet-based Parallel Video Processing</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Horst Eidenberger</string-name>
          <email>eidenberger@ims.tuwien.ac.at</email>
          <xref ref-type="aff" rid="aff0">0</xref>
        </contrib>
        <aff id="aff0">
          <label>0</label>
          <institution>Vienna University of Technology, Institute of Software Technology and Interactive Systems Favoritenstrasse 9-11</institution>
          ,
          <addr-line>A-1040 Vienna</addr-line>
          ,
          <country country="AT">Austria</country>
        </aff>
      </contrib-group>
      <abstract>
        <p>This paper describes solutions for parallel video processing based on LAN-connected PC-like workstations. We outline application scenarios for the processing of video with broadcast TV resolution and promising areas for future research. Additionally, we describe a prototype system implemented in our workgroup. This Network-of-Workstations is based on Gigabit Ethernet, free Java software and standard network protocols. Video data is streamed to and from processing hosts using IP multicast and the Real-time Transfer Protocol. Control mechanisms are based on network file sharing. In comparison to related approaches, our system makes use of highperformance network hardware and open source software. In consequence, our system is easier to implement, cheaper and more flexible than, for example, highly integrated commercial solutions.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="sec-1">
      <title>-</title>
      <p>We have implemented a prototypical multimedia
middleware for parallel video processing. The software
layer is based on a Network-of-Workstations (NOW)
connected by Gigabit Ethernet switches. We used this
prototype to evaluate the principle feasibility of
LANbased processing of streamed media data. The
advantages of such a setup over specialised hardware
configurations are obvious: Using of-the-shelf
hardware and free software reduces the costs
enormously. Additionally, network clusters can easily
be up-scaled (in the worst case by adding/exchanging
network devices). If the software layer is based on free
software and is itself implemented as open source, it
can easily be adapted to new application scenarios.
Finally, since every single component can be
exchanged without affecting the rest of the installation,
network solutions allow for greater flexibility and, in
consequence, potentially higher return of investment.</p>
      <p>
        The presented idea is not that new. Papers [
        <xref ref-type="bibr" rid="ref4 ref5 ref6">4, 5, 6</xref>
        ]
describe PSVP, a similar approach for Internet video
processing based on hardware and software that were
state-of-the-art in the late nineties. Section 2 describes
the PSVP system. Subsection 4.2 discusses the
differences of our approach to PSVP.
      </p>
      <p>We have four reasons for rising this topic again.
Since for the reasons given above, NOW-based video
processing may become an important area of future
multimedia research we wanted to stress the principle,
its potentials and open research issues again.
Especially, Subsection 3.3 lists (encouraged by the
conference call) relevant areas requiring further
research. Secondly, recent years' technological
advances have lead to changed infrastructure
parameters. On the hardware level, Gigabit Ethernet
and increasing power of PC-like workstations as well
as free software development kits for multimedia
programming enable greater flexibility and allow us
aiming at significantly more demanding applications.
Thirdly, we propose an approach for spatial
parallelisation only that is based on a simpler
architecture for processing control and implemented by
exclusively using standard operating system tools.
Finally, technical limitations (mainly network
bandwidth) forced restricting the earlier
implementation on Internet video with lower spatial
and temporal frequency. In contrast, our prototype is
intended for manipulation of live video streams of
television resolution (NTSC, PAL, SECAM). It allows
for performing analysis and effect processing tasks as
well as sophisticated compositing tasks involving
multiple parallel streams. In conclusion, new
developments have lead to a substantially changed
situation that justifies making NOW-based video
processing a multimedia research issue again.</p>
      <p>The paper is organised as follows: Section 2 gives
background information on earlier approaches
(including PSVP). Section 3 discusses application
scenarios for parallel video processing and promising
research areas. Section 4 sketches the architecture of
our prototype. In particular, Subsection 4.2 lists the
major differences to PSVP. Finally, Section 5 discusses
implementation issues and relevant future developments
in the area of multimedia software middleware.
2.</p>
    </sec>
    <sec id="sec-2">
      <title>Related work</title>
      <p>
        For related work we concentrated on approaches
using local area networks for video transport from host
to host. We excluded DSP and VLSI chip design
approaches as well as work on processing units and
network design for MIMD architectures. Surprisingly,
only a handful relevant publications could be
identified. The earliest paper [
        <xref ref-type="bibr" rid="ref2">2</xref>
        ] exploits connecting
video processing elements by an ATM-based LAN.
The authors' focus is on broadcasting (including media
capture, display, storage and retrieval), not video
processing. They investigate the suitability of ATM
technology for parallel video processing. In [
        <xref ref-type="bibr" rid="ref1">1</xref>
        ],
principle solutions for spatial partitioning for video
processing are discussed. The authors introduce novel
heuristic algorithms for frame-wise spatial media
splitting. Unfortunately, the paper does not contain
experimental results. The interesting paper [
        <xref ref-type="bibr" rid="ref7">7</xref>
        ]
describes pipelining strategies for spatially and
temporally parallelised video processing. It describes a
scheduler that schedules tasks at compile time. Two
optimisation criteria are used: minimising the number
of required processors for a task and maximising the
throughput for a fixed number of processors. Parallel
processing algorithms are automatically generated from
high-level descriptions. All algorithms are implemented
in a NOW and experimentally evaluated.
      </p>
      <p>
        In [
        <xref ref-type="bibr" rid="ref4 ref5 ref6">4, 5, 6</xref>
        ] the authors describe the architecture of
the Parallel Software-only Video effects Processing
system (PSVP). [
        <xref ref-type="bibr" rid="ref6">6</xref>
        ] discusses methods and algorithms
for temporal parallelisation. Similarly, [
        <xref ref-type="bibr" rid="ref5">5</xref>
        ] describes
strategies and protocols needed for spatial
parallelisation. In [
        <xref ref-type="bibr" rid="ref4">4</xref>
        ] the authors deal with control
aspects of parallel video processing: selection of
parallelisation method, distribution of algorithms and
control of data flow. Below, the major aspects and
components of the PSVP are described. See Subsection
4.2 for differences of our proposal to PSVP.
      </p>
      <p>The PSVP was implemented in a NOW of 100
SUN Sparc workstations connected by 10 Mbps
Ethernet. Two types of workstation roles are
distinguished: effects server and effects processor. The
effects server is responsible for setup and control of
effects processors. Effects processors are used for
parallel video processing. The application
communicates with the NOW through the effects
server. Video streaming is based on UDP/IP and the
Real-time Transfer Protocol (RTP) using MotionJPEG
and self-defined propriatory formats as payload types.</p>
      <p>Three software components are needed to run the
system: FX compiler, FX mapper and FX processor.
FX compiler translates high-level descriptions of video
effects to binary code that can be executed on effect
processor hosts. Effects are described as directed
graphs by using the vocabulary of the Dali scripting
language. Dali defines high-level operators for image
processing. FX processor is a shell for effect program
execution on effects processor hosts. FX processor
handles configuration calls, video data reception and
transmission. FX mapper maps spatio-temporal video
segments to effects processor hosts. It implements a
recursive partitioning process: Since both spatial and
temporal parallelisation have their advantages, it would
be desirable combining both methods in one video
processing task. FX mapper is able to define a layered
structure in which spatial and temporal parallelisation
strategies are employed on each layer independently of
strategies employed on other layers.</p>
      <p>PSVP uses IP multicast for video transmission. The
entire media stream is sent to all processing hosts. This
approach guarantees that processors have all
information they need and takes advantage of the
network infrastructure by passing on the data
multiplication process to the subnet. Recombination of
processed frames is based on an intermediate format
that allows easy merging. Similarly to the DV format,
YCrCb channels are block-wise DCT encoded and
transmitted using RTP. Temporal parallelisation is
based on a selector function that redirects frames to
effects processors and an interleaver function that
recreates the video stream from processed frames.
3.</p>
    </sec>
    <sec id="sec-3">
      <title>Applications and problems of Parallel</title>
    </sec>
    <sec id="sec-4">
      <title>Video Processing</title>
      <p>The three following sections are dedicated to the
video processor we implemented. We will focus on the
principal architecture and stress major differences to
PSVP. This section lists use cases for parallel video
processing and areas for future research. Section 4
sketches video processor design including control and
data flow. Section 5 deals with implementation issues.
3.1</p>
    </sec>
    <sec id="sec-5">
      <title>Basic use cases</title>
      <p>Using a LAN-connected cluster of workstations for
video processing is a generally appealing idea but in
order to design a practically usable system, use cases
have to be identified and requirements have to be
derived. Such classic software engineering best
practices are – even though being mandatory to
estimate network, processing and software demands
properly – hardly considered in PSVP and related
projects. We have identified three basic application
patterns (depicted in Figure 1). The analysis pattern
takes one media stream as input and extracts media
metadata. Exemplary applications are computer vision
and visual information retrieval tasks. Analysis requires
minimal network bandwidth at (likely) maximum
processing power. Manipulation comprises all
operations on one video stream (e.g. effect processing,
format conversion). Manipulation network load is
about twice as high as for analysis tasks. Compositing
tasks combine two video streams (e.g. animation
foreground and video background) to one output
stream. Compositing induces the highest network load
while processing is mostly simple alpha blending. Our
video processor is able to deal with all three types of
applications. We tested it, for example, with a tabletop
soccer ball tracking application and a video effect that
creates artefacts appearing in old movies.
3.2</p>
    </sec>
    <sec id="sec-6">
      <title>Application use cases</title>
      <p>More complex use cases can be assembled from
these basic patterns. For example, a virtual studio
application would require at least three input streams:
foreground (including foreground mask), background
and mask. Foreground mask and mask are replaced by
the background. A virtual studio could be implemented
Compositing</p>
      <sec id="sec-6-1">
        <title>Manipulation</title>
      </sec>
      <sec id="sec-6-2">
        <title>Geometry</title>
        <p>Information</p>
      </sec>
      <sec id="sec-6-3">
        <title>Compositing</title>
      </sec>
      <sec id="sec-6-4">
        <title>Output Signal</title>
        <p>by firstly merging foreground and mask (compositing)
and then, merging the result with the background video.
Figure 2 illustrates the data flow. Geometry
information is employed to guarantee that the
(computer-generated) background scene is inserted
from the correct perspective. If a camera matrix is used
for tracking of foreground subjects/objects (e.g.
speakers), analysis elements can be used to compute
their coordinates.</p>
        <p>As a second type of application, camera arrays
could be implemented by series of analysis tasks (e.g.
object tracking) and manipulation tasks (e.g.
construction of panorama views). For 3D object
reconstruction from video, input video signals could be
merged pair-wise to stereo signals (by a compositing
element). Then, depth information could be extracted
from the stereo signals (analysis elements). The
resulting data streams could be used matching, object
recognition and reconstruction.
3.3</p>
      </sec>
    </sec>
    <sec id="sec-7">
      <title>Problem areas</title>
      <p>Several promising research directions can be
derived from use cases and related work. Below, we
consider four selected areas: parallelisation, setup and
control, networking, software design.</p>
      <p>Parallelisation can be performed functionally,
spatially or temporally. Functional parallelisation is
classical pipelining: chunks of media streams are
manipulated in parallel using different operations in
different steps. Since there is no difference to
pipelining in other areas (e.g. microprocessor design),
it is not relevant here. Spatial parallelisation raises two
major problems: How/where should the media be split?
Following the simplest solution would mean sending
the entire media stream to all processing nodes.
Obviously, this strategy puts high load on the network.
For sophisticated application scenarios using multiple
instances of the basic use cases, this strategy may be
too resource-demanding. On the other hand, if the
media is split, processing nodes get limited information
that may turn out being insufficient (e.g. for analysis
tasks). The second question is, how merging should be</p>
      <sec id="sec-7-1">
        <title>Media In IN</title>
      </sec>
      <sec id="sec-7-2">
        <title>Control</title>
      </sec>
      <sec id="sec-7-3">
        <title>Media Out OUT Processor #1 ...</title>
      </sec>
      <sec id="sec-7-4">
        <title>Processor #n</title>
      </sec>
      <sec id="sec-7-5">
        <title>Application interface</title>
      </sec>
      <sec id="sec-7-6">
        <title>Network</title>
      </sec>
      <sec id="sec-7-7">
        <title>Processor pool</title>
        <p>performed. This includes issues from adequately
designed media stream formats allowing fast,
resourcesaving merging to the definition of error handling
procedures. Temporal parallelisation rises the
questions, if streams with full or reduced frame rate
should be redirected to processing nodes and how
interleaving of processed streams should be performed.
Answering the first question, again, a reasonable
tradeoff between resource consumption and availability of
information has to be identified. Definition of
interleaving strategies comprises intra-media
synchronisation and handling of frame drops.</p>
        <p>Setup and control problems include the avoidance
of bottlenecks in splitter/merger components, network
configuration and load balancing. Support of
upscaling of processing nodes is not sufficient to justify
judging a NOW-based video processor being scalable.
Performance bottlenecks can only be avoided, if
splitting and merging tasks can be distributed over
multiple nodes (e.g. by using hierarchies of hosts).</p>
        <p>Network configuration includes distribution of
processing scripts and parameters, and start/stop
synchronisation. Performing configuration requires
well-defined protocols and interfaces. For data
transmission, suitable communication channels have to
be defined. Closely related networking problems are
bandwidth estimation and optimisation, selection of
codecs as well as quality of service issues (estimation
of availability of nodes, etc.). Furthermore, monitoring
functionalities are required for controlling and
finetuning of parallel processing.</p>
        <p>The most relevant software design problem is
providing a (visual) language for effect definition. As a
major requirement the language has to allow easy
parallelisation without the need for inter-process
communication. Distribution of scripts to processing
nodes must be as simple as possible. Distribution
requires solving problems such as parameter
distribution, synchronisation and interactive monitoring
through distribution-transparent interfaces.
Furthermore, quality of service issues (e.g. handling of
package losses) have to be considered. The effect
designer must be enabled to define error handling
procedures tailor-made for his effect type. Finally,
graph-based drag-and-drop user interfaces would be
desirable for rapid prototyping of effect scripts. If
standard components (e.g. colour transformations, edge
operators) could be identified, user interfaces could
easily be implemented that allow for assembling effect
graphs from a palette of pre-defined components in a
Visio-like style.
4.</p>
      </sec>
    </sec>
    <sec id="sec-8">
      <title>Prototype design</title>
      <p>Our major goal in designing the video processor
system (VP) was to make use of already existing (free)
standard software whenever possible. The architecture
should be simple and tailor-made for the described use
cases. Open source software has to be considered as an
important factor in software engineering and, today,
even for multimedia applications remarkably powerful
free tools do exist.
4.1</p>
    </sec>
    <sec id="sec-9">
      <title>Architecture</title>
      <p>Figure 3 sketches the VP architecture. Video
streams are read and distributed to processing nodes by
an IN-node. An OUT-node is responsible for collecting
and reassembling the processed media streams. IN and
OUT are software components that can either reside on
a single host, on two dedicated hosts or be distributed
over a hierarchy of hosts. This flexible concept allows
scalability for video stream splitting and merging and
avoids the major bottleneck of PSVP.</p>
      <p>The Control component is responsible for network
setup and configuration of processing nodes. Currently,
VP supports only spatial parallelisation. Media data is
transported through the network using the Real-time
Transfer Protocol, RTCP for quality control and a
Photo JPEG codec with lossless compression. We
selected this codec because it provides effective
compression at minimum encoding/decoding costs. The
input media stream is sent to all processing nodes in its
entirety using IP multicast on the router level. Only
processed video data is sent back using the same codec
and RTP unicast. Since the VP network is based on
Gigabit Ethernet, sufficient bandwidth is available to
support all basic use cases for TV resolution videos
(NTSC, PAL, SECAM) at minimum compression (see
Subsection 5.2 for performance evaluations).</p>
      <p>Effects can be defined in a high-level programming
language. Concrete implementations are derived from a
template that guarantees that spatial parallelisation can
easily be accomplished. Basically, the template dictates
frame processing in loops over rows and columns. The
Control component distributes binary effect programs
to processing nodes using a standard network file
sharing protocol. A processor shell runs on every
processing node that is able to execute digital effect
scripts. Before processing can be started, Control
configures IN and OUT by associating spatial regions
with the IP addresses of processing nodes.</p>
      <p>As mentioned above, IN and OUT are
softwareonly components and may exist in multiple instances.
Associations can be made arbitrarily. Therefore, it is,
for example, possible to define a configuration with a
shadow fail-back system. Providing such features
makes the VP architecture suitable for high availability
applications (TV broadcasting, etc.)</p>
      <p>Video processing starts with the first frame
distributed by IN. Since we use RTP for streaming,
there is no way but providing sufficient network
bandwidth to guarantee quality of service. Luckily, in
our experiments Gigabit Ethernet turned out to be
sufficiently fast. The OUT component manages output
synchronisation using a large cache memory.
4.2</p>
    </sec>
    <sec id="sec-10">
      <title>Differences of VP to PSVP</title>
      <p>The architecture of VP is generally simpler: VP is
assembled of less components. All software
components used are released under open source
licenses. The hardware components involved are cheap
PC-like workstations and of-the-shelf network
switches. Still, VP supports processing of video with
full TV-resolution. The PSVP was designed to be used
on Internet video streams with a resolution of 320 by
240 pixels.</p>
      <p>VP components can be arbitrarily combined. It is
even possible to assemble circuits from multiple VP
instances. This feature allows for implementing
sophisticated application scenarios (e.g. virtual studios)
from basic use cases. Furthermore, since video stream
input and output operations can be distributed over
multiple machines, the input/output bottleneck is
avoided in VP.</p>
      <p>Generally, VP is based on simpler control
mechanisms: Standard protocols are used for media
and control data transmission. Therefore, standard
network tools can be used for configuration and
monitoring. All components use the same
communication interfaces and the same codecs on all
transmission channels. Hence, VP supports highest
configuration flexibility. On the other hand,
configuration is performed on a technically lower level
than in the PSVP. Especially, currently VP supports
only configuration for spatial parallelisation.
5.
5.1</p>
    </sec>
    <sec id="sec-11">
      <title>Implementation</title>
    </sec>
    <sec id="sec-12">
      <title>Software</title>
      <p>
        VP is an ongoing open source project. Currently,
control mechanisms are only partially available and the
OUT component has only alpha status. All other
components are at least on a beta quality level. All
media processing software components (including
effect templates) are implemented in the Java
programming language. We are using the Java Media
Framework [
        <xref ref-type="bibr" rid="ref3">3</xref>
        ] for video manipulation and streaming
instead of Quicktime for Java, because it is open
software, can be used free of charge and implements
RTP, RTCP and the required codecs. At the current
point in time, other options do not exist. The Control
component is implemented using the Samba protocol
and Perl scripts. All nodes are running Linux operating
systems and identical Java Virtual Machines.
      </p>
      <p>Even though Java performs excellently in our test
environment, future implementations may have to be
based on a lower but faster level. Unfortunately,
currently state of the art multimedia environments as
DirectShow, Quicktime and OpenML do not support
RTP-based streaming. Furthermore, they are only
available for a few operating systems.
5.2</p>
    </sec>
    <sec id="sec-13">
      <title>Gigabit Ethernet network</title>
      <p>Gigabit Ethernet technology is the major advance
that allows performing professional parallel video
processing in LAN environments. To proof the
concept, we evaluated the networking performance of a
standard Gigabit Ethernet switch with 16 ports by
connecting three processing nodes.</p>
      <p>Resolution</p>
      <p>Data rate (raw/JPEG)
158,0 / 19,2 Mbps
158,2 / 16,56 Mbps
158,2 / 11,6 Mbps</p>
      <p>We used the VP software for a compositing
application on NTSC, PAL and PALplus (16:9 aspect
ratio) video streams. Applying lossless JPEG
compression, the NTSC stream turned out to require
the highest bandwidth. See Table 1 for details. The
depicted required bandwidth is without overhead for
RTP- and other protocol headers and without
considering RTCP communication bandwidth. In a
compositing scenario, IN sends two video streams to
the processing pool using multicast. The streams sent
from processing nodes to OUT sum up to one unicast
with full TV resolution. We implemented this scenario
and tested it with a number of test streams over a
duration of 20 minutes per test. Independent of the
media format, we could not observe a single frame
drop in any test.</p>
      <p>These results prove that Gigabit Ethernet provides
technically sufficient (and cheap) foundations for
highly sophisticated parallel video processing tasks.
Since the network turned out to be that reliable, using
RTP based on unreliable UDP is also justified as being
a sufficiently good solution.</p>
      <p>
        We present a system for parallel video processing
that is based on of-the-shelf hardware (PC-like
workstations and Gigabit Ethernet) and free software.
The principle is not new: the PSVP system [
        <xref ref-type="bibr" rid="ref6">6</xref>
        ]
implements a similar idea. Major differences to our
approach are that Gigabit Ethernet allows performing
sophisticated video processing tasks in TV resolution
and that all components of our system are free
software. Still, there are many open research problems
that should be tackled in the future. Our future work
will include a reimplementation based on the OpenML
framework as well as performance evaluations for
larger networks.
      </p>
    </sec>
    <sec id="sec-14">
      <title>Acknowledgements</title>
      <p>The author would like to thank Christian
Breiteneder and Axel Filipovic for their valuable
comments and suggestions for improvement.</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref id="ref1">
        <mixed-citation>
          [1]
          <string-name>
            <given-names>D.T.</given-names>
            <surname>Altilar</surname>
          </string-name>
          , and
          <string-name>
            <given-names>Y.</given-names>
            <surname>Paker</surname>
          </string-name>
          ,
          <article-title>"Minimum Overhead Data Partitioning Algorithms for Parallel Video Processing,"</article-title>
          <source>Proceedings Domain Decomposition Methods Conference</source>
          , Chiba Japan, Oct.
          <year>2001</year>
          , pp.
          <fpage>251</fpage>
          -
          <lpage>258</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref2">
        <mixed-citation>
          [2]
          <string-name>
            <given-names>A.</given-names>
            <surname>Guha</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Pavan</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            ,
            <surname>Liu</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Rastogi</surname>
          </string-name>
          , and
          <string-name>
            <given-names>T.</given-names>
            <surname>Steeves</surname>
          </string-name>
          ,
          <article-title>"Supporting Real-Time and Multimedia Applications on the Mercuri Testbed,"</article-title>
          <source>IEEE Journal on Selected Areas in Communications</source>
          , vol.
          <volume>13</volume>
          , no.
          <issue>4</issue>
          , IEEE Press,
          <year>May 1995</year>
          , pp.
          <fpage>749</fpage>
          -
          <lpage>763</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref3">
        <mixed-citation>
          [3]
          <string-name>
            <given-names>SUN</given-names>
            <surname>Microsystems</surname>
          </string-name>
          , Java Media Framework Website, http://java.sun.com/ products/java-media/jmf/ (last visited:
          <fpage>2004</fpage>
          -09-10).
        </mixed-citation>
      </ref>
      <ref id="ref4">
        <mixed-citation>
          [4]
          <string-name>
            <given-names>K.</given-names>
            <surname>Mayer-Patel</surname>
          </string-name>
          , and
          <string-name>
            <given-names>L.A.</given-names>
            <surname>Rowe</surname>
          </string-name>
          ,
          <article-title>"A Multicast Control Scheme for Parallel Software-only Video Effects Processing,"</article-title>
          <source>Proceedings ACM Multimedia Conference</source>
          , ACM Press,
          <string-name>
            <surname>Orlando</surname>
            <given-names>FL</given-names>
          </string-name>
          ,
          <year>Nov</year>
          .
          <year>1999</year>
          , pp.
          <fpage>409</fpage>
          -
          <lpage>418</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref5">
        <mixed-citation>
          [5]
          <string-name>
            <given-names>K.</given-names>
            <surname>Mayer-Patel</surname>
          </string-name>
          , and
          <string-name>
            <given-names>L.A.</given-names>
            <surname>Rowe</surname>
          </string-name>
          ,
          <article-title>"Exploiting Spatial Parallelism for Software-only Video Effects Processing,"</article-title>
          <source>Proceedings SPIE Multimedia Computing and Networking Conference</source>
          , SPIE Press, San Jose CA,
          <year>Jan</year>
          .
          <year>1998</year>
          , pp.
          <fpage>28</fpage>
          -
          <lpage>39</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref6">
        <mixed-citation>
          [6]
          <string-name>
            <given-names>K.</given-names>
            <surname>Mayer-Patel</surname>
          </string-name>
          , and
          <string-name>
            <given-names>L.A.</given-names>
            <surname>Rowe</surname>
          </string-name>
          ,
          <article-title>"Exploiting Temporal Parallelism for Software-only Video Effects Processing,"</article-title>
          <source>Proceedings ACM Multimedia Conference</source>
          , ACM Press,
          <string-name>
            <surname>Bristol</surname>
            <given-names>UK</given-names>
          </string-name>
          ,
          <year>Sep</year>
          .
          <year>1998</year>
          , pp.
          <fpage>161</fpage>
          -
          <lpage>169</lpage>
          .
        </mixed-citation>
      </ref>
      <ref id="ref7">
        <mixed-citation>
          [7]
          <string-name>
            <given-names>M.T.</given-names>
            <surname>Yang</surname>
          </string-name>
          ,
          <string-name>
            <given-names>R.</given-names>
            <surname>Kasturi</surname>
          </string-name>
          ,
          <article-title>and</article-title>
          <string-name>
            <given-names>A.A.</given-names>
            <surname>Sivasubramaniam</surname>
          </string-name>
          ,
          <article-title>"Pipeline-Based Approach for Scheduling Video Processing Algorithms on NOW," IEEE Transactions on Parallel and Distributed Systems</article-title>
          , IEEE Press, vol.
          <volume>14</volume>
          , no.
          <issue>2</issue>
          ,
          <string-name>
            <surname>Feb</surname>
          </string-name>
          .
          <year>2003</year>
          , pp.
          <fpage>119</fpage>
          -
          <lpage>130</lpage>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>